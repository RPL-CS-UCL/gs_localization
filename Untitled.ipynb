{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18c0e224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from random import randint\n",
    "from utils.loss_utils import l1_loss, ssim\n",
    "from gaussian_renderer import render, network_gui\n",
    "import sys\n",
    "from scene import Scene, GaussianModel\n",
    "from utils.general_utils import safe_state\n",
    "import uuid\n",
    "from tqdm import tqdm\n",
    "from utils.image_utils import psnr\n",
    "from utils.graphics_utils import depth_double_to_normal\n",
    "from argparse import ArgumentParser, Namespace\n",
    "from arguments import ModelParams, PipelineParams, OptimizationParams\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    TENSORBOARD_FOUND = True\n",
    "except ImportError:\n",
    "    TENSORBOARD_FOUND = False\n",
    "\n",
    "from scene.cameras import Camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8479632",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--sh_degree SH_DEGREE]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: C:\\ProgramData\\Anaconda3\\envs\\gaussian_splatting\\lib\\site-packages\\ipykernel_launcher.py\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import argparse\n",
    "\n",
    "# Filter out the -f argument passed by Jupyter\n",
    "args = sys.argv\n",
    "if '-f' in args:\n",
    "    args.remove('-f')\n",
    "    args.remove(next(arg for arg in args if arg.endswith('.json')))\n",
    "\n",
    "# Parse the remaining arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "# Add your arguments here, for example:\n",
    "parser.add_argument('--sh_degree', type=int, help='SH Degree')\n",
    "# ... add other arguments ...\n",
    "\n",
    "# Parse arguments\n",
    "parsed_args = parser.parse_args(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1b45ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing \n",
      "Output folder: ./output/370c79ba-0 [26/06 15:57:50]\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'scene_info' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_35712\\1118631834.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    283\u001b[0m              \u001b[0mcheckpoint_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheckpoint_iterations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m              \u001b[0mcheckpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 285\u001b[1;33m              debug_from=args.debug_from)\n\u001b[0m\u001b[0;32m    286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m     \u001b[1;31m# All done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_35712\\1118631834.py\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(dataset, opt, pipe, testing_iterations, saving_iterations, checkpoint_iterations, checkpoint, debug_from)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mtb_writer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepare_output_and_logger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mgaussians\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGaussianModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msh_degree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mscene\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mScene\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgaussians\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0mgaussians\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_setup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\master project\\RaDe-GS\\scene\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, gaussians, load_iteration, shuffle, resolution_scales, gap)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloaded_iter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscene_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mply_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msrc_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"input.ply\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdest_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m                 \u001b[0mdest_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0mjson_cams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'scene_info' referenced before assignment"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing  [26/06 15:58:57]\n",
      "Output folder: ./output/b9b1dacb-a [26/06 15:58:57] [26/06 15:58:57]\n",
      "Reading camera 64/64 [26/06 15:58:57] [26/06 15:58:57]\n",
      "cameras extent: 1.7835390806198121 [26/06 15:58:57] [26/06 15:58:57]\n",
      "Loading Training Cameras: 64 . [26/06 15:59:05] [26/06 15:59:05]\n",
      "Loading Test Cameras: 0 . [26/06 15:59:05] [26/06 15:59:05]\n",
      "Number of points at initialisation :  43031 [26/06 15:59:05] [26/06 15:59:05]\n"
     ]
    }
   ],
   "source": [
    "# function L1_loss_appearance is fork from GOF https://github.com/autonomousvision/gaussian-opacity-fields/blob/main/train.py\n",
    "def L1_loss_appearance(image, gt_image, gaussians, view_idx, return_transformed_image=False):\n",
    "    appearance_embedding = gaussians.get_apperance_embedding(view_idx)\n",
    "    # center crop the image\n",
    "    origH, origW = image.shape[1:]\n",
    "    H = origH // 32 * 32\n",
    "    W = origW // 32 * 32\n",
    "    left = origW // 2 - W // 2\n",
    "    top = origH // 2 - H // 2\n",
    "    crop_image = image[:, top:top+H, left:left+W]\n",
    "    crop_gt_image = gt_image[:, top:top+H, left:left+W]\n",
    "    \n",
    "    # down sample the image\n",
    "    crop_image_down = torch.nn.functional.interpolate(crop_image[None], size=(H//32, W//32), mode=\"bilinear\", align_corners=True)[0]\n",
    "    \n",
    "    crop_image_down = torch.cat([crop_image_down, appearance_embedding[None].repeat(H//32, W//32, 1).permute(2, 0, 1)], dim=0)[None]\n",
    "    mapping_image = gaussians.appearance_network(crop_image_down)\n",
    "    transformed_image = mapping_image * crop_image\n",
    "    if not return_transformed_image:\n",
    "        return l1_loss(transformed_image, crop_gt_image)\n",
    "    else:\n",
    "        transformed_image = torch.nn.functional.interpolate(transformed_image, size=(origH, origW), mode=\"bilinear\", align_corners=True)[0]\n",
    "        return transformed_image\n",
    "\n",
    "\n",
    "def training(dataset, opt, pipe, testing_iterations, saving_iterations, checkpoint_iterations, checkpoint, debug_from):\n",
    "    first_iter = 0\n",
    "    tb_writer = prepare_output_and_logger(dataset)\n",
    "    gaussians = GaussianModel(dataset.sh_degree)\n",
    "    scene = Scene(dataset, gaussians, gap = pipe.interval)\n",
    "    gaussians.training_setup(opt)\n",
    "    if checkpoint:\n",
    "        (model_params, first_iter) = torch.load(checkpoint)\n",
    "        gaussians.restore(model_params, opt)\n",
    "    bg_color = [1, 1, 1] if dataset.white_background else [0, 0, 0]\n",
    "    background = torch.tensor(bg_color, dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "    iter_start = torch.cuda.Event(enable_timing = True)\n",
    "    iter_end = torch.cuda.Event(enable_timing = True)\n",
    "\n",
    "    trainCameras = scene.getTrainCameras().copy()\n",
    "    gaussians.compute_3D_filter(cameras=trainCameras)\n",
    "    \n",
    "    viewpoint_stack = None\n",
    "    ema_loss_for_log, ema_depth_loss_for_log, ema_mask_loss_for_log, ema_normal_loss_for_log = 0.0, 0.0, 0.0, 0.0\n",
    "    progress_bar = tqdm(range(first_iter, opt.iterations), desc=\"Training progress\")\n",
    "    first_iter += 1\n",
    "    for iteration in range(first_iter, opt.iterations + 1):        \n",
    "        if network_gui.conn == None:\n",
    "            network_gui.try_connect()\n",
    "        while network_gui.conn != None:\n",
    "            try:\n",
    "                net_image_bytes = None\n",
    "                custom_cam, do_training, pipe.convert_SHs_python, pipe.compute_cov3D_python, keep_alive, scaling_modifer = network_gui.receive()\n",
    "                if custom_cam != None:\n",
    "                    net_image = render(custom_cam, gaussians, pipe, background, scaling_modifer)[\"render\"]\n",
    "                    net_image_bytes = memoryview((torch.clamp(net_image, min=0, max=1.0) * 255).byte().permute(1, 2, 0).contiguous().cpu().numpy())\n",
    "                network_gui.send(net_image_bytes, dataset.source_path)\n",
    "                if do_training and ((iteration < int(opt.iterations)) or not keep_alive):\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                network_gui.conn = None\n",
    "\n",
    "        iter_start.record()\n",
    "\n",
    "        gaussians.update_learning_rate(iteration)\n",
    "\n",
    "        # Every 1000 its we increase the levels of SH up to a maximum degree\n",
    "        if iteration % 1000 == 0:\n",
    "            gaussians.oneupSHdegree()\n",
    "\n",
    "        # Pick a random Camera\n",
    "        if not viewpoint_stack:\n",
    "            viewpoint_stack = scene.getTrainCameras().copy()\n",
    "        viewpoint_cam: Camera = viewpoint_stack.pop(randint(0, len(viewpoint_stack)-1))\n",
    "\n",
    "        # Render\n",
    "        if (iteration - 1) == debug_from:\n",
    "            pipe.debug = True\n",
    "\n",
    "        render_pkg = render(viewpoint_cam, gaussians, pipe, background)\n",
    "        rendered_image: torch.Tensor\n",
    "        rendered_image, viewspace_point_tensor, visibility_filter, radii = (\n",
    "                                                                    render_pkg[\"render\"], \n",
    "                                                                    render_pkg[\"viewspace_points\"], \n",
    "                                                                    render_pkg[\"visibility_filter\"], \n",
    "                                                                    render_pkg[\"radii\"])\n",
    "        \n",
    "        rendered_mask: torch.Tensor = render_pkg[\"mask\"]\n",
    "        rendered_depth: torch.Tensor = render_pkg[\"depth\"]\n",
    "        rendered_middepth: torch.Tensor = render_pkg[\"middepth\"]\n",
    "        rendered_normal: torch.Tensor = render_pkg[\"normal\"]\n",
    "        depth_distortion: torch.Tensor = render_pkg[\"depth_distortion\"]\n",
    "        \n",
    "\n",
    "        gt_image = viewpoint_cam.original_image\n",
    "        edge = viewpoint_cam.edge\n",
    "\n",
    "        \n",
    "        if dataset.use_decoupled_appearance:\n",
    "            Ll1_render = L1_loss_appearance(rendered_image, gt_image, gaussians, viewpoint_cam.uid)\n",
    "        else:\n",
    "            Ll1_render = l1_loss(rendered_image, gt_image)\n",
    "\n",
    "        \n",
    "        if iteration >= opt.regularization_from_iter:\n",
    "            # depth distortion loss\n",
    "            lambda_distortion = opt.lambda_distortion\n",
    "            depth_distortion = torch.where(rendered_mask>0,depth_distortion/(rendered_mask * rendered_mask).detach(),0)\n",
    "            distortion_map = depth_distortion[0] * edge\n",
    "            distortion_loss = distortion_map.mean()\n",
    "\n",
    "            # normal consistency loss\n",
    "            rendered_depth = rendered_depth / rendered_mask\n",
    "            rendered_depth = torch.nan_to_num(rendered_depth, 0, 0)\n",
    "            depth_middepth_normal, _ = depth_double_to_normal(viewpoint_cam, rendered_depth, rendered_middepth)\n",
    "            depth_ratio = 0.6\n",
    "            rendered_normal = torch.nn.functional.normalize(rendered_normal, p=2, dim=0)\n",
    "            rendered_normal = rendered_normal.permute(1,2,0)\n",
    "            normal_error_map = (1 - (rendered_normal.unsqueeze(0) * depth_middepth_normal).sum(dim=-1))\n",
    "            depth_normal_loss = (1-depth_ratio) * normal_error_map[0].mean() + depth_ratio * normal_error_map[1].mean()\n",
    "            lambda_depth_normal = opt.lambda_depth_normal\n",
    "        else:\n",
    "            lambda_distortion = 0\n",
    "            lambda_depth_normal = 0\n",
    "            distortion_loss = torch.tensor([0],dtype=torch.float32,device=\"cuda\")\n",
    "            depth_normal_loss = torch.tensor([0],dtype=torch.float32,device=\"cuda\")\n",
    "            \n",
    "        rgb_loss = (1.0 - opt.lambda_dssim) * Ll1_render + opt.lambda_dssim * (1.0 - ssim(rendered_image, gt_image.unsqueeze(0)))\n",
    "        \n",
    "        loss = rgb_loss + depth_normal_loss * lambda_depth_normal + distortion_loss * lambda_distortion\n",
    "        loss.backward()\n",
    "\n",
    "        iter_end.record()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Progress bar\n",
    "            ema_loss_for_log = 0.4 * loss.item() + 0.6 * ema_loss_for_log\n",
    "            ema_depth_loss_for_log = 0.4 * distortion_loss.item() + 0.6 * ema_depth_loss_for_log\n",
    "            ema_normal_loss_for_log = 0.4 * depth_normal_loss.item() + 0.6 * ema_normal_loss_for_log\n",
    "\n",
    "            if iteration % 10 == 0:\n",
    "                progress_bar.set_postfix({\"Loss\": f\"{ema_loss_for_log:.{4}f}\", \"loss_dep\": f\"{ema_depth_loss_for_log:.{4}f}\", \"loss_normal\": f\"{ema_normal_loss_for_log:.{4}f}\"})\n",
    "                progress_bar.update(10)\n",
    "            if iteration == opt.iterations:\n",
    "                progress_bar.close()\n",
    "            \n",
    "            # Log and save\n",
    "            training_report(tb_writer, iteration, Ll1_render, loss, distortion_loss, depth_normal_loss, l1_loss, iter_start.elapsed_time(iter_end), testing_iterations, scene, render, (pipe, background))\n",
    "            if (iteration in saving_iterations):\n",
    "                print(\"\\n[ITER {}] Saving Gaussians\".format(iteration))\n",
    "                scene.save(iteration)\n",
    "\n",
    "            # Densification\n",
    "            if iteration < opt.densify_until_iter:\n",
    "                # Keep track of max radii in image-space for pruning\n",
    "                gaussians.max_radii2D[visibility_filter] = torch.max(gaussians.max_radii2D[visibility_filter], radii[visibility_filter])\n",
    "                gaussians.add_densification_stats(viewspace_point_tensor, visibility_filter)\n",
    "\n",
    "                if iteration > opt.densify_from_iter and iteration % opt.densification_interval == 0:\n",
    "                    size_threshold = 20 if iteration > opt.opacity_reset_interval else None\n",
    "                    gaussians.densify_and_prune(opt.densify_grad_threshold, 0.05, scene.cameras_extent, size_threshold)\n",
    "                    gaussians.compute_3D_filter(cameras=trainCameras)\n",
    "\n",
    "                if iteration % opt.opacity_reset_interval == 0 or (dataset.white_background and iteration == opt.densify_from_iter):\n",
    "                    gaussians.reset_opacity()\n",
    "                \n",
    "            if iteration % 100 == 0 and iteration > opt.densify_until_iter:\n",
    "                if iteration < opt.iterations - 100:\n",
    "                    # don't update in the end of training\n",
    "                    gaussians.compute_3D_filter(cameras=trainCameras)\n",
    "\n",
    "            # Optimizer step\n",
    "            if iteration < opt.iterations:\n",
    "                gaussians.optimizer.step()\n",
    "                gaussians.optimizer.zero_grad(set_to_none = True)\n",
    "\n",
    "            if (iteration in checkpoint_iterations):\n",
    "                print(\"\\n[ITER {}] Saving Checkpoint\".format(iteration))\n",
    "                torch.save((gaussians.capture(), iteration), scene.model_path + \"/chkpnt\" + str(iteration) + \".pth\")\n",
    "\n",
    "def prepare_output_and_logger(args):    \n",
    "    if not args.model_path:\n",
    "        if os.getenv('OAR_JOB_ID'):\n",
    "            unique_str=os.getenv('OAR_JOB_ID')\n",
    "        else:\n",
    "            unique_str = str(uuid.uuid4())\n",
    "        args.model_path = os.path.join(\"./output/\", unique_str[0:10])\n",
    "        \n",
    "    # Set up output folder\n",
    "    print(\"Output folder: {}\".format(args.model_path))\n",
    "    os.makedirs(args.model_path, exist_ok = True)\n",
    "    with open(os.path.join(args.model_path, \"cfg_args\"), 'w') as cfg_log_f:\n",
    "        cfg_log_f.write(str(Namespace(**vars(args))))\n",
    "\n",
    "    # Create Tensorboard writer\n",
    "    tb_writer = None\n",
    "    if TENSORBOARD_FOUND:\n",
    "        tb_writer = SummaryWriter(args.model_path)\n",
    "    else:\n",
    "        print(\"Tensorboard not available: not logging progress\")\n",
    "    return tb_writer\n",
    "\n",
    "def training_report(tb_writer, iteration, Ll1, loss, depth_loss, normal_loss, l1_loss, elapsed, testing_iterations, scene : Scene, renderFunc, renderArgs):\n",
    "    if tb_writer:\n",
    "        tb_writer.add_scalar('train_loss_patches/l1_loss', Ll1.item(), iteration)\n",
    "        tb_writer.add_scalar('train_loss_patches/depth_loss', depth_loss.item(), iteration)\n",
    "        tb_writer.add_scalar('train_loss_patches/normal_loss', normal_loss.item(), iteration)\n",
    "        tb_writer.add_scalar('train_loss_patches/total_loss', loss.item(), iteration)\n",
    "        tb_writer.add_scalar('iter_time', elapsed, iteration)\n",
    "\n",
    "    # Report test and samples of training set\n",
    "    if iteration in testing_iterations:\n",
    "        torch.cuda.empty_cache()\n",
    "        validation_configs = ({'name': 'test', 'cameras' : scene.getTestCameras()}, \n",
    "                              {'name': 'train', 'cameras' : [scene.getTrainCameras()[idx % len(scene.getTrainCameras())] for idx in range(5, 30, 5)]})\n",
    "\n",
    "        for config in validation_configs:\n",
    "            if config['cameras'] and len(config['cameras']) > 0:\n",
    "                l1_test = 0.0\n",
    "                psnr_test = 0.0\n",
    "                l1_depth = 0.0\n",
    "                for idx, viewpoint in enumerate(config['cameras']):\n",
    "                    render_result = renderFunc(viewpoint, scene.gaussians, *renderArgs)\n",
    "                    depth = render_result[\"depth\"]\n",
    "                    image = torch.clamp(render_result[\"render\"], 0.0, 1.0)\n",
    "                    gt_image = torch.clamp(viewpoint.original_image, 0.0, 1.0)\n",
    "                    if tb_writer and (idx < 5):\n",
    "                        tb_writer.add_images(config['name'] + \"_view_{}/render\".format(viewpoint.image_name), image[None], global_step=iteration)\n",
    "                        if iteration == testing_iterations[0]:\n",
    "                            tb_writer.add_images(config['name'] + \"_view_{}/ground_truth\".format(viewpoint.image_name), gt_image[None], global_step=iteration)\n",
    "                    l1_test += l1_loss(image, gt_image).mean().double()\n",
    "                    psnr_test += psnr(image, gt_image).mean().double()\n",
    "                psnr_test /= len(config['cameras'])\n",
    "                l1_test /= len(config['cameras'])          \n",
    "                # l1_depth /= len(config['cameras'])  \n",
    "                l1_depth = 0        \n",
    "                print(\"\\n[ITER {}] Evaluating {}: L1 {} PSNR {} depth {}\".format(iteration, config['name'], l1_test, psnr_test, l1_depth))\n",
    "                if config[\"name\"] == \"test\":\n",
    "                    with open(scene.model_path + \"/chkpnt\" + str(iteration) + \".txt\", \"w\") as file_object:\n",
    "                        print(\"\\n[ITER {}] Evaluating {}: L1 {} PSNR {}\".format(iteration, config['name'], l1_test, psnr_test), file=file_object)\n",
    "                if tb_writer:\n",
    "                    tb_writer.add_scalar(config['name'] + '/loss_viewpoint - l1_loss', l1_test, iteration)\n",
    "                    tb_writer.add_scalar(config['name'] + '/loss_viewpoint - psnr', psnr_test, iteration)\n",
    "                    tb_writer.add_scalar(config['name'] + '/loss_viewpoint - l1_depth', l1_depth, iteration)\n",
    "\n",
    "        if tb_writer:\n",
    "            tb_writer.add_histogram(\"scene/opacity_histogram\", scene.gaussians.get_opacity, iteration)\n",
    "            tb_writer.add_scalar('total_points', scene.gaussians.get_xyz.shape[0], iteration)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set up command line argument parser\n",
    "    parser = ArgumentParser(description=\"Training script parameters\")\n",
    "    lp = ModelParams(parser)\n",
    "    op = OptimizationParams(parser)\n",
    "    pp = PipelineParams(parser)\n",
    "    parser.add_argument('--ip', type=str, default=\"127.0.0.1\")\n",
    "    parser.add_argument('--port', type=int, default=6009)\n",
    "    parser.add_argument('--debug_from', type=int, default=-1)\n",
    "    parser.add_argument('--detect_anomaly', action='store_true', default=False)\n",
    "    parser.add_argument(\"--test_iterations\", nargs=\"+\", type=int, default=[7_000, 30_000])\n",
    "    parser.add_argument(\"--save_iterations\", nargs=\"+\", type=int, default=[7_000, 30_000])\n",
    "    parser.add_argument(\"--quiet\", action=\"store_true\")\n",
    "    parser.add_argument(\"--checkpoint_iterations\", nargs=\"+\", type=int, default=[15000])\n",
    "    parser.add_argument(\"--start_checkpoint\", type=str, default = None)\n",
    "    args = parser.parse_args(sys.argv[1:])\n",
    "    args.save_iterations.append(args.iterations)\n",
    "    \n",
    "    print(\"Optimizing \" + args.model_path)\n",
    "\n",
    "    # Initialize system state (RNG)\n",
    "    safe_state(args.quiet)\n",
    "\n",
    "    # Start GUI server, configure and run training\n",
    "    # network_gui.init(args.ip, args.port)\n",
    "    # torch.autograd.set_detect_anomaly(args.detect_anomaly)\n",
    "    training(dataset=lp.extract(args), \n",
    "             opt=op.extract(args), \n",
    "             pipe=pp.extract(args), \n",
    "             testing_iterations=args.test_iterations, \n",
    "             saving_iterations=args.save_iterations, \n",
    "             checkpoint_iterations=args.checkpoint_iterations, \n",
    "             checkpoint=args.start_checkpoint, \n",
    "             debug_from=args.debug_from)\n",
    "\n",
    "    # All done\n",
    "    print(\"\\nTraining complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "959a7f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "Optimizing \n",
      "Output folder: ./output/fa01461e-3 [26/06 16:16:20]\n",
      "Reading camera 64/64 [26/06 16:16:21]\n",
      "cameras extent: 1.7835390806198121 [26/06 16:16:21]\n",
      "Loading Training Cameras: 64 . [26/06 16:16:26]\n",
      "Loading Test Cameras: 0 . [26/06 16:16:26]\n",
      "Number of points at initialisation :  43031 [26/06 16:16:26]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training progress:   0%|          | 0/30000 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 7, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\Desktop\\master project\\RaDe-GS\\train.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    317\u001b[0m              \u001b[0mcheckpoint_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheckpoint_iterations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m              \u001b[0mcheckpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m              debug_from=args.debug_from)\n\u001b[0m\u001b[0;32m    320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m     \u001b[1;31m# All done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\master project\\RaDe-GS\\train.py\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(dataset, opt, pipe, testing_iterations, saving_iterations, checkpoint_iterations, checkpoint, debug_from)\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mpipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[0mrender_pkg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mviewpoint_cam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgaussians\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpipe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackground\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m         \u001b[0mrendered_image\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         rendered_image, viewspace_point_tensor, visibility_filter, radii = (\n",
      "\u001b[1;32m~\\Desktop\\master project\\RaDe-GS\\gaussian_renderer\\__init__.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(viewpoint_camera, pc, pipe, bg_color, scaling_modifier)\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mscales\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscales\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mrotations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrotations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         cov3D_precomp = cov3D_precomp)\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 7, got 2)"
     ]
    }
   ],
   "source": [
    "%run train.py -s \"C:\\Users\\27118\\Desktop\\master project\\RaDe-GS\\dtu\\DTU\\scan114\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918fdba7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
