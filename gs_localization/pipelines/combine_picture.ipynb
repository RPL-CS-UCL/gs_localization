{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "39115ecb-6ee0-43cc-aa48-f1ed30f91759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: Camera(id=1, model='SIMPLE_PINHOLE', width=1008, height=756, params=array([815.13158322, 504.        , 378.        ]))}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import glob\n",
    "import os\n",
    "import h5py\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import trimesh\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from argparse import ArgumentParser\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import yaml\n",
    "import numpy as np\n",
    "from munch import munchify\n",
    "import wandb\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"D:/gs-localization/gaussian_splatting\")\n",
    "sys.path.append(\"D:/gs-localization\")\n",
    "sys.path.append(\"D:/gs-localization/gs_localization/pipelines\")\n",
    "from tools.gaussian_model import GaussianModel\n",
    "from tools.config_utils import load_config, set_config, update_recursive\n",
    "from tools.dataset import v2_360_Dataset\n",
    "from tools import read_write_model\n",
    "from tools.eval_utils import rotation_error, translation_error\n",
    "\n",
    "def set_config(tr_dirs, config):\n",
    "    cameras, _, _ = read_write_model.read_model(tr_dirs, \".bin\")\n",
    "    config[\"Dataset\"][\"dataset_path\"] = tr_dirs\n",
    "    print(cameras)\n",
    "    config[\"Dataset\"][\"Calibration\"][\"fx\"] = cameras[1][4][0]\n",
    "    config[\"Dataset\"][\"Calibration\"][\"fy\"] = cameras[1][4][0]\n",
    "    config[\"Dataset\"][\"Calibration\"][\"cx\"] = cameras[1][4][1]\n",
    "    config[\"Dataset\"][\"Calibration\"][\"cy\"] = cameras[1][4][2]\n",
    "    config[\"Dataset\"][\"Calibration\"][\"width\"] = cameras[1][2]\n",
    "    config[\"Dataset\"][\"Calibration\"][\"height\"] = cameras[1][3]\n",
    "    return config\n",
    "    \n",
    "with open(\"configs/mono/tum/fr3_office.yaml\", \"r\") as f:\n",
    "    cfg_special = yaml.full_load(f)\n",
    "\n",
    "inherit_from = \"configs/mono/tum/base_config.yaml\"\n",
    "\n",
    "if inherit_from is not None:\n",
    "    cfg = load_config(inherit_from)\n",
    "else:\n",
    "    cfg = dict()\n",
    "\n",
    "# merge per dataset cfg. and main cfg.\n",
    "config = update_recursive(cfg, cfg_special)\n",
    "config = cfg\n",
    "    \n",
    "data_folder = \"D:/gs-localization/datasets/nerf_llff_data\"\n",
    "scene = \"fern\"\n",
    "tr_dirs = Path(data_folder) / scene / \"train_views/triangulated\"\n",
    "config = set_config(tr_dirs, config)\n",
    "\n",
    "Model = GaussianModel(3, config)\n",
    "#Model.load_ply(\"C:/Users/27118/Desktop/master_project/RaDe-GS/output/26b22380-1/point_cloud/iteration_30000/point_cloud.ply\")\n",
    "#Model.load_ply(\"D:/gaussian-splatting/output/73bdba8c-0/point_cloud/iteration_25000/point_cloud.ply\")\n",
    "Model.load_ply(f\"D:/gs-localization/output/nerf_llff_data/{scene}/gs_map/iteration_30000/point_cloud.ply\")\n",
    "\n",
    "model_params = munchify(config[\"model_params\"])\n",
    "pipeline_params = munchify(config[\"pipeline_params\"])\n",
    "data_folder = \"D:/gs-localization/datasets/nerf_llff_data\"\n",
    "dataset = v2_360_Dataset(model_params, model_params.source_path, config, data_folder, scene)\n",
    "bg_color = [0, 0, 0] \n",
    "background = torch.tensor(bg_color, dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "from gaussian_splatting.utils.graphics_utils import getProjectionMatrix2, getWorld2View2\n",
    "from tools import render\n",
    "from tools.descent_utils import image_gradient, image_gradient_mask\n",
    "from tools.camera_utils import Camera\n",
    "from tools.descent_utils import get_loss_tracking\n",
    "from tools.pose_utils import update_pose\n",
    "\n",
    "projection_matrix = getProjectionMatrix2(\n",
    "    znear=0.01,\n",
    "    zfar=100.0,\n",
    "    fx=dataset.fx,\n",
    "    fy=dataset.fy,\n",
    "    cx=dataset.cx,\n",
    "    cy=dataset.cy,\n",
    "    W=dataset.width,\n",
    "    H=dataset.height,\n",
    ").transpose(0, 1)\n",
    "projection_matrix = projection_matrix.to(device=\"cuda:0\")\n",
    "\n",
    "config[\"Training\"][\"opacity_threshold\"] = 0.5\n",
    "config[\"Training\"][\"edge_threshold\"] = 0.8\n",
    "from time import time\n",
    "\n",
    "def gradient_decent(viewpoint, config, initial_R, initial_T):\n",
    "\n",
    "    viewpoint.update_RT(initial_R, initial_T)\n",
    "    \n",
    "    opt_params = []\n",
    "    opt_params.append(\n",
    "        {\n",
    "            \"params\": [viewpoint.cam_rot_delta],\n",
    "            \"lr\": 0.0001,\n",
    "            \"name\": \"rot_{}\".format(viewpoint.uid),\n",
    "        }\n",
    "    )\n",
    "    opt_params.append(\n",
    "        {\n",
    "            \"params\": [viewpoint.cam_trans_delta],\n",
    "            \"lr\": 0.001,\n",
    "            \"name\": \"trans_{}\".format(viewpoint.uid),\n",
    "        }\n",
    "    )\n",
    "    opt_params.append(\n",
    "        {\n",
    "            \"params\": [viewpoint.exposure_a],\n",
    "            \"lr\": 0.001,\n",
    "            \"name\": \"exposure_a_{}\".format(viewpoint.uid),\n",
    "        }\n",
    "    )\n",
    "    opt_params.append(\n",
    "        {\n",
    "            \"params\": [viewpoint.exposure_b],\n",
    "            \"lr\": 0.001,\n",
    "            \"name\": \"exposure_b_{}\".format(viewpoint.uid),\n",
    "        }\n",
    "    )\n",
    "    \n",
    "\n",
    "    pose_optimizer = torch.optim.Adam(opt_params)\n",
    "    \n",
    "    for tracking_itr in range(100):\n",
    "        \n",
    "        render_pkg = render(\n",
    "            viewpoint, Model, pipeline_params, background\n",
    "        )\n",
    "        \n",
    "        image, depth, opacity = (\n",
    "            render_pkg[\"render\"],\n",
    "            render_pkg[\"depth\"],\n",
    "            render_pkg[\"opacity\"],\n",
    "        )\n",
    "          \n",
    "        pose_optimizer.zero_grad()\n",
    "        \n",
    "        loss_tracking = get_loss_tracking(\n",
    "            config, image, depth, opacity, viewpoint\n",
    "        )\n",
    "        loss_tracking.backward()\n",
    "        \n",
    "    \n",
    "        with torch.no_grad():\n",
    "            pose_optimizer.step()\n",
    "            converged = update_pose(viewpoint, converged_threshold=1e-5)\n",
    "    \n",
    "        if converged:\n",
    "            break\n",
    "             \n",
    "    return viewpoint.R, viewpoint.T, render_pkg\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class Transformation:\n",
    "    def __init__(self, R=None, T=None):\n",
    "        self.R = R\n",
    "        self.T = T\n",
    "\n",
    "test_infos = defaultdict(Transformation)\n",
    "def quat_to_rotmat(qvec):\n",
    "    qvec = np.array(qvec, dtype=float)\n",
    "    w, x, y, z = qvec\n",
    "    R = np.array([\n",
    "        [1 - 2*y**2 - 2*z**2, 2*x*y - 2*z*w, 2*x*z + 2*y*w],\n",
    "        [2*x*y + 2*z*w, 1 - 2*x**2 - 2*z**2, 2*y*z - 2*x*w],\n",
    "        [2*x*z - 2*y*w, 2*y*z + 2*x*w, 1 - 2*x**2 - 2*y**2]\n",
    "    ])\n",
    "    return R\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e3b608d-eff8-4c8c-9002-c913f4a7b31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\27118/.cache\\torch\\hub\\intel-isl_MiDaS_master\n",
      "D:\\anaconda3\\envs\\gaussian_splatting\\lib\\site-packages\\timm\\models\\_factory.py:117: UserWarning: Mapping deprecated model name vit_base_resnet50_384 to current vit_base_r50_s16_384.orig_in21k_ft_in1k.\n",
      "  model = create_fn(\n",
      "Using cache found in C:\\Users\\27118/.cache\\torch\\hub\\intel-isl_MiDaS_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import glob\n",
    "import os\n",
    "import h5py\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import trimesh\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from argparse import ArgumentParser\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import yaml\n",
    "import numpy as np\n",
    "from munch import munchify\n",
    "import wandb\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"D:/gs-localization/gaussian_splatting\")\n",
    "sys.path.append(\"D:/gs-localization\")\n",
    "sys.path.append(\"D:/gs-localization/gs_localization/pipelines\")\n",
    "from tools.gaussian_model import GaussianModel\n",
    "from tools.config_utils import load_config, set_config, update_recursive\n",
    "from tools.dataset import v2_360_Dataset\n",
    "from tools import read_write_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "156c4792-6e32-436a-9595-1b6b2eed3c57",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'D:/gs-localization/output/7scenes_full_dslam/chess/sfm_superpoint+superglue'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cameras \u001b[38;5;241m=\u001b[39m \u001b[43mread_write_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_cameras_binary\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD:/gs-localization/output/7scenes_full_dslam/chess/sfm_superpoint+superglue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\gs-localization\\gs_localization\\pipelines\\tools\\read_write_model.py:143\u001b[0m, in \u001b[0;36mread_cameras_binary\u001b[1;34m(path_to_model_file)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;124;03msee: src/colmap/scene/reconstruction.cc\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;124;03m    void Reconstruction::WriteCamerasBinary(const std::string& path)\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    void Reconstruction::ReadCamerasBinary(const std::string& path)\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m cameras \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath_to_model_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fid:\n\u001b[0;32m    144\u001b[0m     num_cameras \u001b[38;5;241m=\u001b[39m read_next_bytes(fid, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQ\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_cameras):\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'D:/gs-localization/output/7scenes_full_dslam/chess/sfm_superpoint+superglue'"
     ]
    }
   ],
   "source": [
    "cameras = read_write_model.read_cameras_binary(\"D:/gs-localization/output/7scenes_full_dslam/chess/sfm_superpoint+superglue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ce5403e2-d5d7-4aa8-b48c-ec4c4fd5f4fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40120097-1735-43f1-965a-fff60a48adbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = \"StMarysChurch\"\n",
    "import os\n",
    "import h5py\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import yaml\n",
    "from munch import munchify\n",
    "from math import atan\n",
    "from collections import OrderedDict\n",
    "\n",
    "sys.path.append(\"D:/gs-localization/gaussian_splatting\")\n",
    "sys.path.append(\"D:/gs-localization\")\n",
    "sys.path.append(\"D:/gs-localization/gs_localization/pipelines\")\n",
    "\n",
    "from tools.config_utils import load_config, update_recursive\n",
    "from tools import read_write_model\n",
    "from tools.gaussian_model import GaussianModel\n",
    "from tools import render\n",
    "from tools.camera_utils import Camera\n",
    "from tools.descent_utils import get_loss_tracking\n",
    "from tools.pose_utils import update_pose\n",
    "from tools.graphics_utils import getProjectionMatrix2\n",
    "\n",
    "def gradient_decent(viewpoint, config, initial_R, initial_T):\n",
    "\n",
    "    viewpoint.update_RT(initial_R, initial_T)\n",
    "    \n",
    "    opt_params = []\n",
    "    opt_params.append(\n",
    "        {\n",
    "            \"params\": [viewpoint.cam_rot_delta],\n",
    "            \"lr\": 0.0001,\n",
    "            \"name\": \"rot_{}\".format(viewpoint.uid),\n",
    "        }\n",
    "    )\n",
    "    opt_params.append(\n",
    "        {\n",
    "            \"params\": [viewpoint.cam_trans_delta],\n",
    "            \"lr\": 0.001,\n",
    "            \"name\": \"trans_{}\".format(viewpoint.uid),\n",
    "        }\n",
    "    )\n",
    "    opt_params.append(\n",
    "        {\n",
    "            \"params\": [viewpoint.exposure_a],\n",
    "            \"lr\": 0.001,\n",
    "            \"name\": \"exposure_a_{}\".format(viewpoint.uid),\n",
    "        }\n",
    "    )\n",
    "    opt_params.append(\n",
    "        {\n",
    "            \"params\": [viewpoint.exposure_b],\n",
    "            \"lr\": 0.001,\n",
    "            \"name\": \"exposure_b_{}\".format(viewpoint.uid),\n",
    "        }\n",
    "    )\n",
    "    \n",
    "\n",
    "    pose_optimizer = torch.optim.Adam(opt_params)\n",
    "    \n",
    "    for tracking_itr in range(20):\n",
    "        \n",
    "        render_pkg = render(\n",
    "            viewpoint, Model, pipeline_params, background\n",
    "        )\n",
    "        \n",
    "        image, depth, opacity = (\n",
    "            render_pkg[\"render\"],\n",
    "            render_pkg[\"depth\"],\n",
    "            render_pkg[\"opacity\"],\n",
    "        )\n",
    "          \n",
    "        pose_optimizer.zero_grad()\n",
    "        \n",
    "        loss_tracking = get_loss_tracking(\n",
    "            config, image, depth, opacity, viewpoint\n",
    "        )\n",
    "        loss_tracking.backward()\n",
    "        \n",
    "    \n",
    "        with torch.no_grad():\n",
    "            pose_optimizer.step()\n",
    "            converged = update_pose(viewpoint, converged_threshold=1e-3)\n",
    "    \n",
    "        if converged:\n",
    "            break\n",
    "             \n",
    "    return viewpoint.R, viewpoint.T, render_pkg\n",
    "\n",
    "\n",
    "class Transformation:\n",
    "    def __init__(self, R=None, T=None):\n",
    "        self.R = R\n",
    "        self.T = T\n",
    "\n",
    "def quat_to_rotmat(qvec):\n",
    "    qvec = np.array(qvec, dtype=float)\n",
    "    w, x, y, z = qvec\n",
    "    R = np.array([\n",
    "        [1 - 2*y**2 - 2*z**2, 2*x*y - 2*z*w, 2*x*z + 2*y*w],\n",
    "        [2*x*y + 2*z*w, 1 - 2*x**2 - 2*z**2, 2*y*z - 2*x*w],\n",
    "        [2*x*z - 2*y*w, 2*y*z + 2*x*w, 1 - 2*x**2 - 2*y**2]\n",
    "    ])\n",
    "    return R\n",
    "\n",
    "\n",
    "def focal2fov(focal, pixels):\n",
    "    return 2 * atan(pixels / (2 * focal))\n",
    "\n",
    "def load_pose(pose_txt):\n",
    "    pose = []\n",
    "    with open(pose_txt, 'r') as f:\n",
    "        for line in f:\n",
    "            row = line.strip('\\n').split()\n",
    "            row = [float(c) for c in row]\n",
    "            pose.append(row)\n",
    "    pose = np.array(pose).astype(np.float32)\n",
    "    assert pose.shape == (4,4)\n",
    "    return pose\n",
    "\n",
    "def create_mask(mkpts_lst, width, height, k):\n",
    "    # Initial mask as all False\n",
    "    mask = np.zeros((height, width), dtype=bool)\n",
    "    \n",
    "    # Calculate k radius\n",
    "    half_k = k // 2\n",
    "    \n",
    "    # Iterate through all points\n",
    "    for pt in mkpts_lst:\n",
    "        x, y = int(pt[0]), int(pt[1])\n",
    "        \n",
    "        # Calculate k*k borders\n",
    "        x_min = max(0, x - half_k)\n",
    "        x_max = min(width, x + half_k + 1)\n",
    "        y_min = max(0, y - half_k)\n",
    "        y_max = min(height, y + half_k + 1)\n",
    "        \n",
    "        # Set mask k*k area as True\n",
    "        mask[y_min:y_max, x_min:x_max] = True\n",
    "    \n",
    "    # Shape: (1, height, width)\n",
    "    mask = mask[np.newaxis, :, :]\n",
    "    \n",
    "    return mask\n",
    "\n",
    "class BaseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, args, path, config):\n",
    "        self.args = args\n",
    "        self.path = path\n",
    "        self.config = config\n",
    "        self.device = \"cuda:0\"\n",
    "        self.dtype = torch.float32\n",
    "        self.num_imgs = 999\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_imgs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pass\n",
    "\n",
    "class MonocularDataset(BaseDataset):\n",
    "    def __init__(self, args, path, config):\n",
    "        super().__init__(args, path, config)\n",
    "        calibration = config[\"Dataset\"][\"Calibration\"]\n",
    "\n",
    "        # depth parameters\n",
    "        self.has_depth = True if \"depth_scale\" in calibration.keys() else False\n",
    "        self.depth_scale = calibration[\"depth_scale\"] if self.has_depth else None\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        color_path = self.color_paths[idx]\n",
    "        pose = self.poses[idx]\n",
    "        intrinsic = self.intrinsics[idx]\n",
    "\n",
    "        image = np.array(Image.open(color_path))\n",
    "        depth = None\n",
    "\n",
    "        if self.has_depth:\n",
    "            depth_path = self.depth_paths[idx]\n",
    "            depth = np.array(Image.open(depth_path)) / self.depth_scale\n",
    "\n",
    "        image = (\n",
    "            torch.from_numpy(image / 255.0)\n",
    "            .clamp(0.0, 1.0)\n",
    "            .permute(2, 0, 1)\n",
    "            .to(device=self.device, dtype=self.dtype)\n",
    "        )\n",
    "        pose = torch.from_numpy(pose).to(device=self.device)\n",
    "        intrinsic = torch.from_numpy(intrinsic).to(device=self.device)\n",
    "        projection_matrix = self.projection_matrices[idx].to(device=\"cuda:0\")\n",
    "        \n",
    "        return image, depth, pose, intrinsic, projection_matrix \n",
    "\n",
    "def parse_camera_params(file_path):\n",
    "    camera_params = {}\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            \n",
    "            # Ensure there are at least 8 elements, and the second element is 'PINHOLE'\n",
    "            if len(parts) == 8 and parts[1] == 'PINHOLE':\n",
    "                try:\n",
    "                    # Extract parameters, skipping the second item 'PINHOLE'\n",
    "                    img_name = parts[0]\n",
    "                    w = int(parts[2])\n",
    "                    h = int(parts[3])\n",
    "                    fx = float(parts[4])\n",
    "                    fy = float(parts[5])\n",
    "                    cx = float(parts[6])\n",
    "                    cy = float(parts[7])\n",
    "                    \n",
    "                    # Store in the dictionary\n",
    "                    camera_params[img_name] = {\n",
    "                        'fx': fx,\n",
    "                        'fy': fy,\n",
    "                        'w': w,\n",
    "                        'h': h,\n",
    "                        'cx': cx,\n",
    "                        'cy': cy\n",
    "                    }\n",
    "                except ValueError as e:\n",
    "                    print(f\"Error parsing line: {line}. Error: {e}\")\n",
    "            else:\n",
    "                print(f\"Line skipped due to incorrect format: {line}\")\n",
    "    \n",
    "    return camera_params\n",
    "    \n",
    "class cambridge_Dataset(MonocularDataset):\n",
    "    def __init__(self, args, path, config, data_folder, scene):\n",
    "        super().__init__(args, path, config)\n",
    "        self.has_depth = False\n",
    "        self.cambridge_Parser(data_folder, scene) \n",
    "        \n",
    "    def cambridge_Parser(self, data_folder, scene):\n",
    "        self.color_paths, self.poses, self.depth_paths, self.intrinsics, self.projection_matrices = [], [], [], [], []\n",
    "\n",
    "        gt_dirs = Path(data_folder) / f\"CambridgeLandmarks_Colmap_Retriangulated_1024px/{scene}/empty_all\"\n",
    "        _, images, _ = read_write_model.read_model(gt_dirs, \".txt\")\n",
    "        output_folder = data_folder.replace(\"datasets\", \"output\")\n",
    "        \n",
    "        # Read the filenames from test_fewshot.txt and store them in a set.\n",
    "        test_images_path = Path(data_folder) / scene / \"test_full.txt\"\n",
    "        \n",
    "        with open(test_images_path, 'r') as f:\n",
    "            test_images = set(line.strip() for line in f)\n",
    "\n",
    "        intrinsics = parse_camera_params(f\"D:/gs-localization/output/cambridge_full/{scene}/query_list_with_intrinsics.txt\")\n",
    "\n",
    "        for i, image in tqdm(images.items(),\"Load dataset\"):\n",
    "            image_name = image.name.replace(\"/\",\"_\")\n",
    "            # Execute the following operation only if image.name exists in test_images.\"\n",
    "            if image_name in test_images:\n",
    "                image_path = Path(data_folder) / scene / 'images_full' / image_name\n",
    "                self.color_paths.append(image_path)\n",
    "                R_gt, t_gt = image.qvec2rotmat(), image.tvec\n",
    "                pose = np.eye(4)            \n",
    "                pose[:3, :3] = R_gt         \n",
    "                pose[:3, 3] = t_gt \n",
    "                self.poses.append(pose)\n",
    "                self.depth_paths.append(None)\n",
    "                \n",
    "                intrinsic = intrinsics[image_name]     \n",
    "                projection_matrix = getProjectionMatrix2(\n",
    "                    znear=0.01,\n",
    "                    zfar=100.0,\n",
    "                    fx=intrinsic[\"fx\"],\n",
    "                    fy=intrinsic[\"fy\"],\n",
    "                    cx=intrinsic[\"cx\"],\n",
    "                    cy=intrinsic[\"cy\"],\n",
    "                    W=intrinsic[\"w\"],\n",
    "                    H=intrinsic[\"h\"],\n",
    "                ).transpose(0, 1)\n",
    "    \n",
    "                self.intrinsics.append(np.array([\n",
    "                                       intrinsic[\"fx\"],\n",
    "                                       intrinsic[\"fy\"],\n",
    "                                       intrinsic[\"cx\"],\n",
    "                                       intrinsic[\"cy\"],\n",
    "                                       focal2fov(intrinsic[\"fx\"], intrinsic[\"w\"]),\n",
    "                                       focal2fov(intrinsic[\"fy\"], intrinsic[\"h\"]),\n",
    "                                       intrinsic[\"h\"], \n",
    "                                       intrinsic[\"w\"]\n",
    "                                        ]))\n",
    "                \n",
    "                self.projection_matrices.append(projection_matrix)\n",
    "\n",
    "\n",
    "        # Sort self.color_paths, self.poses, and self.depth_paths based on normal file name order\n",
    "        sorted_data = sorted(zip(self.color_paths, self.depth_paths, self.poses, \n",
    "                                 self.intrinsics, self.projection_matrices), key=lambda x: x[0].name)\n",
    "        self.color_paths, self.depth_paths, self.poses, self.intrinsics, self.projection_matrices = zip(*sorted_data)\n",
    "        del images\n",
    "\n",
    "with open(\"D:/gs-localization/gs_localization/pipelines/configs/mono/tum/fr3_office.yaml\", \"r\") as f:\n",
    "    cfg_special = yaml.full_load(f)\n",
    "\n",
    "inherit_from = \"D:/gs-localization/gs_localization/pipelines/configs/mono/tum/base_config.yaml\"\n",
    "\n",
    "if inherit_from is not None:\n",
    "    cfg = load_config(inherit_from)\n",
    "else:\n",
    "    cfg = dict()\n",
    "\n",
    "# merge per dataset cfg. and main cfg.\n",
    "data_folder = \"D:/gs-localization/datasets/cambridge\"\n",
    "config = update_recursive(cfg, cfg_special)\n",
    "config = cfg\n",
    "config[\"Training\"][\"monocular\"] = True\n",
    "config[\"Training\"][\"opacity_threshold\"] = 0.99\n",
    "config[\"Training\"][\"edge_threshold\"] = 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43b5942f-87bd-4914-afff-5ea55982bc75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'StMarysChurch'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2bb1a463-0549-4438-8432-b68f8ea2c731",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load dataset: 100%|█████████████████████████████████████████████████████████████| 2017/2017 [00:00<00:00, 62004.63it/s]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    Model = GaussianModel(3, config)\n",
    "    Model.load_ply(f\"D:/gs-localization/output/cambridge_full/{scene}/gs_map/iteration_30000/point_cloud.ply\")\n",
    "except:\n",
    "    Model = GaussianModel(1, config)\n",
    "    Model.load_ply(f\"D:/gs-localization/output/cambridge_full/{scene}/gs_map/iteration_30000/point_cloud.ply\")\n",
    "\n",
    "model_params = munchify(config[\"model_params\"])\n",
    "pipeline_params = munchify(config[\"pipeline_params\"])\n",
    "data_folder = \"D:/gs-localization/datasets/cambridge\"\n",
    "dataset = cambridge_Dataset(model_params, model_params.source_path, config, data_folder, scene)\n",
    "bg_color = [0, 0, 0] \n",
    "background = torch.tensor(bg_color, dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "# use OrderedDict to substitute defaultdict\n",
    "test_infos = OrderedDict()\n",
    "\n",
    "# suppose file open and read\n",
    "with open(f\"D:/gs-localization/output/cambridge_full/{scene}/results_sparse.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        name = parts[0]\n",
    "        qvec = list(map(float, parts[1:5]))\n",
    "        tvec = list(map(float, parts[5:8]))\n",
    "\n",
    "        R = quat_to_rotmat(qvec)\n",
    "        T = np.array(tvec)\n",
    "\n",
    "        # insert directly in OrderedDict\n",
    "        test_infos[name] = Transformation(R=R, T=T)\n",
    "\n",
    "# sort OrderedDict according to name \n",
    "test_infos = OrderedDict(sorted(test_infos.items(), key=lambda item: item[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c161f21d-d8e2-4eb6-9d69-71b1c9ba1727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StMarysChurch\n",
      "21\n",
      "最接近中值错误的图片名字: seq13_frame00176.png\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(scene)\n",
    "# 加载 trans_errors.npy 文件\n",
    "trans_errors_path = f'D:/gs-localization/output/cambridge_full/{scene}/trans_errors.npy'\n",
    "trans_errors = np.load(trans_errors_path)\n",
    "\n",
    "# 计算中值\n",
    "median_value = np.median(trans_errors)\n",
    "\n",
    "# 找到最接近中值的索引\n",
    "median_index = np.argmin(np.abs(trans_errors+0.00 - median_value))\n",
    "print(median_index)\n",
    "\n",
    "# 打开 results_sparse.txt 文件并提取对应行的名字\n",
    "results_path = f'D:/gs-localization/output/cambridge_full/{scene}/results_sparse.txt'\n",
    "with open(results_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# 提取每一行的第一个字段（假设是名字）\n",
    "names = [line.split()[0] for line in lines]\n",
    "\n",
    "# 根据找到的索引获取对应的名字\n",
    "median_name = names[median_index]\n",
    "\n",
    "# 输出结果\n",
    "print(f\"最接近中值错误的图片名字: {median_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ba3874-c1ce-4b18-9e9c-73624e8ce164",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe9e057-efa6-45ad-8f8a-845fe66528da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ef400d21-a592-4a3c-a023-f8760d0ee3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\27118\\AppData\\Local\\Temp\\ipykernel_15244\\1048684061.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  initial_R = torch.tensor(viewpoint.R_gt)\n",
      "C:\\Users\\27118\\AppData\\Local\\Temp\\ipykernel_15244\\1048684061.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  initial_T = torch.tensor(viewpoint.T_gt).squeeze()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "???\n",
      "???\n"
     ]
    }
   ],
   "source": [
    "# suppose file open and read\n",
    "with open(f\"D:/gs-localization/output/cambridge_full/{scene}/results_sparse.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        name = parts[0]\n",
    "        qvec = list(map(float, parts[1:5]))\n",
    "        tvec = list(map(float, parts[5:8]))\n",
    "\n",
    "        R = quat_to_rotmat(qvec)\n",
    "        T = np.array(tvec)\n",
    "\n",
    "        # insert directly in OrderedDict\n",
    "        test_infos[name] = Transformation(R=R, T=T)\n",
    "\n",
    "# sort OrderedDict according to name \n",
    "test_infos = OrderedDict(sorted(test_infos.items(), key=lambda item: item[0]))\n",
    "\n",
    "file = h5py.File(f'D:/gs-localization/output/cambridge_full/{scene}/feats-superpoint-n4096-r1024.h5', 'r')\n",
    "\n",
    "i = median_index\n",
    "\n",
    "image = median_name\n",
    "viewpoint = Camera.init_from_intrinsic(dataset, i)\n",
    "viewpoint.compute_grad_mask(config)\n",
    "viewpoint.R = viewpoint.R_gt\n",
    "viewpoint.T = viewpoint.T_gt\n",
    "initial_R = torch.tensor(viewpoint.R_gt)\n",
    "initial_T = torch.tensor(viewpoint.T_gt).squeeze()\n",
    "print(\"???\")\n",
    "gradient_decent(viewpoint, config, initial_R, initial_T)\n",
    "print(\"???\")\n",
    "\n",
    "file.close()\n",
    "\n",
    "render_pkg = render(\n",
    "            viewpoint, Model, pipeline_params, background\n",
    "        )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6ab0b216-b905-4ceb-86a9-de8260c1d5bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\27118\\AppData\\Local\\Temp\\ipykernel_15244\\3520195267.py:33: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
      "  ground_truth_image = ground_truth_image.resize((target_width, new_height_gt), Image.ANTIALIAS)\n",
      "C:\\Users\\27118\\AppData\\Local\\Temp\\ipykernel_15244\\3520195267.py:34: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
      "  localized_image = localized_image.resize((target_width, new_height_loc), Image.ANTIALIAS)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 假设 viewpoint.original_image 和 render_pkg[\"render\"] 是 Tensor\n",
    "ground_truth_tensor = viewpoint.original_image\n",
    "localized_tensor = render_pkg[\"render\"]\n",
    "\n",
    "# 找到每个像素中 R、G、B 三个通道的最大值\n",
    "max_vals, _ = localized_tensor.max(dim=0)  # 得到每个像素的最大值 (H, W)\n",
    "\n",
    "# 找到哪些像素的最大值超过 1\n",
    "exceeds_one_mask = max_vals > 1  # 布尔掩码，标记哪些像素的最大值超过 1\n",
    "\n",
    "# 对超过 1 的地方，将 R、G、B 值同时按最大值进行归一化\n",
    "localized_tensor[:, exceeds_one_mask] = localized_tensor[:, exceeds_one_mask] / (max_vals[exceeds_one_mask] + 0.00001)\n",
    "\n",
    "# 将 Tensor 转换为 PIL 图像\n",
    "tensor_to_pil = transforms.ToPILImage()\n",
    "\n",
    "ground_truth_image = tensor_to_pil(ground_truth_tensor)\n",
    "localized_image = tensor_to_pil(localized_tensor)\n",
    "\n",
    "# 设定目标宽度为 960，调整图像大小，保持比例\n",
    "target_width = 960\n",
    "max_height = 480\n",
    "\n",
    "# 调整 ground_truth_image 和 localized_image 的宽度为 960，高度按比例缩放\n",
    "new_height_gt = int(ground_truth_image.height * target_width / ground_truth_image.width)\n",
    "new_height_loc = int(localized_image.height * target_width / localized_image.width)\n",
    "\n",
    "ground_truth_image = ground_truth_image.resize((target_width, new_height_gt), Image.ANTIALIAS)\n",
    "localized_image = localized_image.resize((target_width, new_height_loc), Image.ANTIALIAS)\n",
    "\n",
    "# 如果调整后的高度超过 480，则裁剪掉多余的部分\n",
    "if new_height_gt > max_height:\n",
    "    top_gt = (new_height_gt - max_height) // 2  # 从中间裁剪\n",
    "    ground_truth_image = ground_truth_image.crop((0, top_gt, target_width, top_gt + max_height))\n",
    "\n",
    "if new_height_loc > max_height:\n",
    "    top_loc = (new_height_loc - max_height) // 2  # 从中间裁剪\n",
    "    localized_image = localized_image.crop((0, top_loc, target_width, top_loc + max_height))\n",
    "\n",
    "# 确保两张图片大小相同\n",
    "width, height = ground_truth_image.size\n",
    "\n",
    "# 创建一个新的空白图像，用来合成 ground truth 和 localized image\n",
    "combined_image = Image.new('RGB', (width, height))\n",
    "\n",
    "# 将图像转换为 NumPy 数组，方便逐像素操作\n",
    "ground_truth_array = np.array(ground_truth_image)\n",
    "localized_image_array = np.array(localized_image)\n",
    "\n",
    "# 根据条件 x < ay 来合成图像\n",
    "for y in range(height):\n",
    "    for x in range(width):\n",
    "        if x > (y * (width / height)):  # 根据比例 x < ay 来判断\n",
    "            combined_image.putpixel((x, y), tuple(localized_image_array[y, x]))  # 放置 localized image\n",
    "        else:\n",
    "            combined_image.putpixel((x, y), tuple(ground_truth_array[y, x]))  # 放置 ground truth\n",
    "\n",
    "# 绘制白色虚线对角线\n",
    "draw = ImageDraw.Draw(combined_image)\n",
    "line_length = 20  # 每个虚线段的长度\n",
    "gap_length = 10   # 每段虚线之间的间隔\n",
    "\n",
    "# 计算对角线的总长度\n",
    "diagonal_length = int((width**2 + height**2)**0.5)\n",
    "\n",
    "# 循环绘制虚线\n",
    "for i in range(0, diagonal_length, line_length + gap_length):\n",
    "    start_x = int(i * (width / diagonal_length))  # 起点 x\n",
    "    start_y = int(i * (height / diagonal_length))  # 起点 y\n",
    "    end_x = int((i + line_length) * (width / diagonal_length))  # 终点 x\n",
    "    end_y = int((i + line_length) * (height / diagonal_length))  # 终点 y\n",
    "\n",
    "    # 绘制虚线的段\n",
    "    draw.line((start_x, start_y, end_x, end_y), fill=\"white\", width=3)\n",
    "\n",
    "# 画小框\n",
    "small_box_start = (200, 100)  # 小框左上角起始点 (x, y)\n",
    "small_box_width = 180         # 小框的宽度\n",
    "small_box_height = 120        # 小框的高度\n",
    "small_box_end = (small_box_start[0] + small_box_width, small_box_start[1] + small_box_height)\n",
    "\n",
    "# 绘制绿色小框\n",
    "draw.rectangle([small_box_start, small_box_end], outline=\"green\", width=2)\n",
    "\n",
    "# 提取小框中的部分\n",
    "small_box_region = combined_image.crop((small_box_start[0], small_box_start[1], small_box_end[0], small_box_end[1]))\n",
    "\n",
    "# 放大小框中的部分\n",
    "scale_factor = 1.6  # 放大倍数\n",
    "large_box_region = small_box_region.resize((int(small_box_width * scale_factor), int(small_box_height * scale_factor)))\n",
    "\n",
    "# 将放大的大框放置在小框旁边，覆盖图片部分区域\n",
    "large_box_start_x = small_box_start[0]   # 小框右边再加10像素\n",
    "large_box_start_y = small_box_start[1] + small_box_width - 20\n",
    "\n",
    "# 确保大框不会超出图片边界\n",
    "if large_box_start_x + large_box_region.width > width:\n",
    "    large_box_start_x = width - large_box_region.width - 10\n",
    "if large_box_start_y + large_box_region.height > height:\n",
    "    large_box_start_y = height - large_box_region.height - 10\n",
    "\n",
    "# 将放大的区域粘贴回原图中\n",
    "combined_image.paste(large_box_region, (large_box_start_x, large_box_start_y))\n",
    "\n",
    "# 画大框\n",
    "large_box_end = (large_box_start_x + large_box_region.width, large_box_start_y + large_box_region.height)\n",
    "draw.rectangle([large_box_start_x, large_box_start_y, large_box_end[0], large_box_end[1]], outline=\"green\", width=3)\n",
    "\n",
    "# 保存图片到指定路径\n",
    "output_dir = \"C:/Users/27118/Desktop\"\n",
    "combined_image.save(os.path.join(output_dir, f\"{scene}_compare.png\"))\n",
    "combined_image.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73944eef-0619-488e-b3f4-062adc1000ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "620b93aa-f042-4ef0-9cf8-877c75930b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# 场景顺序\n",
    "scenes = [\"chess\", \"fire\", \"heads\", \"office\", \"pumpkin\", \"redkitchen\", \"stairs\", \"StMarysChurch\", \"KingsCollege\"]\n",
    "\n",
    "# 图片路径模板\n",
    "image_paths = [f\"C:/Users/27118/Desktop/{scene}_compare.png\" for scene in scenes]\n",
    "\n",
    "# 打开所有图片，并获取每张图片的宽高信息\n",
    "images = [Image.open(img_path) for img_path in image_paths]\n",
    "heights = [img.size[1] for img in images]  # 获取所有图片的高度\n",
    "height = min(heights)  # 使用最小的高度来保证一致性\n",
    "\n",
    "# 将所有图片调整为相同高度，保持宽高比例\n",
    "resized_images = [img.resize((int(img.size[0] * height / img.size[1]), height)) for img in images]\n",
    "\n",
    "# 计算每行的总宽度\n",
    "top_row_width = sum([img.size[0] for img in resized_images[:5]])  # 前5张图片在第一行\n",
    "bottom_row_width = sum([img.size[0] for img in resized_images[5:]])  # 后4张图片在第二行\n",
    "\n",
    "# 确定最终图像的总宽高\n",
    "final_width = max(top_row_width, bottom_row_width)\n",
    "final_height = height * 2  # 总高度为两行图片的高度之和\n",
    "\n",
    "# 创建一个新的空白图像，背景为白色\n",
    "new_image = Image.new('RGB', (final_width, final_height), color='white')\n",
    "\n",
    "# 逐一粘贴图片到新图像中\n",
    "x_offset = 0\n",
    "for img in resized_images[:5]:  # 上面5张图片\n",
    "    new_image.paste(img, (x_offset, 0))  # 粘贴到第一行\n",
    "    x_offset += img.size[0]  # 更新x偏移量\n",
    "\n",
    "x_offset = 0\n",
    "for img in resized_images[5:]:  # 下面4张图片\n",
    "    new_image.paste(img, (x_offset, height))  # 粘贴到第二行\n",
    "    x_offset += img.size[0]  # 更新x偏移量\n",
    "\n",
    "# 显示合成后的图像\n",
    "new_image.show()\n",
    "\n",
    "# 保存最终合成图像\n",
    "new_image.save(\"C:/Users/27118/Desktop/combined_image.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc0ab640-4424-4311-8a6e-6570e4ec1c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48435812485309393\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 指定.npy文件的路径\n",
    "f\"D:/gs-localization/output/7scenes/{scene}/results_sparse.txt\"\n",
    "file_path = 'D:/gs-localization/output/7scenes/{scene}/rot_errors.npy'\n",
    "\n",
    "# 加载.npy文件\n",
    "data = np.load(file_path)\n",
    "\n",
    "# 查看文件内容\n",
    "print(np.median(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19378a46-5737-4330-8661-eefee72db101",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
