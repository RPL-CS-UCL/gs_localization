{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58901cff-4b30-4d55-96f8-e5cc02ae7445",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "DSC07957.JPG 0.0 0.0008846196915854965\n",
      "DSC07961.JPG 0.019026952505869246 0.0005690231295027022\n",
      "DSC07965.JPG 0.0 0.0021364254783642505\n",
      "DSC07969.JPG 0.0 0.0007669433324663702\n",
      "DSC07973.JPG 0.0086168832633178 0.0030191548687424873\n",
      "DSC07977.JPG 0.0 0.0011863078868375827\n",
      "DSC07981.JPG 0.026348006084677758 0.0016592148405787709\n",
      "DSC07985.JPG 0.01996937708652734 0.0013287119768531329\n",
      "DSC07989.JPG 0.0 0.0007050134270544847\n",
      "DSC07993.JPG 0.006475060989470229 0.0018918306607795987\n",
      "DSC07997.JPG 0.0 0.0005930749944654968\n",
      "DSC08001.JPG 0.0 0.0012311268078979893\n",
      "DSC08005.JPG 0.021957261566761626 0.0006743152277777875\n",
      "DSC08009.JPG 0.017490494504312483 0.0006242455519888847\n",
      "DSC08013.JPG 0.0 0.002412824363649014\n",
      "DSC08017.JPG 0.0 0.0015544645504503693\n",
      "DSC08021.JPG 0.022100867825541794 0.0017316720748430004\n",
      "DSC08025.JPG 0.04740091702305817 0.00376938449100659\n",
      "DSC08029.JPG 0.028746957505633036 0.001050986087683523\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 237\u001b[0m\n\u001b[0;32m    234\u001b[0m initial_R \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(test_infos[image]\u001b[38;5;241m.\u001b[39mR)\n\u001b[0;32m    235\u001b[0m initial_T \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(test_infos[image]\u001b[38;5;241m.\u001b[39mT)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m--> 237\u001b[0m rotation_matrix, translation_vector, render_pkg \u001b[38;5;241m=\u001b[39m \u001b[43mgradient_decent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mviewpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_R\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_T\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;66;03m#rotation_matrix, translation_vector = initial_R, initial_T\u001b[39;00m\n\u001b[0;32m    240\u001b[0m rot_error \u001b[38;5;241m=\u001b[39m rotation_error(rotation_matrix\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), viewpoint\u001b[38;5;241m.\u001b[39mR_gt\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "Cell \u001b[1;32mIn[1], line 147\u001b[0m, in \u001b[0;36mgradient_decent\u001b[1;34m(viewpoint, config, initial_R, initial_T)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    146\u001b[0m     pose_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m--> 147\u001b[0m     converged \u001b[38;5;241m=\u001b[39m \u001b[43mupdate_pose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mviewpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconverged_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m converged:\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mD:\\gs-localization\\gs_localization\\pipelines\\tools\\pose_utils.py:112\u001b[0m, in \u001b[0;36mupdate_pose\u001b[1;34m(camera, converged_threshold)\u001b[0m\n\u001b[0;32m    109\u001b[0m T_w2c[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m=\u001b[39m camera\u001b[38;5;241m.\u001b[39mR\n\u001b[0;32m    110\u001b[0m T_w2c[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m=\u001b[39m camera\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m--> 112\u001b[0m new_w2c \u001b[38;5;241m=\u001b[39m \u001b[43mSE3_exp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtau\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m@\u001b[39m T_w2c\n\u001b[0;32m    114\u001b[0m new_R \u001b[38;5;241m=\u001b[39m new_w2c[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m3\u001b[39m]\n\u001b[0;32m    115\u001b[0m new_T \u001b[38;5;241m=\u001b[39m new_w2c[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m]\n",
      "File \u001b[1;32mD:\\gs-localization\\gs_localization\\pipelines\\tools\\pose_utils.py:96\u001b[0m, in \u001b[0;36mSE3_exp\u001b[1;34m(tau)\u001b[0m\n\u001b[0;32m     94\u001b[0m rho \u001b[38;5;241m=\u001b[39m tau[:\u001b[38;5;241m3\u001b[39m]\n\u001b[0;32m     95\u001b[0m theta \u001b[38;5;241m=\u001b[39m tau[\u001b[38;5;241m3\u001b[39m:]\n\u001b[1;32m---> 96\u001b[0m R \u001b[38;5;241m=\u001b[39m \u001b[43mSO3_exp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m t \u001b[38;5;241m=\u001b[39m V(theta) \u001b[38;5;241m@\u001b[39m rho\n\u001b[0;32m     99\u001b[0m T \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meye(\u001b[38;5;241m4\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[1;32mD:\\gs-localization\\gs_localization\\pipelines\\tools\\pose_utils.py:62\u001b[0m, in \u001b[0;36mSO3_exp\u001b[1;34m(theta)\u001b[0m\n\u001b[0;32m     60\u001b[0m angle \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(theta)\n\u001b[0;32m     61\u001b[0m I \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meye(\u001b[38;5;241m3\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m---> 62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mangle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-5\u001b[39;49m:\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m I \u001b[38;5;241m+\u001b[39m W \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m W2\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import glob\n",
    "import os\n",
    "import h5py\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import trimesh\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from argparse import ArgumentParser\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "import yaml\n",
    "import numpy as np\n",
    "from munch import munchify\n",
    "import wandb\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"D:/gs-localization/gaussian_splatting\")\n",
    "sys.path.append(\"D:/gs-localization\")\n",
    "from tools.gaussian_model import GaussianModel\n",
    "from gaussian_splatting.utils.system_utils import mkdir_p\n",
    "from tools.config_utils import load_config, set_config, update_recursive\n",
    "from tools.dataset import v2_360_Dataset\n",
    "from tools.multiprocessing_utils import FakeQueue\n",
    "from tools import read_write_model\n",
    "from tools.eval_utils import rotation_error, translation_error\n",
    "\n",
    "\n",
    "with open(\"configs/mono/tum/fr3_office.yaml\", \"r\") as f:\n",
    "    cfg_special = yaml.full_load(f)\n",
    "\n",
    "inherit_from = \"configs/mono/tum/base_config.yaml\"\n",
    "\n",
    "if inherit_from is not None:\n",
    "    cfg = load_config(inherit_from)\n",
    "else:\n",
    "    cfg = dict()\n",
    "\n",
    "# merge per dataset cfg. and main cfg.\n",
    "config = update_recursive(cfg, cfg_special)\n",
    "config = cfg\n",
    "    \n",
    "data_folder = \"D:/gs-localization/datasets/360_v2\"\n",
    "scene = \"garden\"\n",
    "tr_dirs = Path(data_folder) / scene / \"train_views/triangulated\"\n",
    "config = set_config(tr_dirs, config)\n",
    "\n",
    "Model = GaussianModel(3, config)\n",
    "#Model.load_ply(\"C:/Users/27118/Desktop/master_project/RaDe-GS/output/26b22380-1/point_cloud/iteration_30000/point_cloud.ply\")\n",
    "#Model.load_ply(\"D:/gaussian-splatting/output/73bdba8c-0/point_cloud/iteration_25000/point_cloud.ply\")\n",
    "Model.load_ply(f\"D:/gs-localization/output/360_v2/{scene}/gs_map/iteration_20000/point_cloud.ply\")\n",
    "\n",
    "model_params = munchify(config[\"model_params\"])\n",
    "pipeline_params = munchify(config[\"pipeline_params\"])\n",
    "data_folder = \"D:/gs-localization/datasets/360_v2\"\n",
    "dataset = v2_360_Dataset(model_params, model_params.source_path, config, data_folder, scene)\n",
    "bg_color = [0, 0, 0] \n",
    "background = torch.tensor(bg_color, dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "from gaussian_splatting.utils.graphics_utils import getProjectionMatrix2, getWorld2View2\n",
    "from tools import render\n",
    "from tools.slam_utils import image_gradient, image_gradient_mask\n",
    "from tools.camera_utils import Camera\n",
    "from tools.slam_utils import get_loss_tracking, get_median_depth\n",
    "from tools.pose_utils import update_pose\n",
    "\n",
    "projection_matrix = getProjectionMatrix2(\n",
    "    znear=0.01,\n",
    "    zfar=100.0,\n",
    "    fx=dataset.fx,\n",
    "    fy=dataset.fy,\n",
    "    cx=dataset.cx,\n",
    "    cy=dataset.cy,\n",
    "    W=dataset.width,\n",
    "    H=dataset.height,\n",
    ").transpose(0, 1)\n",
    "projection_matrix = projection_matrix.to(device=\"cuda:0\")\n",
    "\n",
    "config[\"Training\"][\"opacity_threshold\"] = 0.99\n",
    "config[\"Training\"][\"edge_threshold\"] = 1.1\n",
    "from time import time\n",
    "\n",
    "def gradient_decent(viewpoint, config, initial_R, initial_T):\n",
    "\n",
    "    viewpoint.update_RT(initial_R, initial_T)\n",
    "    \n",
    "    opt_params = []\n",
    "    opt_params.append(\n",
    "        {\n",
    "            \"params\": [viewpoint.cam_rot_delta],\n",
    "            \"lr\": 0.0001,\n",
    "            \"name\": \"rot_{}\".format(viewpoint.uid),\n",
    "        }\n",
    "    )\n",
    "    opt_params.append(\n",
    "        {\n",
    "            \"params\": [viewpoint.cam_trans_delta],\n",
    "            \"lr\": 0.001,\n",
    "            \"name\": \"trans_{}\".format(viewpoint.uid),\n",
    "        }\n",
    "    )\n",
    "    opt_params.append(\n",
    "        {\n",
    "            \"params\": [viewpoint.exposure_a],\n",
    "            \"lr\": 0.001,\n",
    "            \"name\": \"exposure_a_{}\".format(viewpoint.uid),\n",
    "        }\n",
    "    )\n",
    "    opt_params.append(\n",
    "        {\n",
    "            \"params\": [viewpoint.exposure_b],\n",
    "            \"lr\": 0.001,\n",
    "            \"name\": \"exposure_b_{}\".format(viewpoint.uid),\n",
    "        }\n",
    "    )\n",
    "    \n",
    "\n",
    "    pose_optimizer = torch.optim.Adam(opt_params)\n",
    "    \n",
    "    for tracking_itr in range(100):\n",
    "        \n",
    "        render_pkg = render(\n",
    "            viewpoint, Model, pipeline_params, background\n",
    "        )\n",
    "        \n",
    "        image, depth, opacity = (\n",
    "            render_pkg[\"render\"],\n",
    "            render_pkg[\"depth\"],\n",
    "            render_pkg[\"opacity\"],\n",
    "        )\n",
    "          \n",
    "        pose_optimizer.zero_grad()\n",
    "        \n",
    "        loss_tracking = get_loss_tracking(\n",
    "            config, image, depth, opacity, viewpoint\n",
    "        )\n",
    "        loss_tracking.backward()\n",
    "        \n",
    "    \n",
    "        with torch.no_grad():\n",
    "            pose_optimizer.step()\n",
    "            converged = update_pose(viewpoint, converged_threshold=1e-4)\n",
    "    \n",
    "        if converged:\n",
    "            break\n",
    "             \n",
    "    return viewpoint.R, viewpoint.T, render_pkg\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class Transformation:\n",
    "    def __init__(self, R=None, T=None):\n",
    "        self.R = R\n",
    "        self.T = T\n",
    "\n",
    "test_infos = defaultdict(Transformation)\n",
    "\n",
    "def quat_to_rotmat(qvec):\n",
    "    qvec = np.array(qvec, dtype=float)\n",
    "    w, x, y, z = qvec\n",
    "    R = np.array([\n",
    "        [1 - 2*y**2 - 2*z**2, 2*x*y - 2*z*w, 2*x*z + 2*y*w],\n",
    "        [2*x*y + 2*z*w, 1 - 2*x**2 - 2*z**2, 2*y*z - 2*x*w],\n",
    "        [2*x*z - 2*y*w, 2*y*z + 2*x*w, 1 - 2*x**2 - 2*y**2]\n",
    "    ])\n",
    "    return R\n",
    "\n",
    "with open(f\"D:/gs-localization/output/360_v2/{scene}/results_sparse.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        name = parts[0]\n",
    "        qvec = list(map(float, parts[1:5]))\n",
    "        tvec = list(map(float, parts[5:8]))\n",
    "\n",
    "        R = quat_to_rotmat(qvec)\n",
    "        T = np.array(tvec)\n",
    "\n",
    "        test_infos[name].R = R\n",
    "        test_infos[name].T = T\n",
    "\n",
    "\n",
    "def create_mask(mkpts_lst, width, height, k):\n",
    "    # 初始化 mask，全为 False\n",
    "    mask = np.zeros((height, width), dtype=bool)\n",
    "    \n",
    "    # 计算 k 的半径\n",
    "    half_k = k // 2\n",
    "    \n",
    "    # 遍历所有点\n",
    "    for pt in mkpts_lst:\n",
    "        x, y = int(pt[0]), int(pt[1])\n",
    "        \n",
    "        # 计算 k*k 区域的边界\n",
    "        x_min = max(0, x - half_k)\n",
    "        x_max = min(width, x + half_k + 1)\n",
    "        y_min = max(0, y - half_k)\n",
    "        y_max = min(height, y + half_k + 1)\n",
    "        \n",
    "        # 设置 mask 中的 k*k 区域为 True\n",
    "        mask[y_min:y_max, x_min:x_max] = True\n",
    "    \n",
    "    # 形状为 (1, height, width)\n",
    "    mask = mask[np.newaxis, :, :]\n",
    "    \n",
    "    return mask\n",
    "\n",
    "total = 0\n",
    "count = 0\n",
    "\n",
    "e = 0\n",
    "rot_errors = []\n",
    "trans_errors = []\n",
    "\n",
    "file = h5py.File(f'D:/gs-localization/output/360_v2/{scene}/feats-superpoint-n4096-r1024.h5', 'r')\n",
    "\n",
    "for i, image in enumerate(test_infos):\n",
    "    viewpoint = Camera.init_from_dataset(dataset, i, projection_matrix)\n",
    "\n",
    "    viewpoint.compute_grad_mask(config)\n",
    "    \n",
    "    group = file[image] \n",
    "    keypoints = group['keypoints'][group['scores'][:]>0.2]  \n",
    "    mask = create_mask(mkpts_lst=keypoints, width=dataset.width, height=dataset.height, k=10)\n",
    "    viewpoint.grad_mask = viewpoint.grad_mask | torch.tensor(mask).to(\"cuda:0\")\n",
    "\n",
    "    config[\"Training\"][\"monocular\"] = True\n",
    "\n",
    "    initial_R = torch.tensor(test_infos[image].R)\n",
    "    initial_T = torch.tensor(test_infos[image].T).squeeze()\n",
    "\n",
    "    rotation_matrix, translation_vector, render_pkg = gradient_decent(viewpoint, config, initial_R, initial_T)\n",
    "    #rotation_matrix, translation_vector = initial_R, initial_T\n",
    "    \n",
    "    rot_error = rotation_error(rotation_matrix.cpu().numpy(), viewpoint.R_gt.cpu().numpy())\n",
    "    trans_error = translation_error(translation_vector.reshape(3,1).cpu().numpy(), viewpoint.T_gt.reshape(3,1).cpu().numpy())\n",
    "    \n",
    "    e += trans_error\n",
    "\n",
    "    print(image, rot_error, trans_error)\n",
    "    rot_errors.append(rot_error)\n",
    "    trans_errors.append(trans_error)\n",
    "    if rot_error < 5 and trans_error < 0.05:\n",
    "        count += 1\n",
    "\n",
    "    total += 1\n",
    "\n",
    "file.close()\n",
    "\n",
    "\"\"\"\n",
    "SCENES = ['bicycle', 'bonsai', 'counter', 'garden',  'kitchen', 'room', 'stump']\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--scenes\", default=SCENES, choices=SCENES, nargs=\"+\")\n",
    "    parser.add_argument(\"--overwrite\", action=\"store_true\")\n",
    "    parser.add_argument(\n",
    "        \"--dataset\",\n",
    "        type=Path,\n",
    "        default=\"datasets/360_v2\",\n",
    "        help=\"Path to the dataset, default: %(default)s\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--outputs\",\n",
    "        type=Path,\n",
    "        default=\"output/360_v2\",\n",
    "        help=\"Path to the output directory, default: %(default)s\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--num_covis\",\n",
    "        type=int,\n",
    "        default=30,\n",
    "        help=\"Number of image pairs for SfM, default: %(default)s\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--num_retrieve\",\n",
    "        type=int,\n",
    "        default=3,\n",
    "        help=\"Number of images for retrieval, default: %(default)s\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    gt_dirs = args.dataset / \"{scene}/sparse/0\" \n",
    "    tr_dirs = args.dataset / \"{scene}/train_views/triangulated\" \n",
    "\n",
    "    for scene in args.scenes:\n",
    "        logger.info(f'Working on scene \"{scene}\".')\n",
    "        if args.overwrite or True:\n",
    "            run_scene(\n",
    "                args.dataset / scene / \"images_4\",\n",
    "                Path(str(gt_dirs).format(scene=scene)),\n",
    "                Path(str(tr_dirs).format(scene=scene)), \n",
    "                args.dataset / scene,\n",
    "                args.outputs / scene,\n",
    "                args.num_covis,\n",
    "                args.num_retrieve)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fb0fb9c-54eb-4d2e-85b7-073a24f6d5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\27118/.cache\\torch\\hub\\intel-isl_MiDaS_master\n",
      "D:\\anaconda3\\envs\\gaussian_splatting\\lib\\site-packages\\timm\\models\\_factory.py:117: UserWarning: Mapping deprecated model name vit_base_resnet50_384 to current vit_base_r50_s16_384.orig_in21k_ft_in1k.\n",
      "  model = create_fn(\n",
      "Using cache found in C:\\Users\\27118/.cache\\torch\\hub\\intel-isl_MiDaS_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import yaml\n",
    "from munch import munchify\n",
    "from math import atan\n",
    "from collections import OrderedDict\n",
    "\n",
    "sys.path.append(\"D:/gs-localization/gaussian_splatting\")\n",
    "sys.path.append(\"D:/gs-localization\")\n",
    "sys.path.append(\"D:/gs-localization/gs_localization/pipelines\")\n",
    "\n",
    "\n",
    "from tools.config_utils import load_config, update_recursive\n",
    "from tools import read_write_model\n",
    "from tools.gaussian_model import GaussianModel\n",
    "from tools import render\n",
    "from tools.camera_utils import Camera\n",
    "from tools.descent_utils import get_loss_tracking\n",
    "from tools.pose_utils import update_pose\n",
    "from tools.graphics_utils import getProjectionMatrix2\n",
    "\n",
    "\n",
    "def gradient_decent(viewpoint, config, initial_R, initial_T):\n",
    "\n",
    "    viewpoint.update_RT(initial_R, initial_T)\n",
    "    \n",
    "    opt_params = []\n",
    "    opt_params.append(\n",
    "        {\n",
    "            \"params\": [viewpoint.cam_rot_delta],\n",
    "            \"lr\": 0.001,\n",
    "            \"name\": \"rot_{}\".format(viewpoint.uid),\n",
    "        }\n",
    "    )\n",
    "    opt_params.append(\n",
    "        {\n",
    "            \"params\": [viewpoint.cam_trans_delta],\n",
    "            \"lr\": 0.001,\n",
    "            \"name\": \"trans_{}\".format(viewpoint.uid),\n",
    "        }\n",
    "    )\n",
    "    opt_params.append(\n",
    "        {\n",
    "            \"params\": [viewpoint.exposure_a],\n",
    "            \"lr\": 0.001,\n",
    "            \"name\": \"exposure_a_{}\".format(viewpoint.uid),\n",
    "        }\n",
    "    )\n",
    "    opt_params.append(\n",
    "        {\n",
    "            \"params\": [viewpoint.exposure_b],\n",
    "            \"lr\": 0.001,\n",
    "            \"name\": \"exposure_b_{}\".format(viewpoint.uid),\n",
    "        }\n",
    "    )\n",
    "    \n",
    "\n",
    "    pose_optimizer = torch.optim.Adam(opt_params)\n",
    "    \n",
    "    for tracking_itr in range(50):\n",
    "        \n",
    "        render_pkg = render(\n",
    "            viewpoint, Model, pipeline_params, background\n",
    "        )\n",
    "        \n",
    "        image, depth, opacity = (\n",
    "            render_pkg[\"render\"],\n",
    "            render_pkg[\"depth\"],\n",
    "            render_pkg[\"opacity\"],\n",
    "        )\n",
    "          \n",
    "        pose_optimizer.zero_grad()\n",
    "        \n",
    "        loss_tracking = get_loss_tracking(\n",
    "            config, image, depth, opacity, viewpoint\n",
    "        )\n",
    "        loss_tracking.backward()\n",
    "        \n",
    "    \n",
    "        with torch.no_grad():\n",
    "            pose_optimizer.step()\n",
    "            converged = update_pose(viewpoint, converged_threshold=1e-4)\n",
    "    \n",
    "        if converged:\n",
    "            break\n",
    "             \n",
    "    return viewpoint.R, viewpoint.T, render_pkg\n",
    "\n",
    "\n",
    "class Transformation:\n",
    "    def __init__(self, R=None, T=None):\n",
    "        self.R = R\n",
    "        self.T = T\n",
    "\n",
    "def quat_to_rotmat(qvec):\n",
    "    qvec = np.array(qvec, dtype=float)\n",
    "    w, x, y, z = qvec\n",
    "    R = np.array([\n",
    "        [1 - 2*y**2 - 2*z**2, 2*x*y - 2*z*w, 2*x*z + 2*y*w],\n",
    "        [2*x*y + 2*z*w, 1 - 2*x**2 - 2*z**2, 2*y*z - 2*x*w],\n",
    "        [2*x*z - 2*y*w, 2*y*z + 2*x*w, 1 - 2*x**2 - 2*y**2]\n",
    "    ])\n",
    "    return R\n",
    "\n",
    "\n",
    "def focal2fov(focal, pixels):\n",
    "    return 2 * atan(pixels / (2 * focal))\n",
    "\n",
    "def load_pose(pose_txt):\n",
    "    pose = []\n",
    "    with open(pose_txt, 'r') as f:\n",
    "        for line in f:\n",
    "            row = line.strip('\\n').split()\n",
    "            row = [float(c) for c in row]\n",
    "            pose.append(row)\n",
    "    pose = np.array(pose).astype(np.float32)\n",
    "    assert pose.shape == (4,4)\n",
    "    return pose\n",
    "\n",
    "def create_mask(mkpts_lst, width, height, k):\n",
    "    # Initial mask as all False\n",
    "    mask = np.zeros((height, width), dtype=bool)\n",
    "    \n",
    "    # Calculat k radius\n",
    "    half_k = k // 2\n",
    "    \n",
    "    # Iterate through all points\n",
    "    for pt in mkpts_lst:\n",
    "        x, y = int(pt[0]), int(pt[1])\n",
    "        \n",
    "        # Calculate k*k borders\n",
    "        x_min = max(0, x - half_k)\n",
    "        x_max = min(width, x + half_k + 1)\n",
    "        y_min = max(0, y - half_k)\n",
    "        y_max = min(height, y + half_k + 1)\n",
    "        \n",
    "        # Set mask k*k area as True\n",
    "        mask[y_min:y_max, x_min:x_max] = True\n",
    "    \n",
    "    # Shape: (1, height, width)\n",
    "    mask = mask[np.newaxis, :, :]\n",
    "    \n",
    "    return mask\n",
    "\n",
    "class BaseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, args, path, config):\n",
    "        self.args = args\n",
    "        self.path = path\n",
    "        self.config = config\n",
    "        self.device = \"cuda:0\"\n",
    "        self.dtype = torch.float32\n",
    "        self.num_imgs = 9999\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_imgs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pass\n",
    "\n",
    "class MonocularDataset(BaseDataset):\n",
    "    def __init__(self, args, path, config):\n",
    "        super().__init__(args, path, config)\n",
    "        calibration = config[\"Dataset\"][\"Calibration\"]\n",
    "        # Camera prameters\n",
    "        self.fx = calibration[\"fx\"]\n",
    "        self.fy = calibration[\"fy\"]\n",
    "        self.cx = calibration[\"cx\"]\n",
    "        self.cy = calibration[\"cy\"]\n",
    "        self.width = calibration[\"width\"]\n",
    "        self.height = calibration[\"height\"]\n",
    "        self.fovx = focal2fov(self.fx, self.width)\n",
    "        self.fovy = focal2fov(self.fy, self.height)\n",
    "        self.K = np.array(\n",
    "            [[self.fx, 0.0, self.cx], [0.0, self.fy, self.cy], [0.0, 0.0, 1.0]]\n",
    "        )\n",
    "        # distortion parameters\n",
    "        self.disorted = calibration[\"distorted\"]\n",
    "        self.dist_coeffs = np.array(\n",
    "            [\n",
    "                calibration[\"k1\"],\n",
    "                calibration[\"k2\"],\n",
    "                calibration[\"p1\"],\n",
    "                calibration[\"p2\"],\n",
    "                calibration[\"k3\"],\n",
    "            ]\n",
    "        )\n",
    "        self.map1x, self.map1y = cv2.initUndistortRectifyMap(\n",
    "            self.K,\n",
    "            self.dist_coeffs,\n",
    "            np.eye(3),\n",
    "            self.K,\n",
    "            (self.width, self.height),\n",
    "            cv2.CV_32FC1,\n",
    "        )\n",
    "        # depth parameters\n",
    "        self.has_depth = True if \"depth_scale\" in calibration.keys() else False\n",
    "        self.depth_scale = calibration[\"depth_scale\"] if self.has_depth else None\n",
    "\n",
    "        # Default scene scale\n",
    "        nerf_normalization_radius = 5\n",
    "        self.scene_info = {\n",
    "            \"nerf_normalization\": {\n",
    "                \"radius\": nerf_normalization_radius,\n",
    "                \"translation\": np.zeros(3),\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        color_path = self.color_paths[idx]\n",
    "        pose = self.poses[idx]\n",
    "\n",
    "        image = np.array(Image.open(color_path))\n",
    "        depth = None\n",
    "\n",
    "        if self.disorted:\n",
    "            image = cv2.remap(image, self.map1x, self.map1y, cv2.INTER_LINEAR)\n",
    "\n",
    "        if self.has_depth:\n",
    "            depth_path = self.depth_paths[idx]\n",
    "            depth = np.array(Image.open(depth_path)) / self.depth_scale\n",
    "\n",
    "        image = (\n",
    "            torch.from_numpy(image / 255.0)\n",
    "            .clamp(0.0, 1.0)\n",
    "            .permute(2, 0, 1)\n",
    "            .to(device=self.device, dtype=self.dtype)\n",
    "        )\n",
    "        pose = torch.from_numpy(pose).to(device=self.device)\n",
    "        return image, depth, pose\n",
    "\n",
    "\n",
    "class seven_scenes_Dataset(MonocularDataset):\n",
    "    def __init__(self, args, path, config, data_folder, scene):\n",
    "        super().__init__(args, path, config)\n",
    "        self.has_depth = True\n",
    "        self.seven_scenes_Parser(data_folder, scene) \n",
    "        \n",
    "    def seven_scenes_Parser(self, data_folder, scene):\n",
    "        self.color_paths, self.poses, self.depth_paths = [], [], []\n",
    "\n",
    "        gt_dirs = Path(data_folder) / scene / \"sparse/0\"\n",
    "        _, images, _ = read_write_model.read_model(gt_dirs, \".txt\")\n",
    "\n",
    "        # Read the filenames from test_fewshot.txt and store them in a set.\n",
    "        test_images_path = Path(data_folder) / scene / \"test_full.txt\"\n",
    "        \n",
    "        with open(test_images_path, 'r') as f:\n",
    "            test_images = set(line.strip() for line in f)\n",
    "            \n",
    "        for i, image in tqdm(images.items(),\"Load dataset\"):\n",
    "            # Execute the following operation only if image.name exists in test_images.\"\n",
    "            if image.name in test_images:\n",
    "                image_path = Path(data_folder) / scene / 'images_full' / image.name\n",
    "                depth_path = Path(data_folder) / scene / 'depths_full' / image.name.replace(\"color\",\"depth\")\n",
    "                self.color_paths.append(image_path)\n",
    "                self.depth_paths.append(depth_path)\n",
    "                R_gt, t_gt = image.qvec2rotmat(), image.tvec\n",
    "                pose = np.eye(4)            \n",
    "                pose[:3, :3] = R_gt         \n",
    "                pose[:3, 3] = t_gt \n",
    "                self.poses.append(pose)\n",
    "\n",
    "        # Sort self.color_paths, self.poses, and self.depth_paths based on normal file name order\n",
    "        sorted_data = sorted(zip(self.color_paths, self.depth_paths, self.poses), key=lambda x: x[0].name)\n",
    "        self.color_paths, self.depth_paths, self.poses = zip(*sorted_data)\n",
    "        del images\n",
    "\n",
    "with open(\"D:/gs-localization/gs_localization/pipelines/configs/mono/tum/fr3_office.yaml\", \"r\") as f:\n",
    "    cfg_special = yaml.full_load(f)\n",
    "\n",
    "inherit_from = \"D:/gs-localization/gs_localization/pipelines/configs/mono/tum/base_config.yaml\"\n",
    "\n",
    "if inherit_from is not None:\n",
    "    cfg = load_config(inherit_from)\n",
    "else:\n",
    "    cfg = dict()\n",
    "\n",
    "# merge per dataset cfg. and main cfg.\n",
    "config = update_recursive(cfg, cfg_special)\n",
    "config = cfg\n",
    "    \n",
    "data_folder = \"D:/gs-localization/datasets/7scenes\"\n",
    "config[\"Dataset\"][\"Calibration\"][\"fx\"] = 525\n",
    "config[\"Dataset\"][\"Calibration\"][\"fy\"] = 525\n",
    "config[\"Dataset\"][\"Calibration\"][\"cx\"] = 320\n",
    "config[\"Dataset\"][\"Calibration\"][\"cy\"] = 240\n",
    "config[\"Dataset\"][\"Calibration\"][\"width\"] = 640\n",
    "config[\"Dataset\"][\"Calibration\"][\"height\"] = 480   \n",
    "config[\"Dataset\"][\"Calibration\"]['depth_scale'] = 1000.0\n",
    "config[\"Training\"][\"monocular\"] = False\n",
    "config[\"Training\"][\"alpha\"] = 0.99\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dce7c514-d3fc-4e6d-9464-3cf4e818c210",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'viewpoint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 假设 viewpoint.original_image 和 render_pkg[\"render\"] 是 Tensor\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m ground_truth_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mviewpoint\u001b[49m\u001b[38;5;241m.\u001b[39moriginal_image\n\u001b[0;32m      7\u001b[0m localized_tensor \u001b[38;5;241m=\u001b[39m render_pkg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrender\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 找到每个像素中 R、G、B 三个通道的最大值\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'viewpoint' is not defined"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "# 假设 viewpoint.original_image 和 render_pkg[\"render\"] 是 Tensor\n",
    "ground_truth_tensor = viewpoint.original_image\n",
    "localized_tensor = render_pkg[\"render\"]\n",
    "\n",
    "# 找到每个像素中 R、G、B 三个通道的最大值\n",
    "max_vals, _ = localized_tensor.max(dim=0)  # 得到每个像素的最大值 (H, W)\n",
    "\n",
    "# 找到哪些像素的最大值超过 1\n",
    "exceeds_one_mask = max_vals > 1  # 布尔掩码，标记哪些像素的最大值超过 1\n",
    "\n",
    "# 对超过 1 的地方，将 R、G、B 值同时按最大值进行归一化\n",
    "localized_tensor[:, exceeds_one_mask] = localized_tensor[:, exceeds_one_mask] / (max_vals[exceeds_one_mask] + 0.00001)\n",
    "\n",
    "# 将 Tensor 转换为 PIL 图像\n",
    "tensor_to_pil = transforms.ToPILImage()\n",
    "\n",
    "ground_truth_image = tensor_to_pil(ground_truth_tensor)\n",
    "localized_image = tensor_to_pil(localized_tensor)\n",
    "\n",
    "# 确保两张图片大小相同（可以选择调整大小）\n",
    "width, height = ground_truth_image.size\n",
    "localized_image = localized_image.resize((width, height))\n",
    "\n",
    "# 创建一个新的空白图像，用来合成 ground truth 和 localized image\n",
    "combined_image = Image.new('RGB', (width, height))\n",
    "\n",
    "# 将图像转换为 NumPy 数组，方便逐像素操作\n",
    "ground_truth_array = np.array(ground_truth_image)\n",
    "localized_image_array = np.array(localized_image)\n",
    "\n",
    "# 根据条件 x < ay 来合成图像\n",
    "for y in range(height):\n",
    "    for x in range(width):\n",
    "        if x < (y * (width / height)):  # 根据比例 x < ay 来判断\n",
    "            combined_image.putpixel((x, y), tuple(ground_truth_array[y, x]))  # 放置 ground truth\n",
    "        else:\n",
    "            combined_image.putpixel((x, y), tuple(ground_truth_array[y, x]))  # 放置 localized image\n",
    "\n",
    "# 画绿色对角线，从左上到右下\n",
    "draw = ImageDraw.Draw(combined_image)\n",
    "draw.line((0, 0, width, height), fill=\"green\", width=3)\n",
    "\n",
    "# 画小框\n",
    "small_box_start = (215, 220)  # 小框左上角起始点 (x, y)\n",
    "small_box_width = 150         # 小框的宽度\n",
    "small_box_height = 100        # 小框的高度\n",
    "small_box_end = (small_box_start[0] + small_box_width, small_box_start[1] + small_box_height)\n",
    "\n",
    "# 绘制蓝色小框\n",
    "draw.rectangle([small_box_start, small_box_end], outline=\"blue\", width=3)\n",
    "\n",
    "# 提取小框中的部分\n",
    "small_box_region = combined_image.crop((small_box_start[0], small_box_start[1], small_box_end[0], small_box_end[1]))\n",
    "\n",
    "# 放大小框中的部分\n",
    "scale_factor = 1.6  # 放大倍数\n",
    "large_box_region = small_box_region.resize((int(small_box_width * scale_factor), int(small_box_height * scale_factor)))\n",
    "\n",
    "# 将放大的大框放置在小框旁边，覆盖图片部分区域\n",
    "large_box_start_x = small_box_end[0] + 10  # 小框右边再加10像素\n",
    "large_box_start_y = small_box_start[1]\n",
    "\n",
    "# 确保大框不会超出图片边界\n",
    "if large_box_start_x + large_box_region.width > width:\n",
    "    large_box_start_x = width - large_box_region.width - 10\n",
    "if large_box_start_y + large_box_region.height > height:\n",
    "    large_box_start_y = height - large_box_region.height - 10\n",
    "\n",
    "# 将放大的区域粘贴回原图中\n",
    "combined_image.paste(large_box_region, (large_box_start_x, large_box_start_y))\n",
    "\n",
    "# 画大框\n",
    "large_box_end = (large_box_start_x + large_box_region.width, large_box_start_y + large_box_region.height)\n",
    "draw.rectangle([large_box_start_x, large_box_start_y, large_box_end[0], large_box_end[1]], outline=\"blue\", width=3)\n",
    "\n",
    "# 显示结果\n",
    "combined_image.show()\n",
    "\n",
    "# 保存图片到指定路径\n",
    "combined_image.save(f\"C:/Users/27118/Desktop/{scene}_compare.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08473371-f91d-4684-b4c6-24e55419cbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Median errors: 0.001m, 0.003deg\n",
      "\n",
      "\t1cm, 1deg : 100.00%\n",
      "\n",
      "\t2cm, 2deg : 100.00%\n",
      "\n",
      "\t3cm, 3deg : 100.00%\n",
      "\n",
      "\t5cm, 5deg : 100.00%\n",
      "\n",
      "\t25cm, 2deg : 100.00%\n",
      "\n",
      "\t50cm, 5deg : 100.00%\n",
      "\n",
      "\t500cm, 10deg : 100.00%\n"
     ]
    }
   ],
   "source": [
    "med_t = np.median(trans_errors)\n",
    "med_R = np.median(rot_errors)\n",
    "print( f\"\\nMedian errors: {med_t:.3f}m, {med_R:.3f}deg\")\n",
    "\n",
    "threshs_t = [0.01, 0.02, 0.03, 0.05, 0.25, 0.5, 5.0]\n",
    "threshs_R = [1.0, 2.0, 3.0, 5.0, 2.0, 5.0, 10.0]\n",
    "for th_t, th_R in zip(threshs_t, threshs_R):\n",
    "    ratio = np.mean((np.array(trans_errors) < th_t) & (np.array(rot_errors) < th_R))\n",
    "    print(f\"\\n\\t{th_t*100:.0f}cm, {th_R:.0f}deg : {ratio*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e825978e-e63c-464b-9479-dac429bd7f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of mkpts_lst: (2205, 2)\n",
      "Keypoints (first few): \n",
      "[[  74.25    9.83]\n",
      " [ 108.3     9.83]\n",
      " [ 452.5     9.83]\n",
      " [ 541.      9.83]\n",
      " [ 587.5     9.83]\n",
      " [ 631.      9.83]\n",
      " [ 648.      9.83]\n",
      " [ 694.5     9.83]\n",
      " [1042.      9.83]\n",
      " [1192.      9.83]]\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "# 打开 HDF5 文件\n",
    "with h5py.File('D:/gs-localization/output/360_v2/stump/feats-superpoint-n4096-r1024.h5', 'r') as file:\n",
    "    # 指定要读取的图像 key\n",
    "    target_image_key = '_DSC9266.JPG'\n",
    "    \n",
    "    # 获取该图像对应的组\n",
    "    group = file[target_image_key]\n",
    "    \n",
    "    # 提取 keypoints 数据集并存储为 mkpts_lst\n",
    "    mkpts_lst = group['keypoints'][:]\n",
    "    \n",
    "    # 打印结果确认\n",
    "    print(f\"Shape of mkpts_lst: {mkpts_lst.shape}\")\n",
    "    print(f\"Keypoints (first few): \\n{mkpts_lst[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2634ebd3-1a84-47d3-8ab1-8b4249e820cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 74.25,   9.83],\n",
       "       [108.3 ,   9.83],\n",
       "       [452.5 ,   9.83],\n",
       "       ...,\n",
       "       [844.  , 805.5 ],\n",
       "       [893.5 , 805.5 ],\n",
       "       [898.5 , 805.5 ]], dtype=float16)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mkpts_lst[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "207f9756-b2a7-46c6-aa37-a296b8c06b6a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys:  ['_DSC8679.JPG', '_DSC8680.JPG', '_DSC8681.JPG', '_DSC8682.JPG', '_DSC8683.JPG', '_DSC8684.JPG', '_DSC8685.JPG', '_DSC8686.JPG', '_DSC8687.JPG', '_DSC8688.JPG', '_DSC8689.JPG', '_DSC8690.JPG', '_DSC8691.JPG', '_DSC8692.JPG', '_DSC8693.JPG', '_DSC8694.JPG', '_DSC8695.JPG', '_DSC8696.JPG', '_DSC8697.JPG', '_DSC8698.JPG', '_DSC8699.JPG', '_DSC8700.JPG', '_DSC8701.JPG', '_DSC8702.JPG', '_DSC8703.JPG', '_DSC8704.JPG', '_DSC8705.JPG', '_DSC8706.JPG', '_DSC8707.JPG', '_DSC8708.JPG', '_DSC8709.JPG', '_DSC8710.JPG', '_DSC8711.JPG', '_DSC8712.JPG', '_DSC8713.JPG', '_DSC8714.JPG', '_DSC8715.JPG', '_DSC8716.JPG', '_DSC8717.JPG', '_DSC8718.JPG', '_DSC8719.JPG', '_DSC8720.JPG', '_DSC8721.JPG', '_DSC8722.JPG', '_DSC8723.JPG', '_DSC8724.JPG', '_DSC8725.JPG', '_DSC8726.JPG', '_DSC8727.JPG', '_DSC8728.JPG', '_DSC8729.JPG', '_DSC8730.JPG', '_DSC8731.JPG', '_DSC8732.JPG', '_DSC8733.JPG', '_DSC8734.JPG', '_DSC8735.JPG', '_DSC8736.JPG', '_DSC8737.JPG', '_DSC8738.JPG', '_DSC8739.JPG', '_DSC8741.JPG', '_DSC8742.JPG', '_DSC8743.JPG', '_DSC8744.JPG', '_DSC8745.JPG', '_DSC8746.JPG', '_DSC8747.JPG', '_DSC8748.JPG', '_DSC8749.JPG', '_DSC8750.JPG', '_DSC8751.JPG', '_DSC8752.JPG', '_DSC8753.JPG', '_DSC8754.JPG', '_DSC8755.JPG', '_DSC8756.JPG', '_DSC8757.JPG', '_DSC8758.JPG', '_DSC8759.JPG', '_DSC8760.JPG', '_DSC8761.JPG', '_DSC8762.JPG', '_DSC8763.JPG', '_DSC8764.JPG', '_DSC8765.JPG', '_DSC8766.JPG', '_DSC8767.JPG', '_DSC8768.JPG', '_DSC8769.JPG', '_DSC8770.JPG', '_DSC8771.JPG', '_DSC8772.JPG', '_DSC8773.JPG', '_DSC8774.JPG', '_DSC8775.JPG', '_DSC8776.JPG', '_DSC8777.JPG', '_DSC8778.JPG', '_DSC8779.JPG', '_DSC8780.JPG', '_DSC8781.JPG', '_DSC8782.JPG', '_DSC8783.JPG', '_DSC8784.JPG', '_DSC8785.JPG', '_DSC8786.JPG', '_DSC8787.JPG', '_DSC8788.JPG', '_DSC8789.JPG', '_DSC8790.JPG', '_DSC8791.JPG', '_DSC8792.JPG', '_DSC8793.JPG', '_DSC8794.JPG', '_DSC8795.JPG', '_DSC8796.JPG', '_DSC8797.JPG', '_DSC8798.JPG', '_DSC8799.JPG', '_DSC8800.JPG', '_DSC8801.JPG', '_DSC8802.JPG', '_DSC8803.JPG', '_DSC8804.JPG', '_DSC8805.JPG', '_DSC8806.JPG', '_DSC8807.JPG', '_DSC8808.JPG', '_DSC8809.JPG', '_DSC8810.JPG', '_DSC8811.JPG', '_DSC8812.JPG', '_DSC8813.JPG', '_DSC8814.JPG', '_DSC8815.JPG', '_DSC8816.JPG', '_DSC8817.JPG', '_DSC8818.JPG', '_DSC8819.JPG', '_DSC8820.JPG', '_DSC8821.JPG', '_DSC8822.JPG', '_DSC8823.JPG', '_DSC8824.JPG', '_DSC8825.JPG', '_DSC8826.JPG', '_DSC8827.JPG', '_DSC8828.JPG', '_DSC8829.JPG', '_DSC8830.JPG', '_DSC8831.JPG', '_DSC8832.JPG', '_DSC8833.JPG', '_DSC8834.JPG', '_DSC8835.JPG', '_DSC8836.JPG', '_DSC8837.JPG', '_DSC8838.JPG', '_DSC8839.JPG', '_DSC8840.JPG', '_DSC8841.JPG', '_DSC8842.JPG', '_DSC8843.JPG', '_DSC8844.JPG', '_DSC8845.JPG', '_DSC8846.JPG', '_DSC8847.JPG', '_DSC8848.JPG', '_DSC8849.JPG', '_DSC8850.JPG', '_DSC8851.JPG', '_DSC8852.JPG', '_DSC8853.JPG', '_DSC8854.JPG', '_DSC8855.JPG', '_DSC8856.JPG', '_DSC8857.JPG', '_DSC8858.JPG', '_DSC8859.JPG', '_DSC8860.JPG', '_DSC8861.JPG', '_DSC8862.JPG', '_DSC8863.JPG', '_DSC8864.JPG', '_DSC8865.JPG', '_DSC8866.JPG', '_DSC8867.JPG', '_DSC8868.JPG', '_DSC8869.JPG', '_DSC8870.JPG', '_DSC8871.JPG', '_DSC8872.JPG', '_DSC8873.JPG']\n",
      "Shape of descriptors: (256, 1638)\n",
      "Descriptors (first few): \n",
      "[[-6.2469e-02  8.0994e-02  1.1676e-01 ... -7.6561e-03  4.5380e-02\n",
      "   7.7515e-02]\n",
      " [-3.7720e-02  9.1919e-02  2.6642e-02 ...  5.3070e-02  1.9150e-02\n",
      "  -3.7251e-03]\n",
      " [-2.8229e-02  4.6692e-02 -6.3538e-02 ...  9.2529e-02  2.1594e-01\n",
      "   1.5686e-01]\n",
      " ...\n",
      " [ 2.6321e-02 -2.3956e-02 -4.7266e-05 ... -1.0492e-01 -4.7607e-02\n",
      "  -9.9640e-03]\n",
      " [ 1.5771e-01  2.3880e-02 -5.1453e-02 ... -4.1016e-02 -7.0374e-02\n",
      "  -4.5929e-02]\n",
      " [ 3.1860e-02 -1.1267e-01 -4.0863e-02 ...  9.0759e-02  1.7126e-01\n",
      "   1.3379e-01]]\n",
      "Image size: [1237  822]\n",
      "Shape of keypoints: (1638, 2)\n",
      "Keypoints (first few): \n",
      "[[  23.06     9.77 ]\n",
      " [ 230.9      9.77 ]\n",
      " [ 420.5      9.77 ]\n",
      " [ 871.       9.77 ]\n",
      " [ 971.5      9.77 ]\n",
      " [1031.       9.77 ]\n",
      " [1058.       9.77 ]\n",
      " [ 314.2     10.984]\n",
      " [ 437.5     10.984]\n",
      " [ 528.      10.984]]\n",
      "Shape of scores: (1638,)\n",
      "Scores (first few): \n",
      "[0.01443  0.007637 0.006588 0.00876  0.065    0.006542 0.04175  0.4375\n",
      " 0.01471  0.2085  ]\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "# 打开 HDF5 文件\n",
    "with h5py.File('D:/gs-localization/output/360_v2/bicycle/feats-superpoint-n4096-r1024.h5', 'r') as file:\n",
    "    # 查看文件中的所有顶层组（类似于文件夹）\n",
    "    keys = list(file.keys())\n",
    "    print(\"Keys: \", keys)\n",
    "    \n",
    "    # 假设你想读取'_DSC8679.JPG'组中的数据\n",
    "    first_image_key = keys[0]  # '_DSC8679.JPG'\n",
    "    group = file[first_image_key]  # 获取组对象\n",
    "    \n",
    "    # 读取'descriptors'数据集\n",
    "    descriptors = group['descriptors'][:]\n",
    "    print(f\"Shape of descriptors: {descriptors.shape}\")\n",
    "    print(f\"Descriptors (first few): \\n{descriptors[:10]}\")\n",
    "    \n",
    "    # 读取'image_size'数据集\n",
    "    image_size = group['image_size'][:]\n",
    "    print(f\"Image size: {image_size}\")\n",
    "    \n",
    "    # 读取'keypoints'数据集\n",
    "    keypoints = group['keypoints'][:]\n",
    "    print(f\"Shape of keypoints: {keypoints.shape}\")\n",
    "    print(f\"Keypoints (first few): \\n{keypoints[:10]}\")\n",
    "    print(keypoints[:][])\n",
    "    \n",
    "    # 读取'scores'数据集\n",
    "    scores = group['scores'][:]\n",
    "    print(f\"Shape of scores: {scores.shape}\")\n",
    "    print(f\"Scores (first few): \\n{scores[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717aae78-d534-40de-8e7f-4ac6457e2059",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read_write_model.read_cameras_binary(\"D:/gs-localization/datasets/nerf_llff_data/fern/train_views/triangulated/cameras.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cdbf24-adb4-44fa-a052-ea68b309cde0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
