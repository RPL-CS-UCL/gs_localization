{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58901cff-4b30-4d55-96f8-e5cc02ae7445",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load dataset: 100%|█████████████████████████████████████████████████████████████| 2292/2292 [00:00<00:00, 49020.17it/s]\n",
      "Localization: 100%|██████████████████████████████████████████████████████████████████| 760/760 [04:08<00:00,  3.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Median errors for GreatCourt: 0.684m, 0.208deg\n",
      "\n",
      "\t1cm, 1deg : 0.00%\n",
      "\n",
      "\t2cm, 2deg : 0.12%\n",
      "\n",
      "\t3cm, 3deg : 0.13%\n",
      "\n",
      "\t5cm, 5deg : 0.13%\n",
      "\n",
      "\t25cm, 2deg : 8.46%\n",
      "\n",
      "\t50cm, 5deg : 31.00%\n",
      "\n",
      "\t500cm, 10deg : 94.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import yaml\n",
    "from munch import munchify\n",
    "from math import atan\n",
    "from collections import OrderedDict\n",
    "\n",
    "sys.path.append(\"D:/gs-localization/gaussian_splatting\")\n",
    "sys.path.append(\"D:/gs-localization\")\n",
    "sys.path.append(\"D:/gs-localization/gs_localization/pipelines\")\n",
    "\n",
    "from tools.config_utils import load_config, update_recursive\n",
    "from tools import read_write_model\n",
    "from tools.gaussian_model import GaussianModel\n",
    "from tools import render\n",
    "from tools.camera_utils import Camera\n",
    "from tools.descent_utils import get_loss_tracking\n",
    "from tools.pose_utils import update_pose\n",
    "from tools.graphics_utils import getProjectionMatrix2\n",
    "\n",
    "\n",
    "def gradient_decent(viewpoint, config, initial_R, initial_T):\n",
    "\n",
    "    viewpoint.update_RT(initial_R, initial_T)\n",
    "    \n",
    "    opt_params = []\n",
    "    opt_params.append(\n",
    "        {\n",
    "            \"params\": [viewpoint.cam_rot_delta],\n",
    "            \"lr\": 0.00001,\n",
    "            \"name\": \"rot_{}\".format(viewpoint.uid),\n",
    "        }\n",
    "    )\n",
    "    opt_params.append(\n",
    "        {\n",
    "            \"params\": [viewpoint.cam_trans_delta],\n",
    "            \"lr\": 0.001,\n",
    "            \"name\": \"trans_{}\".format(viewpoint.uid),\n",
    "        }\n",
    "    )\n",
    "    opt_params.append(\n",
    "        {\n",
    "            \"params\": [viewpoint.exposure_a],\n",
    "            \"lr\": 0.001,\n",
    "            \"name\": \"exposure_a_{}\".format(viewpoint.uid),\n",
    "        }\n",
    "    )\n",
    "    opt_params.append(\n",
    "        {\n",
    "            \"params\": [viewpoint.exposure_b],\n",
    "            \"lr\": 0.001,\n",
    "            \"name\": \"exposure_b_{}\".format(viewpoint.uid),\n",
    "        }\n",
    "    )\n",
    "    \n",
    "\n",
    "    pose_optimizer = torch.optim.Adam(opt_params)\n",
    "    \n",
    "    for tracking_itr in range(20):\n",
    "        \n",
    "        render_pkg = render(\n",
    "            viewpoint, Model, pipeline_params, background\n",
    "        )\n",
    "        \n",
    "        image, depth, opacity = (\n",
    "            render_pkg[\"render\"],\n",
    "            render_pkg[\"depth\"],\n",
    "            render_pkg[\"opacity\"],\n",
    "        )\n",
    "          \n",
    "        pose_optimizer.zero_grad()\n",
    "        \n",
    "        loss_tracking = get_loss_tracking(\n",
    "            config, image, depth, opacity, viewpoint\n",
    "        )\n",
    "        loss_tracking.backward()\n",
    "        \n",
    "    \n",
    "        with torch.no_grad():\n",
    "            pose_optimizer.step()\n",
    "            converged = update_pose(viewpoint, converged_threshold=1e-3)\n",
    "    \n",
    "        if converged:\n",
    "            break\n",
    "             \n",
    "    return viewpoint.R, viewpoint.T, render_pkg\n",
    "\n",
    "\n",
    "class Transformation:\n",
    "    def __init__(self, R=None, T=None):\n",
    "        self.R = R\n",
    "        self.T = T\n",
    "\n",
    "def quat_to_rotmat(qvec):\n",
    "    qvec = np.array(qvec, dtype=float)\n",
    "    w, x, y, z = qvec\n",
    "    R = np.array([\n",
    "        [1 - 2*y**2 - 2*z**2, 2*x*y - 2*z*w, 2*x*z + 2*y*w],\n",
    "        [2*x*y + 2*z*w, 1 - 2*x**2 - 2*z**2, 2*y*z - 2*x*w],\n",
    "        [2*x*z - 2*y*w, 2*y*z + 2*x*w, 1 - 2*x**2 - 2*y**2]\n",
    "    ])\n",
    "    return R\n",
    "\n",
    "\n",
    "def focal2fov(focal, pixels):\n",
    "    return 2 * atan(pixels / (2 * focal))\n",
    "\n",
    "def load_pose(pose_txt):\n",
    "    pose = []\n",
    "    with open(pose_txt, 'r') as f:\n",
    "        for line in f:\n",
    "            row = line.strip('\\n').split()\n",
    "            row = [float(c) for c in row]\n",
    "            pose.append(row)\n",
    "    pose = np.array(pose).astype(np.float32)\n",
    "    assert pose.shape == (4,4)\n",
    "    return pose\n",
    "\n",
    "def create_mask(mkpts_lst, width, height, k):\n",
    "    # Initial mask as all False\n",
    "    mask = np.zeros((height, width), dtype=bool)\n",
    "    \n",
    "    # Calculate k radius\n",
    "    half_k = k // 2\n",
    "    \n",
    "    # Iterate through all points\n",
    "    for pt in mkpts_lst:\n",
    "        x, y = int(pt[0]), int(pt[1])\n",
    "        \n",
    "        # Calculate k*k borders\n",
    "        x_min = max(0, x - half_k)\n",
    "        x_max = min(width, x + half_k + 1)\n",
    "        y_min = max(0, y - half_k)\n",
    "        y_max = min(height, y + half_k + 1)\n",
    "        \n",
    "        # Set mask k*k area as True\n",
    "        mask[y_min:y_max, x_min:x_max] = True\n",
    "    \n",
    "    # Shape: (1, height, width)\n",
    "    mask = mask[np.newaxis, :, :]\n",
    "    \n",
    "    return mask\n",
    "\n",
    "class BaseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, args, path, config):\n",
    "        self.args = args\n",
    "        self.path = path\n",
    "        self.config = config\n",
    "        self.device = \"cuda:0\"\n",
    "        self.dtype = torch.float32\n",
    "        self.num_imgs = 9999\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_imgs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pass\n",
    "\n",
    "class MonocularDataset(BaseDataset):\n",
    "    def __init__(self, args, path, config):\n",
    "        super().__init__(args, path, config)\n",
    "        calibration = config[\"Dataset\"][\"Calibration\"]\n",
    "\n",
    "        # depth parameters\n",
    "        self.has_depth = True if \"depth_scale\" in calibration.keys() else False\n",
    "        self.depth_scale = calibration[\"depth_scale\"] if self.has_depth else None\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        color_path = self.color_paths[idx]\n",
    "        pose = self.poses[idx]\n",
    "        intrinsic = self.intrinsics[idx]\n",
    "\n",
    "        image = np.array(Image.open(color_path))\n",
    "        depth = None\n",
    "\n",
    "        if self.has_depth:\n",
    "            depth_path = self.depth_paths[idx]\n",
    "            depth = np.array(Image.open(depth_path)) / self.depth_scale\n",
    "\n",
    "        image = (\n",
    "            torch.from_numpy(image / 255.0)\n",
    "            .clamp(0.0, 1.0)\n",
    "            .permute(2, 0, 1)\n",
    "            .to(device=self.device, dtype=self.dtype)\n",
    "        )\n",
    "        pose = torch.from_numpy(pose).to(device=self.device)\n",
    "        intrinsic = torch.from_numpy(intrinsic).to(device=self.device)\n",
    "        projection_matrix = self.projection_matrices[idx].to(device=\"cuda:0\")\n",
    "        \n",
    "        return image, depth, pose, intrinsic, projection_matrix \n",
    "\n",
    "def parse_camera_params(file_path):\n",
    "    camera_params = {}\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            \n",
    "            # Ensure there are at least 8 elements, and the second element is 'PINHOLE'\n",
    "            if len(parts) == 8 and parts[1] == 'PINHOLE':\n",
    "                try:\n",
    "                    # Extract parameters, skipping the second item 'PINHOLE'\n",
    "                    img_name = parts[0]\n",
    "                    w = int(parts[2])\n",
    "                    h = int(parts[3])\n",
    "                    fx = float(parts[4])\n",
    "                    fy = float(parts[5])\n",
    "                    cx = float(parts[6])\n",
    "                    cy = float(parts[7])\n",
    "                    \n",
    "                    # Store in the dictionary\n",
    "                    camera_params[img_name] = {\n",
    "                        'fx': fx,\n",
    "                        'fy': fy,\n",
    "                        'w': w,\n",
    "                        'h': h,\n",
    "                        'cx': cx,\n",
    "                        'cy': cy\n",
    "                    }\n",
    "                except ValueError as e:\n",
    "                    print(f\"Error parsing line: {line}. Error: {e}\")\n",
    "            else:\n",
    "                print(f\"Line skipped due to incorrect format: {line}\")\n",
    "    \n",
    "    return camera_params\n",
    "    \n",
    "class cambridge_Dataset(MonocularDataset):\n",
    "    def __init__(self, args, path, config, data_folder, scene):\n",
    "        super().__init__(args, path, config)\n",
    "        self.has_depth = False\n",
    "        self.cambridge_Parser(data_folder, scene) \n",
    "        \n",
    "    def cambridge_Parser(self, data_folder, scene):\n",
    "        self.color_paths, self.poses, self.depth_paths, self.intrinsics, self.projection_matrices = [], [], [], [], []\n",
    "\n",
    "        gt_dirs = Path(data_folder) / f\"CambridgeLandmarks_Colmap_Retriangulated_1024px/{scene}/empty_all\"\n",
    "        _, images, _ = read_write_model.read_model(gt_dirs, \".txt\")\n",
    "        output_folder = data_folder.replace(\"datasets\", \"output\")\n",
    "        \n",
    "        # Read the filenames from test_fewshot.txt and store them in a set.\n",
    "        test_images_path = Path(data_folder) / scene / \"test_fewshot.txt\"\n",
    "        \n",
    "        with open(test_images_path, 'r') as f:\n",
    "            test_images = set(line.strip() for line in f)\n",
    "\n",
    "        intrinsics = parse_camera_params(f\"D:/gs-localization/output/cambridge/{scene}/query_list_with_intrinsics.txt\")\n",
    "\n",
    "        for i, image in tqdm(images.items(),\"Load dataset\"):\n",
    "            image_name = image.name.replace(\"/\",\"_\")\n",
    "            # Execute the following operation only if image.name exists in test_images.\"\n",
    "            if image_name in test_images:\n",
    "                image_path = Path(data_folder) / scene / 'images' / image_name\n",
    "                self.color_paths.append(image_path)\n",
    "                R_gt, t_gt = image.qvec2rotmat(), image.tvec\n",
    "                pose = np.eye(4)            \n",
    "                pose[:3, :3] = R_gt         \n",
    "                pose[:3, 3] = t_gt \n",
    "                self.poses.append(pose)\n",
    "                self.depth_paths.append(None)\n",
    "                \n",
    "                intrinsic = intrinsics[image_name]     \n",
    "                projection_matrix = getProjectionMatrix2(\n",
    "                    znear=0.01,\n",
    "                    zfar=100.0,\n",
    "                    fx=intrinsic[\"fx\"],\n",
    "                    fy=intrinsic[\"fy\"],\n",
    "                    cx=intrinsic[\"cx\"],\n",
    "                    cy=intrinsic[\"cy\"],\n",
    "                    W=intrinsic[\"w\"],\n",
    "                    H=intrinsic[\"h\"],\n",
    "                ).transpose(0, 1)\n",
    "    \n",
    "                self.intrinsics.append(np.array([\n",
    "                                       intrinsic[\"fx\"],\n",
    "                                       intrinsic[\"fy\"],\n",
    "                                       intrinsic[\"cx\"],\n",
    "                                       intrinsic[\"cy\"],\n",
    "                                       focal2fov(intrinsic[\"fx\"], intrinsic[\"w\"]),\n",
    "                                       focal2fov(intrinsic[\"fy\"], intrinsic[\"h\"]),\n",
    "                                       intrinsic[\"h\"], \n",
    "                                       intrinsic[\"w\"]\n",
    "                                        ]))\n",
    "                \n",
    "                self.projection_matrices.append(projection_matrix)\n",
    "\n",
    "\n",
    "        # Sort self.color_paths, self.poses, and self.depth_paths based on normal file name order\n",
    "        sorted_data = sorted(zip(self.color_paths, self.depth_paths, self.poses, \n",
    "                                 self.intrinsics, self.projection_matrices), key=lambda x: x[0].name)\n",
    "        self.color_paths, self.depth_paths, self.poses, self.intrinsics, self.projection_matrices = zip(*sorted_data)\n",
    "        del images\n",
    "\n",
    "with open(\"D:/gs-localization/gs_localization/pipelines/configs/mono/tum/fr3_office.yaml\", \"r\") as f:\n",
    "    cfg_special = yaml.full_load(f)\n",
    "\n",
    "inherit_from = \"D:/gs-localization/gs_localization/pipelines/configs/mono/tum/base_config.yaml\"\n",
    "\n",
    "if inherit_from is not None:\n",
    "    cfg = load_config(inherit_from)\n",
    "else:\n",
    "    cfg = dict()\n",
    "\n",
    "# merge per dataset cfg. and main cfg.\n",
    "data_folder = \"D:/gs-localization/datasets/cambridge\"\n",
    "config = update_recursive(cfg, cfg_special)\n",
    "config = cfg\n",
    "config[\"Training\"][\"monocular\"] = True\n",
    "config[\"Training\"][\"opacity_threshold\"] = 0.9\n",
    "config[\"Training\"][\"edge_threshold\"] = 1\n",
    "\n",
    "#for scene in ['GreatCourt', 'KingsCollege', 'OldHospital', 'ShopFacade', 'StMarysChurch']:\n",
    "for scene in ['GreatCourt']:\n",
    "    Model = GaussianModel(3, config)\n",
    "    Model.load_ply(f\"D:/gs-localization/output/cambridge/{scene}/gs_map/iteration_30000/point_cloud.ply\")\n",
    "    \n",
    "    model_params = munchify(config[\"model_params\"])\n",
    "    pipeline_params = munchify(config[\"pipeline_params\"])\n",
    "    data_folder = \"D:/gs-localization/datasets/cambridge\"\n",
    "    dataset = cambridge_Dataset(model_params, model_params.source_path, config, data_folder, scene)\n",
    "    bg_color = [0, 0, 0] \n",
    "    background = torch.tensor(bg_color, dtype=torch.float32, device=\"cuda\")\n",
    "    \n",
    "    # use OrderedDict to substitute defaultdict\n",
    "    test_infos = OrderedDict()\n",
    "    \n",
    "    # suppose file open and read\n",
    "    with open(f\"D:/gs-localization/output/cambridge/{scene}/results_sparse.txt\", \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            name = parts[0]\n",
    "            qvec = list(map(float, parts[1:5]))\n",
    "            tvec = list(map(float, parts[5:8]))\n",
    "\n",
    "            R = quat_to_rotmat(qvec)\n",
    "            T = np.array(tvec)\n",
    "    \n",
    "            # insert directly in OrderedDict\n",
    "            test_infos[name] = Transformation(R=R, T=T)\n",
    "    \n",
    "    # sort OrderedDict according to name \n",
    "    test_infos = OrderedDict(sorted(test_infos.items(), key=lambda item: item[0]))\n",
    "    \n",
    "    rot_errors = []\n",
    "    trans_errors = []\n",
    "    \n",
    "    file = h5py.File(f'D:/gs-localization/output/cambridge/{scene}/feats-superpoint-n4096-r1024.h5', 'r')\n",
    "    \n",
    "    for i, image in enumerate(tqdm(test_infos, desc=\"Localization\")):\n",
    "        viewpoint = Camera.init_from_intrinsic(dataset, i)\n",
    "\n",
    "        viewpoint.compute_grad_mask(config)\n",
    "        \n",
    "        group = file[image] \n",
    "        keypoints = group['keypoints'][group['scores'][:]>0.2]  \n",
    "        mask = create_mask(mkpts_lst=keypoints, width=1024, height=576, k=10)\n",
    "        viewpoint.grad_mask = viewpoint.grad_mask | torch.tensor(mask).to(\"cuda:0\")\n",
    "    \n",
    "        initial_R = torch.tensor(test_infos[image].R)\n",
    "        initial_T = torch.tensor(test_infos[image].T).squeeze()\n",
    "    \n",
    "        rotation_matrix, translation_vector, render_pkg = gradient_decent(viewpoint, config, initial_R, initial_T)\n",
    "        #rotation_matrix, translation_vector = initial_R, initial_T\n",
    "    \n",
    "        R_gt = viewpoint.R_gt.cpu().numpy()\n",
    "        t_gt = viewpoint.T_gt.reshape(3,1).cpu().numpy()\n",
    "        R = rotation_matrix.cpu().numpy()\n",
    "        t = translation_vector.reshape(3,1).cpu().numpy()\n",
    "        trans_error = np.linalg.norm(-R_gt.T @ t_gt + R.T @ t, axis=0)\n",
    "        cos = np.clip((np.trace(np.dot(R_gt.T, R)) - 1) / 2, -1.0, 1.0)\n",
    "        rot_error = np.rad2deg(np.abs(np.arccos(cos)))\n",
    "        #print(image, rot_error, trans_error)\n",
    "        rot_errors.append(rot_error)\n",
    "        trans_errors.append(trans_error)\n",
    "    \n",
    "    np.save(f\"D:/gs-localization/output/cambridge/{scene}/rot_errors.npy\", rot_errors)\n",
    "    np.save(f\"D:/gs-localization/output/cambridge/{scene}/trans_errors.npy\", trans_errors)\n",
    "    med_t = np.median(trans_errors)\n",
    "    med_R = np.median(rot_errors)\n",
    "    print( f\"\\nMedian errors for {scene}: {med_t:.3f}m, {med_R:.3f}deg\")\n",
    "    \n",
    "    threshs_t = [0.01, 0.02, 0.03, 0.05, 0.25, 0.5, 5.0]\n",
    "    threshs_R = [1.0, 2.0, 3.0, 5.0, 2.0, 5.0, 10.0]\n",
    "    for th_t, th_R in zip(threshs_t, threshs_R):\n",
    "        ratio = np.mean((np.array(trans_errors) < th_t) & (np.array(rot_errors) < th_R))\n",
    "        print(f\"\\n\\t{th_t*100:.0f}cm, {th_R:.0f}deg : {ratio*100:.2f}%\")\n",
    "        \n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36791f9-d378-4c63-ad05-e15ce5c69e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
