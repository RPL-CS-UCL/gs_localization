{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "58901cff-4b30-4d55-96f8-e5cc02ae7445",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: Camera(id=1, model='SIMPLE_PINHOLE', width=1008, height=756, params=array([857.11887943, 504.        , 378.        ]))}\n",
      "IMG_2998.JPG 0.04661545293510105 0.08656948739942903\n",
      "IMG_3007.JPG 0.04899758223045376 0.11719118078972326\n",
      "IMG_3015.JPG 0.07410958155865707 0.021438555476278952\n",
      "IMG_3023.JPG 0.04328194425789133 0.0802657327684691\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nSCENES = [\\'bicycle\\', \\'bonsai\\', \\'counter\\', \\'garden\\',  \\'kitchen\\', \\'room\\', \\'stump\\']\\n\\n\\nif __name__ == \"__main__\":\\n    parser = argparse.ArgumentParser()\\n    parser.add_argument(\"--scenes\", default=SCENES, choices=SCENES, nargs=\"+\")\\n    parser.add_argument(\"--overwrite\", action=\"store_true\")\\n    parser.add_argument(\\n        \"--dataset\",\\n        type=Path,\\n        default=\"datasets/360_v2\",\\n        help=\"Path to the dataset, default: %(default)s\",\\n    )\\n    parser.add_argument(\\n        \"--outputs\",\\n        type=Path,\\n        default=\"output/360_v2\",\\n        help=\"Path to the output directory, default: %(default)s\",\\n    )\\n\\n    parser.add_argument(\\n        \"--num_covis\",\\n        type=int,\\n        default=30,\\n        help=\"Number of image pairs for SfM, default: %(default)s\",\\n    )\\n\\n    parser.add_argument(\\n        \"--num_retrieve\",\\n        type=int,\\n        default=3,\\n        help=\"Number of images for retrieval, default: %(default)s\",\\n    )\\n    args = parser.parse_args()\\n\\n    gt_dirs = args.dataset / \"{scene}/sparse/0\" \\n    tr_dirs = args.dataset / \"{scene}/train_views/triangulated\" \\n\\n    for scene in args.scenes:\\n        logger.info(f\\'Working on scene \"{scene}\".\\')\\n        if args.overwrite or True:\\n            run_scene(\\n                args.dataset / scene / \"images_4\",\\n                Path(str(gt_dirs).format(scene=scene)),\\n                Path(str(tr_dirs).format(scene=scene)), \\n                args.dataset / scene,\\n                args.outputs / scene,\\n                args.num_covis,\\n                args.num_retrieve)\\n\\n'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import glob\n",
    "import os\n",
    "import h5py\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import trimesh\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from argparse import ArgumentParser\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "import yaml\n",
    "import numpy as np\n",
    "from munch import munchify\n",
    "import wandb\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"D:/gs-localization/gaussian_splatting\")\n",
    "sys.path.append(\"D:/gs-localization\")\n",
    "from tools.gaussian_model import GaussianModel\n",
    "from gaussian_splatting.utils.system_utils import mkdir_p\n",
    "from tools.config_utils import load_config, set_config, update_recursive\n",
    "from tools.dataset import v2_360_Dataset\n",
    "from tools.multiprocessing_utils import FakeQueue\n",
    "from tools import read_write_model\n",
    "from tools.eval_utils import rotation_error, translation_error\n",
    "\n",
    "def set_config(tr_dirs, config):\n",
    "    cameras, _, _ = read_write_model.read_model(tr_dirs, \".bin\")\n",
    "    config[\"Dataset\"][\"dataset_path\"] = tr_dirs\n",
    "    print(cameras)\n",
    "    config[\"Dataset\"][\"Calibration\"][\"fx\"] = cameras[1][4][0]\n",
    "    config[\"Dataset\"][\"Calibration\"][\"fy\"] = cameras[1][4][0]\n",
    "    config[\"Dataset\"][\"Calibration\"][\"cx\"] = cameras[1][4][1]\n",
    "    config[\"Dataset\"][\"Calibration\"][\"cy\"] = cameras[1][4][2]\n",
    "    config[\"Dataset\"][\"Calibration\"][\"width\"] = cameras[1][2]\n",
    "    config[\"Dataset\"][\"Calibration\"][\"height\"] = cameras[1][3]\n",
    "    return config\n",
    "    \n",
    "with open(\"configs/mono/tum/fr3_office.yaml\", \"r\") as f:\n",
    "    cfg_special = yaml.full_load(f)\n",
    "\n",
    "inherit_from = \"configs/mono/tum/base_config.yaml\"\n",
    "\n",
    "if inherit_from is not None:\n",
    "    cfg = load_config(inherit_from)\n",
    "else:\n",
    "    cfg = dict()\n",
    "\n",
    "# merge per dataset cfg. and main cfg.\n",
    "config = update_recursive(cfg, cfg_special)\n",
    "config = cfg\n",
    "    \n",
    "data_folder = \"D:/gs-localization/datasets/nerf_llff_data\"\n",
    "scene = \"leaves\"\n",
    "tr_dirs = Path(data_folder) / scene / \"train_views/triangulated\"\n",
    "config = set_config(tr_dirs, config)\n",
    "\n",
    "Model = GaussianModel(3, config)\n",
    "#Model.load_ply(\"C:/Users/27118/Desktop/master_project/RaDe-GS/output/26b22380-1/point_cloud/iteration_30000/point_cloud.ply\")\n",
    "#Model.load_ply(\"D:/gaussian-splatting/output/73bdba8c-0/point_cloud/iteration_25000/point_cloud.ply\")\n",
    "Model.load_ply(f\"D:/gs-localization/output/nerf_llff_data/{scene}/gs_map/iteration_30000/point_cloud.ply\")\n",
    "\n",
    "model_params = munchify(config[\"model_params\"])\n",
    "pipeline_params = munchify(config[\"pipeline_params\"])\n",
    "data_folder = \"D:/gs-localization/datasets/nerf_llff_data\"\n",
    "dataset = v2_360_Dataset(model_params, model_params.source_path, config, data_folder, scene)\n",
    "bg_color = [0, 0, 0] \n",
    "background = torch.tensor(bg_color, dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "from gaussian_splatting.utils.graphics_utils import getProjectionMatrix2, getWorld2View2\n",
    "from tools import render\n",
    "from tools.slam_utils import image_gradient, image_gradient_mask\n",
    "from tools.camera_utils import Camera\n",
    "from tools.slam_utils import get_loss_tracking, get_median_depth\n",
    "from tools.pose_utils import update_pose\n",
    "\n",
    "projection_matrix = getProjectionMatrix2(\n",
    "    znear=0.01,\n",
    "    zfar=100.0,\n",
    "    fx=dataset.fx,\n",
    "    fy=dataset.fy,\n",
    "    cx=dataset.cx,\n",
    "    cy=dataset.cy,\n",
    "    W=dataset.width,\n",
    "    H=dataset.height,\n",
    ").transpose(0, 1)\n",
    "projection_matrix = projection_matrix.to(device=\"cuda:0\")\n",
    "\n",
    "config[\"Training\"][\"opacity_threshold\"] = 0.5\n",
    "config[\"Training\"][\"edge_threshold\"] = 0.8\n",
    "from time import time\n",
    "\n",
    "def gradient_decent(viewpoint, config, initial_R, initial_T):\n",
    "\n",
    "    viewpoint.update_RT(initial_R, initial_T)\n",
    "    \n",
    "    opt_params = []\n",
    "    opt_params.append(\n",
    "        {\n",
    "            \"params\": [viewpoint.cam_rot_delta],\n",
    "            \"lr\": 0.0001,\n",
    "            \"name\": \"rot_{}\".format(viewpoint.uid),\n",
    "        }\n",
    "    )\n",
    "    opt_params.append(\n",
    "        {\n",
    "            \"params\": [viewpoint.cam_trans_delta],\n",
    "            \"lr\": 0.001,\n",
    "            \"name\": \"trans_{}\".format(viewpoint.uid),\n",
    "        }\n",
    "    )\n",
    "    opt_params.append(\n",
    "        {\n",
    "            \"params\": [viewpoint.exposure_a],\n",
    "            \"lr\": 0.001,\n",
    "            \"name\": \"exposure_a_{}\".format(viewpoint.uid),\n",
    "        }\n",
    "    )\n",
    "    opt_params.append(\n",
    "        {\n",
    "            \"params\": [viewpoint.exposure_b],\n",
    "            \"lr\": 0.001,\n",
    "            \"name\": \"exposure_b_{}\".format(viewpoint.uid),\n",
    "        }\n",
    "    )\n",
    "    \n",
    "\n",
    "    pose_optimizer = torch.optim.Adam(opt_params)\n",
    "    \n",
    "    for tracking_itr in range(100):\n",
    "        \n",
    "        render_pkg = render(\n",
    "            viewpoint, Model, pipeline_params, background\n",
    "        )\n",
    "        \n",
    "        image, depth, opacity = (\n",
    "            render_pkg[\"render\"],\n",
    "            render_pkg[\"depth\"],\n",
    "            render_pkg[\"opacity\"],\n",
    "        )\n",
    "          \n",
    "        pose_optimizer.zero_grad()\n",
    "        \n",
    "        loss_tracking = get_loss_tracking(\n",
    "            config, image, depth, opacity, viewpoint\n",
    "        )\n",
    "        loss_tracking.backward()\n",
    "        \n",
    "    \n",
    "        with torch.no_grad():\n",
    "            pose_optimizer.step()\n",
    "            converged = update_pose(viewpoint, converged_threshold=1e-5)\n",
    "    \n",
    "        if converged:\n",
    "            break\n",
    "             \n",
    "    return viewpoint.R, viewpoint.T, render_pkg\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class Transformation:\n",
    "    def __init__(self, R=None, T=None):\n",
    "        self.R = R\n",
    "        self.T = T\n",
    "\n",
    "test_infos = defaultdict(Transformation)\n",
    "\n",
    "def quat_to_rotmat(qvec):\n",
    "    qvec = np.array(qvec, dtype=float)\n",
    "    w, x, y, z = qvec\n",
    "    R = np.array([\n",
    "        [1 - 2*y**2 - 2*z**2, 2*x*y - 2*z*w, 2*x*z + 2*y*w],\n",
    "        [2*x*y + 2*z*w, 1 - 2*x**2 - 2*z**2, 2*y*z - 2*x*w],\n",
    "        [2*x*z - 2*y*w, 2*y*z + 2*x*w, 1 - 2*x**2 - 2*y**2]\n",
    "    ])\n",
    "    return R\n",
    "\n",
    "with open(f\"D:/gs-localization/output/nerf_llff_data/{scene}/results_sparse.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        name = parts[0]\n",
    "        qvec = list(map(float, parts[1:5]))\n",
    "        tvec = list(map(float, parts[5:8]))\n",
    "\n",
    "        R = quat_to_rotmat(qvec)\n",
    "        T = np.array(tvec)\n",
    "\n",
    "        test_infos[name].R = R\n",
    "        test_infos[name].T = T\n",
    "\n",
    "\n",
    "def create_mask(mkpts_lst, width, height, k):\n",
    "    # 初始化 mask，全为 False\n",
    "    mask = np.zeros((height, width), dtype=bool)\n",
    "    \n",
    "    # 计算 k 的半径\n",
    "    half_k = k // 2\n",
    "    \n",
    "    # 遍历所有点\n",
    "    for pt in mkpts_lst:\n",
    "        x, y = int(pt[0]), int(pt[1])\n",
    "        \n",
    "        # 计算 k*k 区域的边界\n",
    "        x_min = max(0, x - half_k)\n",
    "        x_max = min(width, x + half_k + 1)\n",
    "        y_min = max(0, y - half_k)\n",
    "        y_max = min(height, y + half_k + 1)\n",
    "        \n",
    "        # 设置 mask 中的 k*k 区域为 True\n",
    "        mask[y_min:y_max, x_min:x_max] = True\n",
    "    \n",
    "    # 形状为 (1, height, width)\n",
    "    mask = mask[np.newaxis, :, :]\n",
    "    \n",
    "    return mask\n",
    "\n",
    "rot_errors = []\n",
    "trans_errors = []\n",
    "\n",
    "file = h5py.File(f'D:/gs-localization/output/nerf_llff_data/{scene}/feats-superpoint-n4096-r1024.h5', 'r')\n",
    "\n",
    "for i, image in enumerate(test_infos):\n",
    "    viewpoint = Camera.init_from_dataset(dataset, i, projection_matrix)\n",
    "\n",
    "    viewpoint.compute_grad_mask(config)\n",
    "\n",
    "    group = file[image] \n",
    "    keypoints = group['keypoints'][group['scores'][:]>0.2]  \n",
    "    mask = create_mask(mkpts_lst=keypoints, width=dataset.width, height=dataset.height, k=20)\n",
    "    viewpoint.grad_mask = viewpoint.grad_mask | torch.tensor(mask).to(\"cuda:0\")\n",
    "\n",
    "    viewpoint.grad_mask[:,:20,:] = 0; viewpoint.grad_mask[:,-20:,:] = 0\n",
    "    viewpoint.grad_mask[:,:,:50] = 0; viewpoint.grad_mask[:,:,-50:] = 0\n",
    "\n",
    "    config[\"Training\"][\"monocular\"] = True\n",
    "\n",
    "    initial_R = torch.tensor(test_infos[image].R)\n",
    "    initial_T = torch.tensor(test_infos[image].T).squeeze()\n",
    "\n",
    "    rotation_matrix, translation_vector, render_pkg = gradient_decent(viewpoint, config, initial_R, initial_T)\n",
    "    \n",
    "    #rotation_matrix, translation_vector = initial_R, initial_T\n",
    "    \n",
    "    rot_error = rotation_error(rotation_matrix.cpu().numpy(), viewpoint.R_gt.cpu().numpy())\n",
    "    trans_error = translation_error(translation_vector.reshape(3,1).cpu().numpy(), viewpoint.T_gt.reshape(3,1).cpu().numpy())\n",
    "\n",
    "    print(image, rot_error, trans_error)\n",
    "    rot_errors.append(rot_error)\n",
    "    trans_errors.append(trans_error)\n",
    "\n",
    "file.close()\n",
    "\n",
    "\"\"\"\n",
    "SCENES = ['bicycle', 'bonsai', 'counter', 'garden',  'kitchen', 'room', 'stump']\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--scenes\", default=SCENES, choices=SCENES, nargs=\"+\")\n",
    "    parser.add_argument(\"--overwrite\", action=\"store_true\")\n",
    "    parser.add_argument(\n",
    "        \"--dataset\",\n",
    "        type=Path,\n",
    "        default=\"datasets/360_v2\",\n",
    "        help=\"Path to the dataset, default: %(default)s\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--outputs\",\n",
    "        type=Path,\n",
    "        default=\"output/360_v2\",\n",
    "        help=\"Path to the output directory, default: %(default)s\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--num_covis\",\n",
    "        type=int,\n",
    "        default=30,\n",
    "        help=\"Number of image pairs for SfM, default: %(default)s\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--num_retrieve\",\n",
    "        type=int,\n",
    "        default=3,\n",
    "        help=\"Number of images for retrieval, default: %(default)s\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    gt_dirs = args.dataset / \"{scene}/sparse/0\" \n",
    "    tr_dirs = args.dataset / \"{scene}/train_views/triangulated\" \n",
    "\n",
    "    for scene in args.scenes:\n",
    "        logger.info(f'Working on scene \"{scene}\".')\n",
    "        if args.overwrite or True:\n",
    "            run_scene(\n",
    "                args.dataset / scene / \"images_4\",\n",
    "                Path(str(gt_dirs).format(scene=scene)),\n",
    "                Path(str(tr_dirs).format(scene=scene)), \n",
    "                args.dataset / scene,\n",
    "                args.outputs / scene,\n",
    "                args.num_covis,\n",
    "                args.num_retrieve)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "08473371-f91d-4684-b4c6-24e55419cbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Median errors: 0.082m, 0.060deg\n",
      "\n",
      "\t1cm, 1deg : 0.00%\n",
      "\n",
      "\t2cm, 2deg : 25.00%\n",
      "\n",
      "\t3cm, 3deg : 25.00%\n",
      "\n",
      "\t5cm, 5deg : 25.00%\n",
      "\n",
      "\t25cm, 2deg : 100.00%\n",
      "\n",
      "\t50cm, 5deg : 100.00%\n",
      "\n",
      "\t500cm, 10deg : 100.00%\n"
     ]
    }
   ],
   "source": [
    "med_t = np.median(trans_errors)\n",
    "med_R = np.median(rot_errors)\n",
    "print( f\"\\nMedian errors: {med_t:.3f}m, {med_R:.3f}deg\")\n",
    "\n",
    "threshs_t = [0.01, 0.02, 0.03, 0.05, 0.25, 0.5, 5.0]\n",
    "threshs_R = [1.0, 2.0, 3.0, 5.0, 2.0, 5.0, 10.0]\n",
    "for th_t, th_R in zip(threshs_t, threshs_R):\n",
    "    ratio = np.mean((np.array(trans_errors) < th_t) & (np.array(rot_errors) < th_R))\n",
    "    print(f\"\\n\\t{th_t*100:.0f}cm, {th_R:.0f}deg : {ratio*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e825978e-e63c-464b-9479-dac429bd7f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of mkpts_lst: (2205, 2)\n",
      "Keypoints (first few): \n",
      "[[  74.25    9.83]\n",
      " [ 108.3     9.83]\n",
      " [ 452.5     9.83]\n",
      " [ 541.      9.83]\n",
      " [ 587.5     9.83]\n",
      " [ 631.      9.83]\n",
      " [ 648.      9.83]\n",
      " [ 694.5     9.83]\n",
      " [1042.      9.83]\n",
      " [1192.      9.83]]\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "# 打开 HDF5 文件\n",
    "with h5py.File('D:/gs-localization/output/360_v2/stump/feats-superpoint-n4096-r1024.h5', 'r') as file:\n",
    "    # 指定要读取的图像 key\n",
    "    target_image_key = '_DSC9266.JPG'\n",
    "    \n",
    "    # 获取该图像对应的组\n",
    "    group = file[target_image_key]\n",
    "    \n",
    "    # 提取 keypoints 数据集并存储为 mkpts_lst\n",
    "    mkpts_lst = group['keypoints'][:]\n",
    "    \n",
    "    # 打印结果确认\n",
    "    print(f\"Shape of mkpts_lst: {mkpts_lst.shape}\")\n",
    "    print(f\"Keypoints (first few): \\n{mkpts_lst[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2634ebd3-1a84-47d3-8ab1-8b4249e820cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 74.25,   9.83],\n",
       "       [108.3 ,   9.83],\n",
       "       [452.5 ,   9.83],\n",
       "       ...,\n",
       "       [844.  , 805.5 ],\n",
       "       [893.5 , 805.5 ],\n",
       "       [898.5 , 805.5 ]], dtype=float16)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mkpts_lst[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "207f9756-b2a7-46c6-aa37-a296b8c06b6a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys:  ['_DSC8679.JPG', '_DSC8680.JPG', '_DSC8681.JPG', '_DSC8682.JPG', '_DSC8683.JPG', '_DSC8684.JPG', '_DSC8685.JPG', '_DSC8686.JPG', '_DSC8687.JPG', '_DSC8688.JPG', '_DSC8689.JPG', '_DSC8690.JPG', '_DSC8691.JPG', '_DSC8692.JPG', '_DSC8693.JPG', '_DSC8694.JPG', '_DSC8695.JPG', '_DSC8696.JPG', '_DSC8697.JPG', '_DSC8698.JPG', '_DSC8699.JPG', '_DSC8700.JPG', '_DSC8701.JPG', '_DSC8702.JPG', '_DSC8703.JPG', '_DSC8704.JPG', '_DSC8705.JPG', '_DSC8706.JPG', '_DSC8707.JPG', '_DSC8708.JPG', '_DSC8709.JPG', '_DSC8710.JPG', '_DSC8711.JPG', '_DSC8712.JPG', '_DSC8713.JPG', '_DSC8714.JPG', '_DSC8715.JPG', '_DSC8716.JPG', '_DSC8717.JPG', '_DSC8718.JPG', '_DSC8719.JPG', '_DSC8720.JPG', '_DSC8721.JPG', '_DSC8722.JPG', '_DSC8723.JPG', '_DSC8724.JPG', '_DSC8725.JPG', '_DSC8726.JPG', '_DSC8727.JPG', '_DSC8728.JPG', '_DSC8729.JPG', '_DSC8730.JPG', '_DSC8731.JPG', '_DSC8732.JPG', '_DSC8733.JPG', '_DSC8734.JPG', '_DSC8735.JPG', '_DSC8736.JPG', '_DSC8737.JPG', '_DSC8738.JPG', '_DSC8739.JPG', '_DSC8741.JPG', '_DSC8742.JPG', '_DSC8743.JPG', '_DSC8744.JPG', '_DSC8745.JPG', '_DSC8746.JPG', '_DSC8747.JPG', '_DSC8748.JPG', '_DSC8749.JPG', '_DSC8750.JPG', '_DSC8751.JPG', '_DSC8752.JPG', '_DSC8753.JPG', '_DSC8754.JPG', '_DSC8755.JPG', '_DSC8756.JPG', '_DSC8757.JPG', '_DSC8758.JPG', '_DSC8759.JPG', '_DSC8760.JPG', '_DSC8761.JPG', '_DSC8762.JPG', '_DSC8763.JPG', '_DSC8764.JPG', '_DSC8765.JPG', '_DSC8766.JPG', '_DSC8767.JPG', '_DSC8768.JPG', '_DSC8769.JPG', '_DSC8770.JPG', '_DSC8771.JPG', '_DSC8772.JPG', '_DSC8773.JPG', '_DSC8774.JPG', '_DSC8775.JPG', '_DSC8776.JPG', '_DSC8777.JPG', '_DSC8778.JPG', '_DSC8779.JPG', '_DSC8780.JPG', '_DSC8781.JPG', '_DSC8782.JPG', '_DSC8783.JPG', '_DSC8784.JPG', '_DSC8785.JPG', '_DSC8786.JPG', '_DSC8787.JPG', '_DSC8788.JPG', '_DSC8789.JPG', '_DSC8790.JPG', '_DSC8791.JPG', '_DSC8792.JPG', '_DSC8793.JPG', '_DSC8794.JPG', '_DSC8795.JPG', '_DSC8796.JPG', '_DSC8797.JPG', '_DSC8798.JPG', '_DSC8799.JPG', '_DSC8800.JPG', '_DSC8801.JPG', '_DSC8802.JPG', '_DSC8803.JPG', '_DSC8804.JPG', '_DSC8805.JPG', '_DSC8806.JPG', '_DSC8807.JPG', '_DSC8808.JPG', '_DSC8809.JPG', '_DSC8810.JPG', '_DSC8811.JPG', '_DSC8812.JPG', '_DSC8813.JPG', '_DSC8814.JPG', '_DSC8815.JPG', '_DSC8816.JPG', '_DSC8817.JPG', '_DSC8818.JPG', '_DSC8819.JPG', '_DSC8820.JPG', '_DSC8821.JPG', '_DSC8822.JPG', '_DSC8823.JPG', '_DSC8824.JPG', '_DSC8825.JPG', '_DSC8826.JPG', '_DSC8827.JPG', '_DSC8828.JPG', '_DSC8829.JPG', '_DSC8830.JPG', '_DSC8831.JPG', '_DSC8832.JPG', '_DSC8833.JPG', '_DSC8834.JPG', '_DSC8835.JPG', '_DSC8836.JPG', '_DSC8837.JPG', '_DSC8838.JPG', '_DSC8839.JPG', '_DSC8840.JPG', '_DSC8841.JPG', '_DSC8842.JPG', '_DSC8843.JPG', '_DSC8844.JPG', '_DSC8845.JPG', '_DSC8846.JPG', '_DSC8847.JPG', '_DSC8848.JPG', '_DSC8849.JPG', '_DSC8850.JPG', '_DSC8851.JPG', '_DSC8852.JPG', '_DSC8853.JPG', '_DSC8854.JPG', '_DSC8855.JPG', '_DSC8856.JPG', '_DSC8857.JPG', '_DSC8858.JPG', '_DSC8859.JPG', '_DSC8860.JPG', '_DSC8861.JPG', '_DSC8862.JPG', '_DSC8863.JPG', '_DSC8864.JPG', '_DSC8865.JPG', '_DSC8866.JPG', '_DSC8867.JPG', '_DSC8868.JPG', '_DSC8869.JPG', '_DSC8870.JPG', '_DSC8871.JPG', '_DSC8872.JPG', '_DSC8873.JPG']\n",
      "Shape of descriptors: (256, 1638)\n",
      "Descriptors (first few): \n",
      "[[-6.2469e-02  8.0994e-02  1.1676e-01 ... -7.6561e-03  4.5380e-02\n",
      "   7.7515e-02]\n",
      " [-3.7720e-02  9.1919e-02  2.6642e-02 ...  5.3070e-02  1.9150e-02\n",
      "  -3.7251e-03]\n",
      " [-2.8229e-02  4.6692e-02 -6.3538e-02 ...  9.2529e-02  2.1594e-01\n",
      "   1.5686e-01]\n",
      " ...\n",
      " [ 2.6321e-02 -2.3956e-02 -4.7266e-05 ... -1.0492e-01 -4.7607e-02\n",
      "  -9.9640e-03]\n",
      " [ 1.5771e-01  2.3880e-02 -5.1453e-02 ... -4.1016e-02 -7.0374e-02\n",
      "  -4.5929e-02]\n",
      " [ 3.1860e-02 -1.1267e-01 -4.0863e-02 ...  9.0759e-02  1.7126e-01\n",
      "   1.3379e-01]]\n",
      "Image size: [1237  822]\n",
      "Shape of keypoints: (1638, 2)\n",
      "Keypoints (first few): \n",
      "[[  23.06     9.77 ]\n",
      " [ 230.9      9.77 ]\n",
      " [ 420.5      9.77 ]\n",
      " [ 871.       9.77 ]\n",
      " [ 971.5      9.77 ]\n",
      " [1031.       9.77 ]\n",
      " [1058.       9.77 ]\n",
      " [ 314.2     10.984]\n",
      " [ 437.5     10.984]\n",
      " [ 528.      10.984]]\n",
      "Shape of scores: (1638,)\n",
      "Scores (first few): \n",
      "[0.01443  0.007637 0.006588 0.00876  0.065    0.006542 0.04175  0.4375\n",
      " 0.01471  0.2085  ]\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "# 打开 HDF5 文件\n",
    "with h5py.File('D:/gs-localization/output/360_v2/bicycle/feats-superpoint-n4096-r1024.h5', 'r') as file:\n",
    "    # 查看文件中的所有顶层组（类似于文件夹）\n",
    "    keys = list(file.keys())\n",
    "    print(\"Keys: \", keys)\n",
    "    \n",
    "    # 假设你想读取'_DSC8679.JPG'组中的数据\n",
    "    first_image_key = keys[0]  # '_DSC8679.JPG'\n",
    "    group = file[first_image_key]  # 获取组对象\n",
    "    \n",
    "    # 读取'descriptors'数据集\n",
    "    descriptors = group['descriptors'][:]\n",
    "    print(f\"Shape of descriptors: {descriptors.shape}\")\n",
    "    print(f\"Descriptors (first few): \\n{descriptors[:10]}\")\n",
    "    \n",
    "    # 读取'image_size'数据集\n",
    "    image_size = group['image_size'][:]\n",
    "    print(f\"Image size: {image_size}\")\n",
    "    \n",
    "    # 读取'keypoints'数据集\n",
    "    keypoints = group['keypoints'][:]\n",
    "    print(f\"Shape of keypoints: {keypoints.shape}\")\n",
    "    print(f\"Keypoints (first few): \\n{keypoints[:10]}\")\n",
    "    print(keypoints[:][])\n",
    "    \n",
    "    # 读取'scores'数据集\n",
    "    scores = group['scores'][:]\n",
    "    print(f\"Shape of scores: {scores.shape}\")\n",
    "    print(f\"Scores (first few): \\n{scores[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "301520a4-e52d-4960-8189-1aef5c77c315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: Camera(id=1, model='SIMPLE_PINHOLE', width=1008, height=756, params=array([815.13158322, 504.        , 378.        ]))}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_write_model.read_cameras_binary(\"D:/gs-localization/datasets/nerf_llff_data/fern/train_views/triangulated/cameras.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717aae78-d534-40de-8e7f-4ac6457e2059",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
