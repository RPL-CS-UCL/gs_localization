{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "39115ecb-6ee0-43cc-aa48-f1ed30f91759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: Camera(id=1, model='SIMPLE_PINHOLE', width=1008, height=756, params=array([815.13158322, 504.        , 378.        ]))}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import glob\n",
    "import os\n",
    "import h5py\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import trimesh\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from argparse import ArgumentParser\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import yaml\n",
    "import numpy as np\n",
    "from munch import munchify\n",
    "import wandb\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"D:/gs-localization/gaussian_splatting\")\n",
    "sys.path.append(\"D:/gs-localization\")\n",
    "sys.path.append(\"D:/gs-localization/gs_localization/pipelines\")\n",
    "from tools.gaussian_model import GaussianModel\n",
    "from tools.config_utils import load_config, set_config, update_recursive\n",
    "from tools.dataset import v2_360_Dataset\n",
    "from tools import read_write_model\n",
    "from tools.eval_utils import rotation_error, translation_error\n",
    "\n",
    "def set_config(tr_dirs, config):\n",
    "    cameras, _, _ = read_write_model.read_model(tr_dirs, \".bin\")\n",
    "    config[\"Dataset\"][\"dataset_path\"] = tr_dirs\n",
    "    print(cameras)\n",
    "    config[\"Dataset\"][\"Calibration\"][\"fx\"] = cameras[1][4][0]\n",
    "    config[\"Dataset\"][\"Calibration\"][\"fy\"] = cameras[1][4][0]\n",
    "    config[\"Dataset\"][\"Calibration\"][\"cx\"] = cameras[1][4][1]\n",
    "    config[\"Dataset\"][\"Calibration\"][\"cy\"] = cameras[1][4][2]\n",
    "    config[\"Dataset\"][\"Calibration\"][\"width\"] = cameras[1][2]\n",
    "    config[\"Dataset\"][\"Calibration\"][\"height\"] = cameras[1][3]\n",
    "    return config\n",
    "    \n",
    "with open(\"configs/mono/tum/fr3_office.yaml\", \"r\") as f:\n",
    "    cfg_special = yaml.full_load(f)\n",
    "\n",
    "inherit_from = \"configs/mono/tum/base_config.yaml\"\n",
    "\n",
    "if inherit_from is not None:\n",
    "    cfg = load_config(inherit_from)\n",
    "else:\n",
    "    cfg = dict()\n",
    "\n",
    "# merge per dataset cfg. and main cfg.\n",
    "config = update_recursive(cfg, cfg_special)\n",
    "config = cfg\n",
    "    \n",
    "data_folder = \"D:/gs-localization/datasets/nerf_llff_data\"\n",
    "scene = \"fern\"\n",
    "tr_dirs = Path(data_folder) / scene / \"train_views/triangulated\"\n",
    "config = set_config(tr_dirs, config)\n",
    "\n",
    "Model = GaussianModel(3, config)\n",
    "#Model.load_ply(\"C:/Users/27118/Desktop/master_project/RaDe-GS/output/26b22380-1/point_cloud/iteration_30000/point_cloud.ply\")\n",
    "#Model.load_ply(\"D:/gaussian-splatting/output/73bdba8c-0/point_cloud/iteration_25000/point_cloud.ply\")\n",
    "Model.load_ply(f\"D:/gs-localization/output/nerf_llff_data/{scene}/gs_map/iteration_30000/point_cloud.ply\")\n",
    "\n",
    "model_params = munchify(config[\"model_params\"])\n",
    "pipeline_params = munchify(config[\"pipeline_params\"])\n",
    "data_folder = \"D:/gs-localization/datasets/nerf_llff_data\"\n",
    "dataset = v2_360_Dataset(model_params, model_params.source_path, config, data_folder, scene)\n",
    "bg_color = [0, 0, 0] \n",
    "background = torch.tensor(bg_color, dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "from gaussian_splatting.utils.graphics_utils import getProjectionMatrix2, getWorld2View2\n",
    "from tools import render\n",
    "from tools.descent_utils import image_gradient, image_gradient_mask\n",
    "from tools.camera_utils import Camera\n",
    "from tools.descent_utils import get_loss_tracking\n",
    "from tools.pose_utils import update_pose\n",
    "\n",
    "projection_matrix = getProjectionMatrix2(\n",
    "    znear=0.01,\n",
    "    zfar=100.0,\n",
    "    fx=dataset.fx,\n",
    "    fy=dataset.fy,\n",
    "    cx=dataset.cx,\n",
    "    cy=dataset.cy,\n",
    "    W=dataset.width,\n",
    "    H=dataset.height,\n",
    ").transpose(0, 1)\n",
    "projection_matrix = projection_matrix.to(device=\"cuda:0\")\n",
    "\n",
    "config[\"Training\"][\"opacity_threshold\"] = 0.5\n",
    "config[\"Training\"][\"edge_threshold\"] = 0.8\n",
    "from time import time\n",
    "\n",
    "def gradient_decent(viewpoint, config, initial_R, initial_T):\n",
    "\n",
    "    viewpoint.update_RT(initial_R, initial_T)\n",
    "    \n",
    "    opt_params = []\n",
    "    opt_params.append(\n",
    "        {\n",
    "            \"params\": [viewpoint.cam_rot_delta],\n",
    "            \"lr\": 0.0001,\n",
    "            \"name\": \"rot_{}\".format(viewpoint.uid),\n",
    "        }\n",
    "    )\n",
    "    opt_params.append(\n",
    "        {\n",
    "            \"params\": [viewpoint.cam_trans_delta],\n",
    "            \"lr\": 0.001,\n",
    "            \"name\": \"trans_{}\".format(viewpoint.uid),\n",
    "        }\n",
    "    )\n",
    "    opt_params.append(\n",
    "        {\n",
    "            \"params\": [viewpoint.exposure_a],\n",
    "            \"lr\": 0.001,\n",
    "            \"name\": \"exposure_a_{}\".format(viewpoint.uid),\n",
    "        }\n",
    "    )\n",
    "    opt_params.append(\n",
    "        {\n",
    "            \"params\": [viewpoint.exposure_b],\n",
    "            \"lr\": 0.001,\n",
    "            \"name\": \"exposure_b_{}\".format(viewpoint.uid),\n",
    "        }\n",
    "    )\n",
    "    \n",
    "\n",
    "    pose_optimizer = torch.optim.Adam(opt_params)\n",
    "    \n",
    "    for tracking_itr in range(100):\n",
    "        \n",
    "        render_pkg = render(\n",
    "            viewpoint, Model, pipeline_params, background\n",
    "        )\n",
    "        \n",
    "        image, depth, opacity = (\n",
    "            render_pkg[\"render\"],\n",
    "            render_pkg[\"depth\"],\n",
    "            render_pkg[\"opacity\"],\n",
    "        )\n",
    "          \n",
    "        pose_optimizer.zero_grad()\n",
    "        \n",
    "        loss_tracking = get_loss_tracking(\n",
    "            config, image, depth, opacity, viewpoint\n",
    "        )\n",
    "        loss_tracking.backward()\n",
    "        \n",
    "    \n",
    "        with torch.no_grad():\n",
    "            pose_optimizer.step()\n",
    "            converged = update_pose(viewpoint, converged_threshold=1e-5)\n",
    "    \n",
    "        if converged:\n",
    "            break\n",
    "             \n",
    "    return viewpoint.R, viewpoint.T, render_pkg\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class Transformation:\n",
    "    def __init__(self, R=None, T=None):\n",
    "        self.R = R\n",
    "        self.T = T\n",
    "\n",
    "test_infos = defaultdict(Transformation)\n",
    "def quat_to_rotmat(qvec):\n",
    "    qvec = np.array(qvec, dtype=float)\n",
    "    w, x, y, z = qvec\n",
    "    R = np.array([\n",
    "        [1 - 2*y**2 - 2*z**2, 2*x*y - 2*z*w, 2*x*z + 2*y*w],\n",
    "        [2*x*y + 2*z*w, 1 - 2*x**2 - 2*z**2, 2*y*z - 2*x*w],\n",
    "        [2*x*z - 2*y*w, 2*y*z + 2*x*w, 1 - 2*x**2 - 2*y**2]\n",
    "    ])\n",
    "    return R\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e3b608d-eff8-4c8c-9002-c913f4a7b31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\27118/.cache\\torch\\hub\\intel-isl_MiDaS_master\n",
      "D:\\anaconda3\\envs\\gaussian_splatting\\lib\\site-packages\\timm\\models\\_factory.py:117: UserWarning: Mapping deprecated model name vit_base_resnet50_384 to current vit_base_r50_s16_384.orig_in21k_ft_in1k.\n",
      "  model = create_fn(\n",
      "Using cache found in C:\\Users\\27118/.cache\\torch\\hub\\intel-isl_MiDaS_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import glob\n",
    "import os\n",
    "import h5py\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import trimesh\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from argparse import ArgumentParser\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import yaml\n",
    "import numpy as np\n",
    "from munch import munchify\n",
    "import wandb\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"D:/gs-localization/gaussian_splatting\")\n",
    "sys.path.append(\"D:/gs-localization\")\n",
    "sys.path.append(\"D:/gs-localization/gs_localization/pipelines\")\n",
    "from tools.gaussian_model import GaussianModel\n",
    "from tools.config_utils import load_config, set_config, update_recursive\n",
    "from tools.dataset import v2_360_Dataset\n",
    "from tools import read_write_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "156c4792-6e32-436a-9595-1b6b2eed3c57",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'D:/gs-localization/output/7scenes_full_dslam/chess/sfm_superpoint+superglue'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cameras \u001b[38;5;241m=\u001b[39m \u001b[43mread_write_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_cameras_binary\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD:/gs-localization/output/7scenes_full_dslam/chess/sfm_superpoint+superglue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\gs-localization\\gs_localization\\pipelines\\tools\\read_write_model.py:143\u001b[0m, in \u001b[0;36mread_cameras_binary\u001b[1;34m(path_to_model_file)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;124;03msee: src/colmap/scene/reconstruction.cc\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;124;03m    void Reconstruction::WriteCamerasBinary(const std::string& path)\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    void Reconstruction::ReadCamerasBinary(const std::string& path)\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m cameras \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath_to_model_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fid:\n\u001b[0;32m    144\u001b[0m     num_cameras \u001b[38;5;241m=\u001b[39m read_next_bytes(fid, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQ\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_cameras):\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'D:/gs-localization/output/7scenes_full_dslam/chess/sfm_superpoint+superglue'"
     ]
    }
   ],
   "source": [
    "cameras = read_write_model.read_cameras_binary(\"D:/gs-localization/output/7scenes_full_dslam/chess/sfm_superpoint+superglue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "40120097-1735-43f1-965a-fff60a48adbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import yaml\n",
    "from munch import munchify\n",
    "from math import atan\n",
    "from collections import OrderedDict\n",
    "\n",
    "sys.path.append(\"D:/gs-localization/gaussian_splatting\")\n",
    "sys.path.append(\"D:/gs-localization\")\n",
    "sys.path.append(\"D:/gs-localization/gs_localization/pipelines\")\n",
    "\n",
    "\n",
    "from tools.config_utils import load_config, update_recursive\n",
    "from tools import read_write_model\n",
    "from tools.gaussian_model import GaussianModel\n",
    "from tools import render\n",
    "from tools.camera_utils import Camera\n",
    "from tools.descent_utils import get_loss_tracking\n",
    "from tools.pose_utils import update_pose\n",
    "from tools.graphics_utils import getProjectionMatrix2\n",
    "\n",
    "\n",
    "def gradient_decent(viewpoint, config, initial_R, initial_T):\n",
    "\n",
    "    viewpoint.update_RT(initial_R, initial_T)\n",
    "    \n",
    "    opt_params = []\n",
    "    opt_params.append(\n",
    "        {\n",
    "            \"params\": [viewpoint.cam_rot_delta],\n",
    "            \"lr\": 0.001,\n",
    "            \"name\": \"rot_{}\".format(viewpoint.uid),\n",
    "        }\n",
    "    )\n",
    "    opt_params.append(\n",
    "        {\n",
    "            \"params\": [viewpoint.cam_trans_delta],\n",
    "            \"lr\": 0.001,\n",
    "            \"name\": \"trans_{}\".format(viewpoint.uid),\n",
    "        }\n",
    "    )\n",
    "    opt_params.append(\n",
    "        {\n",
    "            \"params\": [viewpoint.exposure_a],\n",
    "            \"lr\": 0.001,\n",
    "            \"name\": \"exposure_a_{}\".format(viewpoint.uid),\n",
    "        }\n",
    "    )\n",
    "    opt_params.append(\n",
    "        {\n",
    "            \"params\": [viewpoint.exposure_b],\n",
    "            \"lr\": 0.001,\n",
    "            \"name\": \"exposure_b_{}\".format(viewpoint.uid),\n",
    "        }\n",
    "    )\n",
    "    \n",
    "\n",
    "    pose_optimizer = torch.optim.Adam(opt_params)\n",
    "    \n",
    "    for tracking_itr in range(50):\n",
    "        \n",
    "        render_pkg = render(\n",
    "            viewpoint, Model, pipeline_params, background\n",
    "        )\n",
    "        \n",
    "        image, depth, opacity = (\n",
    "            render_pkg[\"render\"],\n",
    "            render_pkg[\"depth\"],\n",
    "            render_pkg[\"opacity\"],\n",
    "        )\n",
    "          \n",
    "        pose_optimizer.zero_grad()\n",
    "        \n",
    "        loss_tracking = get_loss_tracking(\n",
    "            config, image, depth, opacity, viewpoint\n",
    "        )\n",
    "        loss_tracking.backward()\n",
    "        \n",
    "    \n",
    "        with torch.no_grad():\n",
    "            pose_optimizer.step()\n",
    "            converged = update_pose(viewpoint, converged_threshold=1e-4)\n",
    "    \n",
    "        if converged:\n",
    "            break\n",
    "             \n",
    "    return viewpoint.R, viewpoint.T, render_pkg\n",
    "\n",
    "\n",
    "class Transformation:\n",
    "    def __init__(self, R=None, T=None):\n",
    "        self.R = R\n",
    "        self.T = T\n",
    "\n",
    "def quat_to_rotmat(qvec):\n",
    "    qvec = np.array(qvec, dtype=float)\n",
    "    w, x, y, z = qvec\n",
    "    R = np.array([\n",
    "        [1 - 2*y**2 - 2*z**2, 2*x*y - 2*z*w, 2*x*z + 2*y*w],\n",
    "        [2*x*y + 2*z*w, 1 - 2*x**2 - 2*z**2, 2*y*z - 2*x*w],\n",
    "        [2*x*z - 2*y*w, 2*y*z + 2*x*w, 1 - 2*x**2 - 2*y**2]\n",
    "    ])\n",
    "    return R\n",
    "\n",
    "\n",
    "def focal2fov(focal, pixels):\n",
    "    return 2 * atan(pixels / (2 * focal))\n",
    "\n",
    "def load_pose(pose_txt):\n",
    "    pose = []\n",
    "    with open(pose_txt, 'r') as f:\n",
    "        for line in f:\n",
    "            row = line.strip('\\n').split()\n",
    "            row = [float(c) for c in row]\n",
    "            pose.append(row)\n",
    "    pose = np.array(pose).astype(np.float32)\n",
    "    assert pose.shape == (4,4)\n",
    "    return pose\n",
    "\n",
    "def create_mask(mkpts_lst, width, height, k):\n",
    "    # Initial mask as all False\n",
    "    mask = np.zeros((height, width), dtype=bool)\n",
    "    \n",
    "    # Calculat k radius\n",
    "    half_k = k // 2\n",
    "    \n",
    "    # Iterate through all points\n",
    "    for pt in mkpts_lst:\n",
    "        x, y = int(pt[0]), int(pt[1])\n",
    "        \n",
    "        # Calculate k*k borders\n",
    "        x_min = max(0, x - half_k)\n",
    "        x_max = min(width, x + half_k + 1)\n",
    "        y_min = max(0, y - half_k)\n",
    "        y_max = min(height, y + half_k + 1)\n",
    "        \n",
    "        # Set mask k*k area as True\n",
    "        mask[y_min:y_max, x_min:x_max] = True\n",
    "    \n",
    "    # Shape: (1, height, width)\n",
    "    mask = mask[np.newaxis, :, :]\n",
    "    \n",
    "    return mask\n",
    "\n",
    "class BaseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, args, path, config):\n",
    "        self.args = args\n",
    "        self.path = path\n",
    "        self.config = config\n",
    "        self.device = \"cuda:0\"\n",
    "        self.dtype = torch.float32\n",
    "        self.num_imgs = 9999\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_imgs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pass\n",
    "\n",
    "class MonocularDataset(BaseDataset):\n",
    "    def __init__(self, args, path, config):\n",
    "        super().__init__(args, path, config)\n",
    "        calibration = config[\"Dataset\"][\"Calibration\"]\n",
    "        # Camera prameters\n",
    "        self.fx = calibration[\"fx\"]\n",
    "        self.fy = calibration[\"fy\"]\n",
    "        self.cx = calibration[\"cx\"]\n",
    "        self.cy = calibration[\"cy\"]\n",
    "        self.width = calibration[\"width\"]\n",
    "        self.height = calibration[\"height\"]\n",
    "        self.fovx = focal2fov(self.fx, self.width)\n",
    "        self.fovy = focal2fov(self.fy, self.height)\n",
    "        self.K = np.array(\n",
    "            [[self.fx, 0.0, self.cx], [0.0, self.fy, self.cy], [0.0, 0.0, 1.0]]\n",
    "        )\n",
    "        # distortion parameters\n",
    "        self.disorted = calibration[\"distorted\"]\n",
    "        self.dist_coeffs = np.array(\n",
    "            [\n",
    "                calibration[\"k1\"],\n",
    "                calibration[\"k2\"],\n",
    "                calibration[\"p1\"],\n",
    "                calibration[\"p2\"],\n",
    "                calibration[\"k3\"],\n",
    "            ]\n",
    "        )\n",
    "        self.map1x, self.map1y = cv2.initUndistortRectifyMap(\n",
    "            self.K,\n",
    "            self.dist_coeffs,\n",
    "            np.eye(3),\n",
    "            self.K,\n",
    "            (self.width, self.height),\n",
    "            cv2.CV_32FC1,\n",
    "        )\n",
    "        # depth parameters\n",
    "        self.has_depth = True if \"depth_scale\" in calibration.keys() else False\n",
    "        self.depth_scale = calibration[\"depth_scale\"] if self.has_depth else None\n",
    "\n",
    "        # Default scene scale\n",
    "        nerf_normalization_radius = 5\n",
    "        self.scene_info = {\n",
    "            \"nerf_normalization\": {\n",
    "                \"radius\": nerf_normalization_radius,\n",
    "                \"translation\": np.zeros(3),\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        color_path = self.color_paths[idx]\n",
    "        pose = self.poses[idx]\n",
    "\n",
    "        image = np.array(Image.open(color_path))\n",
    "        depth = None\n",
    "\n",
    "        if self.disorted:\n",
    "            image = cv2.remap(image, self.map1x, self.map1y, cv2.INTER_LINEAR)\n",
    "\n",
    "        if self.has_depth:\n",
    "            depth_path = self.depth_paths[idx]\n",
    "            depth = np.array(Image.open(depth_path)) / self.depth_scale\n",
    "\n",
    "        image = (\n",
    "            torch.from_numpy(image / 255.0)\n",
    "            .clamp(0.0, 1.0)\n",
    "            .permute(2, 0, 1)\n",
    "            .to(device=self.device, dtype=self.dtype)\n",
    "        )\n",
    "        pose = torch.from_numpy(pose).to(device=self.device)\n",
    "        return image, depth, pose\n",
    "\n",
    "\n",
    "class seven_scenes_Dataset(MonocularDataset):\n",
    "    def __init__(self, args, path, config, data_folder, scene):\n",
    "        super().__init__(args, path, config)\n",
    "        self.has_depth = True\n",
    "        self.seven_scenes_Parser(data_folder, scene) \n",
    "        \n",
    "    def seven_scenes_Parser(self, data_folder, scene):\n",
    "        self.color_paths, self.poses, self.depth_paths = [], [], []\n",
    "\n",
    "        gt_dirs = Path(data_folder) / scene / \"sparse/0\"\n",
    "        _, images, _ = read_write_model.read_model(gt_dirs, \".txt\")\n",
    "\n",
    "        # Read the filenames from test_fewshot.txt and store them in a set.\n",
    "        test_images_path = Path(data_folder) / scene / \"test_full.txt\"\n",
    "        \n",
    "        with open(test_images_path, 'r') as f:\n",
    "            test_images = set(line.strip() for line in f)\n",
    "            \n",
    "        for i, image in tqdm(images.items(),\"Load dataset\"):\n",
    "            # Execute the following operation only if image.name exists in test_images.\"\n",
    "            if image.name in test_images:\n",
    "                image_path = Path(data_folder) / scene / 'images_full' / image.name\n",
    "                depth_path = Path(data_folder) / scene / 'depths_full' / image.name.replace(\"color\",\"depth\")\n",
    "                self.color_paths.append(image_path)\n",
    "                self.depth_paths.append(depth_path)\n",
    "                R_gt, t_gt = image.qvec2rotmat(), image.tvec\n",
    "                pose = np.eye(4)            \n",
    "                pose[:3, :3] = R_gt         \n",
    "                pose[:3, 3] = t_gt \n",
    "                self.poses.append(pose)\n",
    "\n",
    "        # Sort self.color_paths, self.poses, and self.depth_paths based on normal file name order\n",
    "        sorted_data = sorted(zip(self.color_paths, self.depth_paths, self.poses), key=lambda x: x[0].name)\n",
    "        self.color_paths, self.depth_paths, self.poses = zip(*sorted_data)\n",
    "        del images\n",
    "\n",
    "with open(\"D:/gs-localization/gs_localization/pipelines/configs/mono/tum/fr3_office.yaml\", \"r\") as f:\n",
    "    cfg_special = yaml.full_load(f)\n",
    "\n",
    "inherit_from = \"D:/gs-localization/gs_localization/pipelines/configs/mono/tum/base_config.yaml\"\n",
    "\n",
    "if inherit_from is not None:\n",
    "    cfg = load_config(inherit_from)\n",
    "else:\n",
    "    cfg = dict()\n",
    "\n",
    "# merge per dataset cfg. and main cfg.\n",
    "config = update_recursive(cfg, cfg_special)\n",
    "config = cfg\n",
    "    \n",
    "data_folder = \"D:/gs-localization/datasets/7scenes\"\n",
    "config[\"Dataset\"][\"Calibration\"][\"fx\"] = 525\n",
    "config[\"Dataset\"][\"Calibration\"][\"fy\"] = 525\n",
    "config[\"Dataset\"][\"Calibration\"][\"cx\"] = 320\n",
    "config[\"Dataset\"][\"Calibration\"][\"cy\"] = 240\n",
    "config[\"Dataset\"][\"Calibration\"][\"width\"] = 640\n",
    "config[\"Dataset\"][\"Calibration\"][\"height\"] = 480   \n",
    "config[\"Dataset\"][\"Calibration\"]['depth_scale'] = 1000.0\n",
    "config[\"Training\"][\"monocular\"] = False\n",
    "config[\"Training\"][\"alpha\"] = 0.99\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2bb1a463-0549-4438-8432-b68f8ea2c731",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load dataset: 100%|█████████████████████████████████████████████████████████████| 6000/6000 [00:00<00:00, 99019.18it/s]\n"
     ]
    }
   ],
   "source": [
    "scene = \"chess\"\n",
    "Model = GaussianModel(3, config)\n",
    "Model.load_ply(f\"D:/gs-localization/output/7scenes_full/{scene}/gs_map/iteration_30000/point_cloud.ply\")\n",
    "\n",
    "model_params = munchify(config[\"model_params\"])\n",
    "pipeline_params = munchify(config[\"pipeline_params\"])\n",
    "data_folder = \"D:/gs-localization/datasets/7scenes\"\n",
    "dataset = seven_scenes_Dataset(model_params, model_params.source_path, config, data_folder, scene)\n",
    "bg_color = [0, 0, 0] \n",
    "background = torch.tensor(bg_color, dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "projection_matrix = getProjectionMatrix2(\n",
    "    znear=0.01,\n",
    "    zfar=100.0,\n",
    "    fx=dataset.fx,\n",
    "    fy=dataset.fy,\n",
    "    cx=dataset.cx,\n",
    "    cy=dataset.cy,\n",
    "    W=dataset.width,\n",
    "    H=dataset.height,\n",
    ").transpose(0, 1)\n",
    "projection_matrix = projection_matrix.to(device=\"cuda:0\")\n",
    "\n",
    "config[\"Training\"][\"opacity_threshold\"] = 0.99\n",
    "config[\"Training\"][\"edge_threshold\"] = 1.1\n",
    "\n",
    "# use OrderedDict to substitute defaultdict\n",
    "test_infos = OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c161f21d-d8e2-4eb6-9d69-71b1c9ba1727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chess\n",
      "53\n",
      "最接近中值错误的图片名字: seq-03-frame-000053-color.png\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(scene)\n",
    "# 加载 trans_errors.npy 文件\n",
    "trans_errors_path = f'D:/gs-localization/output/7scenes_full/{scene}/trans_sfm_errors.npy'\n",
    "trans_errors = np.load(trans_errors_path)\n",
    "\n",
    "# 计算中值\n",
    "median_value = np.median(trans_errors)\n",
    "\n",
    "# 找到最接近中值的索引\n",
    "median_index = np.argmin(np.abs(trans_errors - median_value))\n",
    "print(median_index)\n",
    "\n",
    "# 打开 results_sparse.txt 文件并提取对应行的名字\n",
    "results_path = f'D:/gs-localization/output/7scenes_full/{scene}/results_sparse.txt'\n",
    "with open(results_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# 提取每一行的第一个字段（假设是名字）\n",
    "names = [line.split()[0] for line in lines]\n",
    "\n",
    "# 根据找到的索引获取对应的名字\n",
    "median_name = names[median_index]\n",
    "\n",
    "# 输出结果\n",
    "print(f\"最接近中值错误的图片名字: {median_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ba3874-c1ce-4b18-9e9c-73624e8ce164",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe9e057-efa6-45ad-8f8a-845fe66528da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ef400d21-a592-4a3c-a023-f8760d0ee3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\27118\\AppData\\Local\\Temp\\ipykernel_47772\\3732968887.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  initial_R = torch.tensor(viewpoint.R_gt)\n",
      "C:\\Users\\27118\\AppData\\Local\\Temp\\ipykernel_47772\\3732968887.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  initial_T = torch.tensor(viewpoint.T_gt).squeeze()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "???\n",
      "???\n"
     ]
    }
   ],
   "source": [
    "# suppose file open and read\n",
    "with open(f\"D:/gs-localization/output/7scenes_full/{scene}/results_sparse.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        name = parts[0]\n",
    "        qvec = list(map(float, parts[1:5]))\n",
    "        tvec = list(map(float, parts[5:8]))\n",
    "\n",
    "        R = quat_to_rotmat(qvec)\n",
    "        T = np.array(tvec)\n",
    "\n",
    "        # insert directly in OrderedDict\n",
    "        test_infos[name] = Transformation(R=R, T=T)\n",
    "\n",
    "# sort OrderedDict according to name \n",
    "test_infos = OrderedDict(sorted(test_infos.items(), key=lambda item: item[0]))\n",
    "\n",
    "file = h5py.File(f'D:/gs-localization/output/7scenes_full/{scene}/feats-superpoint-n4096-r1024.h5', 'r')\n",
    "\n",
    "i = 53\n",
    "\n",
    "image = \"seq-03-frame-000053-color.png\"\n",
    "viewpoint = Camera.init_from_dataset(dataset, i, projection_matrix)\n",
    "viewpoint.compute_grad_mask(config)\n",
    "viewpoint.R = viewpoint.R_gt\n",
    "viewpoint.T = viewpoint.T_gt\n",
    "initial_R = torch.tensor(viewpoint.R_gt)\n",
    "initial_T = torch.tensor(viewpoint.T_gt).squeeze()\n",
    "print(\"???\")\n",
    "gradient_decent(viewpoint, config, initial_R, initial_T)\n",
    "print(\"???\")\n",
    "\n",
    "file.close()\n",
    "\n",
    "render_pkg = render(\n",
    "            viewpoint, Model, pipeline_params, background\n",
    "        )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6ab0b216-b905-4ceb-86a9-de8260c1d5bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "# 假设 viewpoint.original_image 和 render_pkg[\"render\"] 是 Tensor\n",
    "ground_truth_tensor = viewpoint.original_image\n",
    "localized_tensor = render_pkg[\"render\"]\n",
    "\n",
    "# 找到每个像素中 R、G、B 三个通道的最大值\n",
    "max_vals, _ = localized_tensor.max(dim=0)  # 得到每个像素的最大值 (H, W)\n",
    "\n",
    "# 找到哪些像素的最大值超过 1\n",
    "exceeds_one_mask = max_vals > 1  # 布尔掩码，标记哪些像素的最大值超过 1\n",
    "\n",
    "# 对超过 1 的地方，将 R、G、B 值同时按最大值进行归一化\n",
    "localized_tensor[:, exceeds_one_mask] = localized_tensor[:, exceeds_one_mask] / (max_vals[exceeds_one_mask] + 0.00001)\n",
    "\n",
    "# 将 Tensor 转换为 PIL 图像\n",
    "tensor_to_pil = transforms.ToPILImage()\n",
    "\n",
    "ground_truth_image = tensor_to_pil(ground_truth_tensor)\n",
    "localized_image = tensor_to_pil(localized_tensor)\n",
    "\n",
    "# 确保两张图片大小相同（可以选择调整大小）\n",
    "width, height = ground_truth_image.size\n",
    "localized_image = localized_image.resize((width, height))\n",
    "\n",
    "# 创建一个新的空白图像，用来合成 ground truth 和 localized image\n",
    "combined_image = Image.new('RGB', (width, height))\n",
    "\n",
    "# 将图像转换为 NumPy 数组，方便逐像素操作\n",
    "ground_truth_array = np.array(ground_truth_image)\n",
    "localized_image_array = np.array(localized_image)\n",
    "\n",
    "# 根据条件 x < ay 来合成图像\n",
    "for y in range(height):\n",
    "    for x in range(width):\n",
    "        if x < (y * (width / height)):  # 根据比例 x < ay 来判断\n",
    "            combined_image.putpixel((x, y), tuple(ground_truth_array[y, x]))  # 放置 ground truth\n",
    "        else:\n",
    "            combined_image.putpixel((x, y), tuple(localized_image_array[y, x]))  # 放置 localized image\n",
    "\n",
    "# 绘制白色虚线对角线\n",
    "draw = ImageDraw.Draw(combined_image)\n",
    "line_length = 20  # 每个虚线段的长度\n",
    "gap_length = 10   # 每段虚线之间的间隔\n",
    "\n",
    "# 计算对角线的总长度\n",
    "diagonal_length = int((width**2 + height**2)**0.5)\n",
    "\n",
    "# 循环绘制虚线\n",
    "for i in range(0, diagonal_length, line_length + gap_length):\n",
    "    start_x = int(i * (width / diagonal_length))  # 起点 x\n",
    "    start_y = int(i * (height / diagonal_length))  # 起点 y\n",
    "    end_x = int((i + line_length) * (width / diagonal_length))  # 终点 x\n",
    "    end_y = int((i + line_length) * (height / diagonal_length))  # 终点 y\n",
    "\n",
    "    # 绘制虚线的段\n",
    "    draw.line((start_x, start_y, end_x, end_y), fill=\"white\", width=3)\n",
    "\n",
    "# 画小框\n",
    "small_box_start = (200, 150)  # 小框左上角起始点 (x, y)\n",
    "small_box_width = 150         # 小框的宽度\n",
    "small_box_height = 100        # 小框的高度\n",
    "small_box_end = (small_box_start[0] + small_box_width, small_box_start[1] + small_box_height)\n",
    "\n",
    "# 绘制蓝色小框\n",
    "draw.rectangle([small_box_start, small_box_end], outline=\"green\", width=2)\n",
    "\n",
    "# 提取小框中的部分\n",
    "small_box_region = combined_image.crop((small_box_start[0], small_box_start[1], small_box_end[0], small_box_end[1]))\n",
    "\n",
    "# 放大小框中的部分\n",
    "scale_factor = 1.6  # 放大倍数\n",
    "large_box_region = small_box_region.resize((int(small_box_width * scale_factor), int(small_box_height * scale_factor)))\n",
    "\n",
    "# 将放大的大框放置在小框旁边，覆盖图片部分区域\n",
    "large_box_start_x = small_box_start[0]   # 小框右边再加10像素\n",
    "large_box_start_y = small_box_start[1] + small_box_width - 20\n",
    "\n",
    "# 确保大框不会超出图片边界\n",
    "if large_box_start_x + large_box_region.width > width:\n",
    "    large_box_start_x = width - large_box_region.width - 10\n",
    "if large_box_start_y + large_box_region.height > height:\n",
    "    large_box_start_y = height - large_box_region.height - 10\n",
    "\n",
    "# 将放大的区域粘贴回原图中\n",
    "combined_image.paste(large_box_region, (large_box_start_x, large_box_start_y))\n",
    "\n",
    "# 画大框\n",
    "large_box_end = (large_box_start_x + large_box_region.width, large_box_start_y + large_box_region.height)\n",
    "draw.rectangle([large_box_start_x, large_box_start_y, large_box_end[0], large_box_end[1]], outline=\"green\", width=3)\n",
    "\n",
    "# 显示结果\n",
    "combined_image.show()\n",
    "\n",
    "# 保存图片到指定路径\n",
    "combined_image.save(f\"C:/Users/27118/Desktop/{scene}_compare.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "620b93aa-f042-4ef0-9cf8-877c75930b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# 场景顺序\n",
    "scenes = [\"chess\", \"fire\", \"heads\", \"office\", \"pumpkin\", \"redkitchen\", \"stairs\"]\n",
    "\n",
    "# 图片路径模板\n",
    "image_paths = [f\"C:/Users/27118/Desktop/{scene}_compare.png\" for scene in scenes]\n",
    "\n",
    "# 打开所有图片，并获取宽高信息\n",
    "images = [Image.open(img_path) for img_path in image_paths]\n",
    "width, height = images[0].size  # 假设所有图片大小相同\n",
    "\n",
    "# 创建一个白色的图片，用作第 8 张图\n",
    "white_image = Image.new('RGB', (width, height), color='white')\n",
    "\n",
    "# 添加白色图片到 images 列表中，确保最后总共有 8 张图片\n",
    "images.append(white_image)\n",
    "\n",
    "# 创建一个新的空白图像，宽度为4张图片的宽度，高度为两行的图片高度\n",
    "new_image = Image.new('RGB', (width * 4, height * 2))\n",
    "\n",
    "# 逐一粘贴图片到新图像中\n",
    "for i, img in enumerate(images):\n",
    "    # 计算每张图片的位置\n",
    "    x_offset = (i % 4) * width  # 每行最多放置4张图片\n",
    "    y_offset = (i // 4) * height  # 放置到第几行\n",
    "    new_image.paste(img, (x_offset, y_offset))\n",
    "\n",
    "# 显示合成后的图像\n",
    "new_image.show()\n",
    "\n",
    "# 保存最终合成图像\n",
    "new_image.save(\"C:/Users/27118/Desktop/combined_image.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc0ab640-4424-4311-8a6e-6570e4ec1c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48435812485309393\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 指定.npy文件的路径\n",
    "f\"D:/gs-localization/output/7scenes/{scene}/results_sparse.txt\"\n",
    "file_path = 'D:/gs-localization/output/7scenes/{scene}/rot_errors.npy'\n",
    "\n",
    "# 加载.npy文件\n",
    "data = np.load(file_path)\n",
    "\n",
    "# 查看文件内容\n",
    "print(np.median(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19378a46-5737-4330-8661-eefee72db101",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
