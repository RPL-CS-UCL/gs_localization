{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c161f21d-d8e2-4eb6-9d69-71b1c9ba1727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39115ecb-6ee0-43cc-aa48-f1ed30f91759",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load dataset: 100%|████████████████████████████████████████████████████████████| 6000/6000 [00:00<00:00, 120157.10it/s]\n",
      "Localization: 100%|████████████████████████████████████████████████████████████████| 2000/2000 [21:17<00:00,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Median errors for chess: 0.005m, 0.157deg\n",
      "\n",
      "\t1cm, 1deg : 87.63%\n",
      "\n",
      "\t2cm, 2deg : 96.61%\n",
      "\n",
      "\t3cm, 3deg : 97.81%\n",
      "\n",
      "\t5cm, 5deg : 98.75%\n",
      "\n",
      "\t25cm, 2deg : 99.10%\n",
      "\n",
      "\t50cm, 5deg : 99.15%\n",
      "\n",
      "\t500cm, 10deg : 99.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import yaml\n",
    "from munch import munchify\n",
    "from math import atan\n",
    "from collections import OrderedDict\n",
    "\n",
    "sys.path.append(\"D:/gs-localization/gaussian_splatting\")\n",
    "sys.path.append(\"D:/gs-localization\")\n",
    "sys.path.append(\"D:/gs-localization/gs_localization/pipelines\")\n",
    "\n",
    "from tools.config_utils import load_config, update_recursive\n",
    "from tools import read_write_model\n",
    "from tools.gaussian_model import GaussianModel\n",
    "from tools import render\n",
    "from tools.camera_utils import Camera\n",
    "from tools.descent_utils import get_loss_tracking\n",
    "from tools.pose_utils import update_pose\n",
    "from tools.graphics_utils import getProjectionMatrix2\n",
    "\n",
    "\n",
    "def gradient_decent(viewpoint, config, initial_R, initial_T):\n",
    "\n",
    "    viewpoint.update_RT(initial_R, initial_T)\n",
    "    \n",
    "    opt_params = []\n",
    "    opt_params.append(\n",
    "        {\n",
    "            \"params\": [viewpoint.cam_rot_delta],\n",
    "            \"lr\": 0.001,\n",
    "            \"name\": \"rot_{}\".format(viewpoint.uid),\n",
    "        }\n",
    "    )\n",
    "    opt_params.append(\n",
    "        {\n",
    "            \"params\": [viewpoint.cam_trans_delta],\n",
    "            \"lr\": 0.001,\n",
    "            \"name\": \"trans_{}\".format(viewpoint.uid),\n",
    "        }\n",
    "    )\n",
    "    opt_params.append(\n",
    "        {\n",
    "            \"params\": [viewpoint.exposure_a],\n",
    "            \"lr\": 0.001,\n",
    "            \"name\": \"exposure_a_{}\".format(viewpoint.uid),\n",
    "        }\n",
    "    )\n",
    "    opt_params.append(\n",
    "        {\n",
    "            \"params\": [viewpoint.exposure_b],\n",
    "            \"lr\": 0.001,\n",
    "            \"name\": \"exposure_b_{}\".format(viewpoint.uid),\n",
    "        }\n",
    "    )\n",
    "    \n",
    "\n",
    "    pose_optimizer = torch.optim.Adam(opt_params)\n",
    "    \n",
    "    for tracking_itr in range(50):\n",
    "        \n",
    "        render_pkg = render(\n",
    "            viewpoint, Model, pipeline_params, background\n",
    "        )\n",
    "        \n",
    "        image, depth, opacity = (\n",
    "            render_pkg[\"render\"],\n",
    "            render_pkg[\"depth\"],\n",
    "            render_pkg[\"opacity\"],\n",
    "        )\n",
    "          \n",
    "        pose_optimizer.zero_grad()\n",
    "        \n",
    "        loss_tracking = get_loss_tracking(\n",
    "            config, image, depth, opacity, viewpoint\n",
    "        )\n",
    "        loss_tracking.backward()\n",
    "        \n",
    "    \n",
    "        with torch.no_grad():\n",
    "            pose_optimizer.step()\n",
    "            converged = update_pose(viewpoint, converged_threshold=1e-4)\n",
    "    \n",
    "        if converged:\n",
    "            break\n",
    "             \n",
    "    return viewpoint.R, viewpoint.T, render_pkg\n",
    "\n",
    "\n",
    "class Transformation:\n",
    "    def __init__(self, R=None, T=None):\n",
    "        self.R = R\n",
    "        self.T = T\n",
    "\n",
    "def quat_to_rotmat(qvec):\n",
    "    qvec = np.array(qvec, dtype=float)\n",
    "    w, x, y, z = qvec\n",
    "    R = np.array([\n",
    "        [1 - 2*y**2 - 2*z**2, 2*x*y - 2*z*w, 2*x*z + 2*y*w],\n",
    "        [2*x*y + 2*z*w, 1 - 2*x**2 - 2*z**2, 2*y*z - 2*x*w],\n",
    "        [2*x*z - 2*y*w, 2*y*z + 2*x*w, 1 - 2*x**2 - 2*y**2]\n",
    "    ])\n",
    "    return R\n",
    "\n",
    "\n",
    "def focal2fov(focal, pixels):\n",
    "    return 2 * atan(pixels / (2 * focal))\n",
    "\n",
    "def load_pose(pose_txt):\n",
    "    pose = []\n",
    "    with open(pose_txt, 'r') as f:\n",
    "        for line in f:\n",
    "            row = line.strip('\\n').split()\n",
    "            row = [float(c) for c in row]\n",
    "            pose.append(row)\n",
    "    pose = np.array(pose).astype(np.float32)\n",
    "    assert pose.shape == (4,4)\n",
    "    return pose\n",
    "\n",
    "def create_mask(mkpts_lst, width, height, k):\n",
    "    # Initial mask as all False\n",
    "    mask = np.zeros((height, width), dtype=bool)\n",
    "    \n",
    "    # Calculat k radius\n",
    "    half_k = k // 2\n",
    "    \n",
    "    # Iterate through all points\n",
    "    for pt in mkpts_lst:\n",
    "        x, y = int(pt[0]), int(pt[1])\n",
    "        \n",
    "        # Calculate k*k borders\n",
    "        x_min = max(0, x - half_k)\n",
    "        x_max = min(width, x + half_k + 1)\n",
    "        y_min = max(0, y - half_k)\n",
    "        y_max = min(height, y + half_k + 1)\n",
    "        \n",
    "        # Set mask k*k area as True\n",
    "        mask[y_min:y_max, x_min:x_max] = True\n",
    "    \n",
    "    # Shape: (1, height, width)\n",
    "    mask = mask[np.newaxis, :, :]\n",
    "    \n",
    "    return mask\n",
    "\n",
    "class BaseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, args, path, config):\n",
    "        self.args = args\n",
    "        self.path = path\n",
    "        self.config = config\n",
    "        self.device = \"cuda:0\"\n",
    "        self.dtype = torch.float32\n",
    "        self.num_imgs = 9999\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_imgs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pass\n",
    "\n",
    "class MonocularDataset(BaseDataset):\n",
    "    def __init__(self, args, path, config):\n",
    "        super().__init__(args, path, config)\n",
    "        calibration = config[\"Dataset\"][\"Calibration\"]\n",
    "        # Camera prameters\n",
    "        self.fx = calibration[\"fx\"]\n",
    "        self.fy = calibration[\"fy\"]\n",
    "        self.cx = calibration[\"cx\"]\n",
    "        self.cy = calibration[\"cy\"]\n",
    "        self.width = calibration[\"width\"]\n",
    "        self.height = calibration[\"height\"]\n",
    "        self.fovx = focal2fov(self.fx, self.width)\n",
    "        self.fovy = focal2fov(self.fy, self.height)\n",
    "        self.K = np.array(\n",
    "            [[self.fx, 0.0, self.cx], [0.0, self.fy, self.cy], [0.0, 0.0, 1.0]]\n",
    "        )\n",
    "        # distortion parameters\n",
    "        self.disorted = calibration[\"distorted\"]\n",
    "        self.dist_coeffs = np.array(\n",
    "            [\n",
    "                calibration[\"k1\"],\n",
    "                calibration[\"k2\"],\n",
    "                calibration[\"p1\"],\n",
    "                calibration[\"p2\"],\n",
    "                calibration[\"k3\"],\n",
    "            ]\n",
    "        )\n",
    "        self.map1x, self.map1y = cv2.initUndistortRectifyMap(\n",
    "            self.K,\n",
    "            self.dist_coeffs,\n",
    "            np.eye(3),\n",
    "            self.K,\n",
    "            (self.width, self.height),\n",
    "            cv2.CV_32FC1,\n",
    "        )\n",
    "        # depth parameters\n",
    "        self.has_depth = True if \"depth_scale\" in calibration.keys() else False\n",
    "        self.depth_scale = calibration[\"depth_scale\"] if self.has_depth else None\n",
    "\n",
    "        # Default scene scale\n",
    "        nerf_normalization_radius = 5\n",
    "        self.scene_info = {\n",
    "            \"nerf_normalization\": {\n",
    "                \"radius\": nerf_normalization_radius,\n",
    "                \"translation\": np.zeros(3),\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        color_path = self.color_paths[idx]\n",
    "        pose = self.poses[idx]\n",
    "\n",
    "        image = np.array(Image.open(color_path))\n",
    "        depth = None\n",
    "\n",
    "        if self.disorted:\n",
    "            image = cv2.remap(image, self.map1x, self.map1y, cv2.INTER_LINEAR)\n",
    "\n",
    "        if self.has_depth:\n",
    "            depth_path = self.depth_paths[idx]\n",
    "            depth = np.array(Image.open(depth_path)) / self.depth_scale\n",
    "\n",
    "        image = (\n",
    "            torch.from_numpy(image / 255.0)\n",
    "            .clamp(0.0, 1.0)\n",
    "            .permute(2, 0, 1)\n",
    "            .to(device=self.device, dtype=self.dtype)\n",
    "        )\n",
    "        pose = torch.from_numpy(pose).to(device=self.device)\n",
    "        return image, depth, pose\n",
    "\n",
    "\n",
    "class seven_scenes_Dataset(MonocularDataset):\n",
    "    def __init__(self, args, path, config, data_folder, scene):\n",
    "        super().__init__(args, path, config)\n",
    "        self.has_depth = True\n",
    "        self.seven_scenes_Parser(data_folder, scene) \n",
    "        \n",
    "    def seven_scenes_Parser(self, data_folder, scene):\n",
    "        self.color_paths, self.poses, self.depth_paths = [], [], []\n",
    "\n",
    "        gt_dirs = Path(data_folder) / scene / \"sparse/0\"\n",
    "        _, images, _ = read_write_model.read_model(gt_dirs, \".txt\")\n",
    "\n",
    "        # Read the filenames from test_fewshot.txt and store them in a set.\n",
    "        test_images_path = Path(data_folder) / scene / \"test_fewshot.txt\"\n",
    "        \n",
    "        with open(test_images_path, 'r') as f:\n",
    "            test_images = set(line.strip() for line in f)\n",
    "            \n",
    "        for i, image in tqdm(images.items(),\"Load dataset\"):\n",
    "            # Execute the following operation only if image.name exists in test_images.\"\n",
    "            if image.name in test_images:\n",
    "                image_path = Path(data_folder) / scene / 'images' / image.name\n",
    "                depth_path = Path(data_folder) / scene / 'depths' / image.name.replace(\"color\",\"depth\")\n",
    "                self.color_paths.append(image_path)\n",
    "                self.depth_paths.append(depth_path)\n",
    "                R_gt, t_gt = image.qvec2rotmat(), image.tvec\n",
    "                pose = np.eye(4)            \n",
    "                pose[:3, :3] = R_gt         \n",
    "                pose[:3, 3] = t_gt \n",
    "                self.poses.append(pose)\n",
    "\n",
    "        # Sort self.color_paths, self.poses, and self.depth_paths based on normal file name order\n",
    "        sorted_data = sorted(zip(self.color_paths, self.depth_paths, self.poses), key=lambda x: x[0].name)\n",
    "        self.color_paths, self.depth_paths, self.poses = zip(*sorted_data)\n",
    "        del images\n",
    "\n",
    "with open(\"D:/gs-localization/gs_localization/pipelines/configs/mono/tum/fr3_office.yaml\", \"r\") as f:\n",
    "    cfg_special = yaml.full_load(f)\n",
    "\n",
    "inherit_from = \"D:/gs-localization/gs_localization/pipelines/configs/mono/tum/base_config.yaml\"\n",
    "\n",
    "if inherit_from is not None:\n",
    "    cfg = load_config(inherit_from)\n",
    "else:\n",
    "    cfg = dict()\n",
    "\n",
    "# merge per dataset cfg. and main cfg.\n",
    "config = update_recursive(cfg, cfg_special)\n",
    "config = cfg\n",
    "    \n",
    "data_folder = \"D:/gs-localization/datasets/7scenes\"\n",
    "config[\"Dataset\"][\"Calibration\"][\"fx\"] = 525\n",
    "config[\"Dataset\"][\"Calibration\"][\"fy\"] = 525\n",
    "config[\"Dataset\"][\"Calibration\"][\"cx\"] = 320\n",
    "config[\"Dataset\"][\"Calibration\"][\"cy\"] = 240\n",
    "config[\"Dataset\"][\"Calibration\"][\"width\"] = 640\n",
    "config[\"Dataset\"][\"Calibration\"][\"height\"] = 480   \n",
    "config[\"Dataset\"][\"Calibration\"]['depth_scale'] = 1000.0\n",
    "config[\"Training\"][\"monocular\"] = False\n",
    "config[\"Training\"][\"alpha\"] = 0.99\n",
    "\n",
    "#for scene in [\"chess\", \"fire\", \"heads\", \"office\", \"pumpkin\", \"redkitchen\", \"stairs\"]:\n",
    "scene = \"chess\"\n",
    "    Model = GaussianModel(3, config)\n",
    "    Model.load_ply(f\"D:/gs-localization/output/7scenes/{scene}/gs_map/iteration_30000/point_cloud.ply\")\n",
    "    \n",
    "    model_params = munchify(config[\"model_params\"])\n",
    "    pipeline_params = munchify(config[\"pipeline_params\"])\n",
    "    data_folder = \"D:/gs-localization/datasets/7scenes\"\n",
    "    dataset = seven_scenes_Dataset(model_params, model_params.source_path, config, data_folder, scene)\n",
    "    bg_color = [0, 0, 0] \n",
    "    background = torch.tensor(bg_color, dtype=torch.float32, device=\"cuda\")\n",
    "    \n",
    "    projection_matrix = getProjectionMatrix2(\n",
    "        znear=0.01,\n",
    "        zfar=100.0,\n",
    "        fx=dataset.fx,\n",
    "        fy=dataset.fy,\n",
    "        cx=dataset.cx,\n",
    "        cy=dataset.cy,\n",
    "        W=dataset.width,\n",
    "        H=dataset.height,\n",
    "    ).transpose(0, 1)\n",
    "    projection_matrix = projection_matrix.to(device=\"cuda:0\")\n",
    "    \n",
    "    config[\"Training\"][\"opacity_threshold\"] = 0.99\n",
    "    config[\"Training\"][\"edge_threshold\"] = 1.1\n",
    "    \n",
    "    # use OrderedDict to substitute defaultdict\n",
    "    test_infos = OrderedDict()\n",
    "    \n",
    "    # suppose file open and read\n",
    "    with open(f\"D:/gs-localization/output/7scenes/{scene}/results_sparse.txt\", \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            name = parts[0]\n",
    "            qvec = list(map(float, parts[1:5]))\n",
    "            tvec = list(map(float, parts[5:8]))\n",
    "\n",
    "            R = quat_to_rotmat(qvec)\n",
    "            T = np.array(tvec)\n",
    "    \n",
    "            # insert directly in OrderedDict\n",
    "            test_infos[name] = Transformation(R=R, T=T)\n",
    "    \n",
    "    # sort OrderedDict according to name \n",
    "    test_infos = OrderedDict(sorted(test_infos.items(), key=lambda item: item[0]))\n",
    "    \n",
    "    rot_errors = []\n",
    "    trans_errors = []\n",
    "    \n",
    "    file = h5py.File(f'D:/gs-localization/output/7scenes/{scene}/feats-superpoint-n4096-r1024.h5', 'r')\n",
    "    \n",
    "    \n",
    "    for i, image in enumerate(tqdm(test_infos, desc=\"Localization\")):\n",
    "        viewpoint = Camera.init_from_dataset(dataset, i, projection_matrix)\n",
    "    \n",
    "        viewpoint.compute_grad_mask(config)\n",
    "        \n",
    "        group = file[image] \n",
    "        keypoints = group['keypoints'][group['scores'][:]>0.2]  \n",
    "        mask = create_mask(mkpts_lst=keypoints, width=dataset.width, height=dataset.height, k=10)\n",
    "        viewpoint.grad_mask = viewpoint.grad_mask | torch.tensor(mask).to(\"cuda:0\")\n",
    "    \n",
    "        initial_R = torch.tensor(test_infos[image].R)\n",
    "        initial_T = torch.tensor(test_infos[image].T).squeeze()\n",
    "    \n",
    "        rotation_matrix, translation_vector, render_pkg = gradient_decent(viewpoint, config, initial_R, initial_T)\n",
    "        #rotation_matrix, translation_vector = initial_R, initial_T\n",
    "    \n",
    "        R_gt = viewpoint.R_gt.cpu().numpy()\n",
    "        t_gt = viewpoint.T_gt.reshape(3,1).cpu().numpy()\n",
    "        R = rotation_matrix.cpu().numpy()\n",
    "        t = translation_vector.reshape(3,1).cpu().numpy()\n",
    "        trans_error = np.linalg.norm(-R_gt.T @ t_gt + R.T @ t, axis=0)\n",
    "        cos = np.clip((np.trace(np.dot(R_gt.T, R)) - 1) / 2, -1.0, 1.0)\n",
    "        rot_error = np.rad2deg(np.abs(np.arccos(cos)))\n",
    "        #print(image, rot_error, trans_error)\n",
    "        rot_errors.append(rot_error)\n",
    "        trans_errors.append(trans_error)\n",
    "    \n",
    "    np.save(f\"D:/gs-localization/output/7scenes/{scene}/rot_errors.npy\", rot_errors)\n",
    "    np.save(f\"D:/gs-localization/output/7scenes/{scene}/trans_errors.npy\", trans_errors)\n",
    "    med_t = np.median(trans_errors)\n",
    "    med_R = np.median(rot_errors)\n",
    "    print( f\"\\nMedian errors for {scene}: {med_t:.3f}m, {med_R:.3f}deg\")\n",
    "    \n",
    "    threshs_t = [0.01, 0.02, 0.03, 0.05, 0.25, 0.5, 5.0]\n",
    "    threshs_R = [1.0, 2.0, 3.0, 5.0, 2.0, 5.0, 10.0]\n",
    "    for th_t, th_R in zip(threshs_t, threshs_R):\n",
    "        ratio = np.mean((np.array(trans_errors) < th_t) & (np.array(rot_errors) < th_R))\n",
    "        print(f\"\\n\\t{th_t*100:.0f}cm, {th_R:.0f}deg : {ratio*100:.2f}%\")\n",
    "        \n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc0ab640-4424-4311-8a6e-6570e4ec1c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48435812485309393\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 指定.npy文件的路径\n",
    "f\"D:/gs-localization/output/7scenes/{scene}/results_sparse.txt\"\n",
    "file_path = 'D:/gs-localization/output/7scenes/{scene}/rot_errors.npy'\n",
    "\n",
    "# 加载.npy文件\n",
    "data = np.load(file_path)\n",
    "\n",
    "# 查看文件内容\n",
    "print(np.median(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19378a46-5737-4330-8661-eefee72db101",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
