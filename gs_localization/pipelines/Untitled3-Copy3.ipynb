{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a07f2fcb-69e5-49af-86e2-bab558af02ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def mse(img1, img2):\n",
    "    return (((img1 - img2)) ** 2).view(img1.shape[0], -1).mean(1, keepdim=True)\n",
    "\n",
    "def psnr(img1, img2):\n",
    "    mse = (((img1 - img2)) ** 2).view(img1.shape[0], -1).mean(1, keepdim=True)\n",
    "    return 20 * torch.log10(1.0 / torch.sqrt(mse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "40120097-1735-43f1-965a-fff60a48adbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load dataset: 100%|█████████████████████████████████████████████████████████████| 2017/2017 [00:00<00:00, 58310.03it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import yaml\n",
    "from munch import munchify\n",
    "from math import atan\n",
    "from collections import OrderedDict\n",
    "\n",
    "sys.path.append(\"D:/gs-localization/gaussian_splatting\")\n",
    "sys.path.append(\"D:/gs-localization\")\n",
    "sys.path.append(\"D:/gs-localization/gs_localization/pipelines\")\n",
    "\n",
    "\n",
    "from tools.config_utils import load_config, update_recursive\n",
    "from tools import read_write_model\n",
    "from tools.gaussian_model import GaussianModel\n",
    "from tools import render\n",
    "from tools.camera_utils import Camera\n",
    "from tools.descent_utils import get_loss_tracking\n",
    "from tools.pose_utils import update_pose\n",
    "from tools.graphics_utils import getProjectionMatrix2\n",
    "\n",
    "\n",
    "def gradient_decent(viewpoint, config, initial_R, initial_T):\n",
    "\n",
    "    viewpoint.update_RT(initial_R, initial_T)\n",
    "    \n",
    "    opt_params = []\n",
    "    opt_params.append(\n",
    "        {\n",
    "            \"params\": [viewpoint.cam_rot_delta],\n",
    "            \"lr\": 0.000001,\n",
    "            \"name\": \"rot_{}\".format(viewpoint.uid),\n",
    "        }\n",
    "    )\n",
    "    opt_params.append(\n",
    "        {\n",
    "            \"params\": [viewpoint.cam_trans_delta],\n",
    "            \"lr\": 0.001,\n",
    "            \"name\": \"trans_{}\".format(viewpoint.uid),\n",
    "        }\n",
    "    )\n",
    "    opt_params.append(\n",
    "        {\n",
    "            \"params\": [viewpoint.exposure_a],\n",
    "            \"lr\": 0.001,\n",
    "            \"name\": \"exposure_a_{}\".format(viewpoint.uid),\n",
    "        }\n",
    "    )\n",
    "    opt_params.append(\n",
    "        {\n",
    "            \"params\": [viewpoint.exposure_b],\n",
    "            \"lr\": 0.001,\n",
    "            \"name\": \"exposure_b_{}\".format(viewpoint.uid),\n",
    "        }\n",
    "    )\n",
    "    \n",
    "\n",
    "    pose_optimizer = torch.optim.Adam(opt_params)\n",
    "    \n",
    "    for tracking_itr in range(50):\n",
    "        \n",
    "        render_pkg = render(\n",
    "            viewpoint, Model, pipeline_params, background\n",
    "        )\n",
    "        \n",
    "        image, depth, opacity = (\n",
    "            render_pkg[\"render\"],\n",
    "            render_pkg[\"depth\"],\n",
    "            render_pkg[\"opacity\"],\n",
    "        )\n",
    "          \n",
    "        pose_optimizer.zero_grad()\n",
    "        \n",
    "        loss_tracking = get_loss_tracking(\n",
    "            config, image, depth, opacity, viewpoint\n",
    "        )\n",
    "        loss_tracking.backward()\n",
    "        \n",
    "    \n",
    "        with torch.no_grad():\n",
    "            pose_optimizer.step()\n",
    "            converged = update_pose(viewpoint, converged_threshold=1e-4)\n",
    "    \n",
    "        if converged:\n",
    "            break\n",
    "             \n",
    "    return viewpoint.R, viewpoint.T, render_pkg\n",
    "\n",
    "\n",
    "class Transformation:\n",
    "    def __init__(self, R=None, T=None):\n",
    "        self.R = R\n",
    "        self.T = T\n",
    "\n",
    "def quat_to_rotmat(qvec):\n",
    "    qvec = np.array(qvec, dtype=float)\n",
    "    w, x, y, z = qvec\n",
    "    R = np.array([\n",
    "        [1 - 2*y**2 - 2*z**2, 2*x*y - 2*z*w, 2*x*z + 2*y*w],\n",
    "        [2*x*y + 2*z*w, 1 - 2*x**2 - 2*z**2, 2*y*z - 2*x*w],\n",
    "        [2*x*z - 2*y*w, 2*y*z + 2*x*w, 1 - 2*x**2 - 2*y**2]\n",
    "    ])\n",
    "    return R\n",
    "\n",
    "\n",
    "def focal2fov(focal, pixels):\n",
    "    return 2 * atan(pixels / (2 * focal))\n",
    "\n",
    "def load_pose(pose_txt):\n",
    "    pose = []\n",
    "    with open(pose_txt, 'r') as f:\n",
    "        for line in f:\n",
    "            row = line.strip('\\n').split()\n",
    "            row = [float(c) for c in row]\n",
    "            pose.append(row)\n",
    "    pose = np.array(pose).astype(np.float32)\n",
    "    assert pose.shape == (4,4)\n",
    "    return pose\n",
    "\n",
    "def create_mask(mkpts_lst, width, height, k):\n",
    "    # Initial mask as all False\n",
    "    mask = np.zeros((height, width), dtype=bool)\n",
    "    \n",
    "    # Calculat k radius\n",
    "    half_k = k // 2\n",
    "    \n",
    "    # Iterate through all points\n",
    "    for pt in mkpts_lst:\n",
    "        x, y = int(pt[0]), int(pt[1])\n",
    "        \n",
    "        # Calculate k*k borders\n",
    "        x_min = max(0, x - half_k)\n",
    "        x_max = min(width, x + half_k + 1)\n",
    "        y_min = max(0, y - half_k)\n",
    "        y_max = min(height, y + half_k + 1)\n",
    "        \n",
    "        # Set mask k*k area as True\n",
    "        mask[y_min:y_max, x_min:x_max] = True\n",
    "    \n",
    "    # Shape: (1, height, width)\n",
    "    mask = mask[np.newaxis, :, :]\n",
    "    \n",
    "    return mask\n",
    "\n",
    "class BaseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, args, path, config):\n",
    "        self.args = args\n",
    "        self.path = path\n",
    "        self.config = config\n",
    "        self.device = \"cuda:0\"\n",
    "        self.dtype = torch.float32\n",
    "        self.num_imgs = 999\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_imgs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pass\n",
    "\n",
    "class MonocularDataset(BaseDataset):\n",
    "    def __init__(self, args, path, config):\n",
    "        super().__init__(args, path, config)\n",
    "        calibration = config[\"Dataset\"][\"Calibration\"]\n",
    "\n",
    "        # depth parameters\n",
    "        self.has_depth = True if \"depth_scale\" in calibration.keys() else False\n",
    "        self.depth_scale = calibration[\"depth_scale\"] if self.has_depth else None\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        color_path = self.color_paths[idx]\n",
    "        pose = self.poses[idx]\n",
    "        intrinsic = self.intrinsics[idx]\n",
    "\n",
    "        image = np.array(Image.open(color_path))\n",
    "        depth = None\n",
    "\n",
    "        if self.has_depth:\n",
    "            depth_path = self.depth_paths[idx]\n",
    "            depth = np.array(Image.open(depth_path)) / self.depth_scale\n",
    "\n",
    "        image = (\n",
    "            torch.from_numpy(image / 255.0)\n",
    "            .clamp(0.0, 1.0)\n",
    "            .permute(2, 0, 1)\n",
    "            .to(device=self.device, dtype=self.dtype)\n",
    "        )\n",
    "        pose = torch.from_numpy(pose).to(device=self.device)\n",
    "        intrinsic = torch.from_numpy(intrinsic).to(device=self.device)\n",
    "        projection_matrix = self.projection_matrices[idx].to(device=\"cuda:0\")\n",
    "        \n",
    "        return image, depth, pose, intrinsic, projection_matrix \n",
    "\n",
    "def parse_camera_params(file_path):\n",
    "    camera_params = {}\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            \n",
    "            # Ensure there are at least 8 elements, and the second element is 'PINHOLE'\n",
    "            if len(parts) == 8 and parts[1] == 'PINHOLE':\n",
    "                try:\n",
    "                    # Extract parameters, skipping the second item 'PINHOLE'\n",
    "                    img_name = parts[0]\n",
    "                    w = int(parts[2])\n",
    "                    h = int(parts[3])\n",
    "                    fx = float(parts[4])\n",
    "                    fy = float(parts[5])\n",
    "                    cx = float(parts[6])\n",
    "                    cy = float(parts[7])\n",
    "                    \n",
    "                    # Store in the dictionary\n",
    "                    camera_params[img_name] = {\n",
    "                        'fx': fx,\n",
    "                        'fy': fy,\n",
    "                        'w': w,\n",
    "                        'h': h,\n",
    "                        'cx': cx,\n",
    "                        'cy': cy\n",
    "                    }\n",
    "                except ValueError as e:\n",
    "                    print(f\"Error parsing line: {line}. Error: {e}\")\n",
    "            else:\n",
    "                print(f\"Line skipped due to incorrect format: {line}\")\n",
    "    \n",
    "    return camera_params\n",
    "    \n",
    "class cambridge_Dataset(MonocularDataset):\n",
    "    def __init__(self, args, path, config, data_folder, scene):\n",
    "        super().__init__(args, path, config)\n",
    "        self.has_depth = False\n",
    "        self.cambridge_Parser(data_folder, scene) \n",
    "        \n",
    "    def cambridge_Parser(self, data_folder, scene):\n",
    "        self.color_paths, self.poses, self.depth_paths, self.intrinsics, self.projection_matrices = [], [], [], [], []\n",
    "\n",
    "        gt_dirs = Path(data_folder) / f\"CambridgeLandmarks_Colmap_Retriangulated_1024px/{scene}/empty_all\"\n",
    "        _, images, _ = read_write_model.read_model(gt_dirs, \".txt\")\n",
    "        output_folder = data_folder.replace(\"datasets\", \"output\")\n",
    "        \n",
    "        # Read the filenames from test_fewshot.txt and store them in a set.\n",
    "        test_images_path = Path(data_folder) / scene / \"test_full.txt\"\n",
    "        \n",
    "        with open(test_images_path, 'r') as f:\n",
    "            test_images = set(line.strip() for line in f)\n",
    "\n",
    "        intrinsics = parse_camera_params(f\"D:/gs-localization/output/cambridge_full/{scene}/query_list_with_intrinsics.txt\")\n",
    "\n",
    "        for i, image in tqdm(images.items(),\"Load dataset\"):\n",
    "            image_name = image.name.replace(\"/\",\"_\")\n",
    "            # Execute the following operation only if image.name exists in test_images.\"\n",
    "            if image_name in test_images:\n",
    "                image_path = Path(data_folder) / scene / 'images_full' / image_name\n",
    "                self.color_paths.append(image_path)\n",
    "                R_gt, t_gt = image.qvec2rotmat(), image.tvec\n",
    "                pose = np.eye(4)            \n",
    "                pose[:3, :3] = R_gt         \n",
    "                pose[:3, 3] = t_gt \n",
    "                self.poses.append(pose)\n",
    "                self.depth_paths.append(None)\n",
    "                \n",
    "                intrinsic = intrinsics[image_name]     \n",
    "                projection_matrix = getProjectionMatrix2(\n",
    "                    znear=0.01,\n",
    "                    zfar=100.0,\n",
    "                    fx=intrinsic[\"fx\"],\n",
    "                    fy=intrinsic[\"fy\"],\n",
    "                    cx=intrinsic[\"cx\"],\n",
    "                    cy=intrinsic[\"cy\"],\n",
    "                    W=intrinsic[\"w\"],\n",
    "                    H=intrinsic[\"h\"],\n",
    "                ).transpose(0, 1)\n",
    "    \n",
    "                self.intrinsics.append(np.array([\n",
    "                                       intrinsic[\"fx\"],\n",
    "                                       intrinsic[\"fy\"],\n",
    "                                       intrinsic[\"cx\"],\n",
    "                                       intrinsic[\"cy\"],\n",
    "                                       focal2fov(intrinsic[\"fx\"], intrinsic[\"w\"]),\n",
    "                                       focal2fov(intrinsic[\"fy\"], intrinsic[\"h\"]),\n",
    "                                       intrinsic[\"h\"], \n",
    "                                       intrinsic[\"w\"]\n",
    "                                        ]))\n",
    "                \n",
    "                self.projection_matrices.append(projection_matrix)\n",
    "\n",
    "\n",
    "        # Sort self.color_paths, self.poses, and self.depth_paths based on normal file name order\n",
    "        sorted_data = sorted(zip(self.color_paths, self.depth_paths, self.poses, \n",
    "                                 self.intrinsics, self.projection_matrices), key=lambda x: x[0].name)\n",
    "        self.color_paths, self.depth_paths, self.poses, self.intrinsics, self.projection_matrices = zip(*sorted_data)\n",
    "        del images\n",
    "\n",
    "with open(\"D:/gs-localization/gs_localization/pipelines/configs/mono/tum/fr3_office.yaml\", \"r\") as f:\n",
    "    cfg_special = yaml.full_load(f)\n",
    "\n",
    "inherit_from = \"D:/gs-localization/gs_localization/pipelines/configs/mono/tum/base_config.yaml\"\n",
    "\n",
    "if inherit_from is not None:\n",
    "    cfg = load_config(inherit_from)\n",
    "else:\n",
    "    cfg = dict()\n",
    "\n",
    "data_folder = \"D:/gs-localization/datasets/cambridge\"\n",
    "config = update_recursive(cfg, cfg_special)\n",
    "config = cfg\n",
    "config[\"Training\"][\"monocular\"] = True\n",
    "config[\"Training\"][\"opacity_threshold\"] = 0.99\n",
    "config[\"Training\"][\"edge_threshold\"] = 1.1\n",
    "for scene in ['StMarysChurch']:\n",
    "    try:\n",
    "        Model = GaussianModel(3, config)\n",
    "        Model.load_ply(f\"D:/gs-localization/output/cambridge_full/{scene}/gs_map/iteration_30000/point_cloud.ply\")\n",
    "    except:\n",
    "        Model = GaussianModel(1, config)\n",
    "        Model.load_ply(f\"D:/gs-localization/output/cambridge_full/{scene}/gs_map/iteration_30000/point_cloud.ply\")\n",
    "    \n",
    "    model_params = munchify(config[\"model_params\"])\n",
    "    pipeline_params = munchify(config[\"pipeline_params\"])\n",
    "    data_folder = \"D:/gs-localization/datasets/cambridge\"\n",
    "    dataset = cambridge_Dataset(model_params, model_params.source_path, config, data_folder, scene)\n",
    "    bg_color = [0, 0, 0] \n",
    "    background = torch.tensor(bg_color, dtype=torch.float32, device=\"cuda\")\n",
    "    \n",
    "    # use OrderedDict to substitute defaultdict\n",
    "    test_infos = OrderedDict()\n",
    "    \n",
    "    # suppose file open and read\n",
    "    with open(f\"D:/gs-localization/output/cambridge_full/{scene}/results_sparse.txt\", \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            name = parts[0]\n",
    "            qvec = list(map(float, parts[1:5]))\n",
    "            tvec = list(map(float, parts[5:8]))\n",
    "\n",
    "            R = quat_to_rotmat(qvec)\n",
    "            T = np.array(tvec)\n",
    "    \n",
    "            # insert directly in OrderedDict\n",
    "            test_infos[name] = Transformation(R=R, T=T)\n",
    "    \n",
    "    # sort OrderedDict according to name \n",
    "    test_infos = OrderedDict(sorted(test_infos.items(), key=lambda item: item[0]))\n",
    "    \n",
    "    rot_errors = []\n",
    "    trans_errors = []\n",
    "    \n",
    "    file = h5py.File(f'D:/gs-localization/output/cambridge_full/{scene}/feats-superpoint-n4096-r1024.h5', 'r')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8a839608-1452-4e51-9813-0466455186bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StMarysChurch\n",
      "21\n",
      "最接近中值错误的图片名字: seq13_frame00176.png\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(scene)\n",
    "# 加载 trans_errors.npy 文件\n",
    "trans_errors_path = f'D:/gs-localization/output/cambridge_full/{scene}/trans_errors.npy'\n",
    "trans_errors = np.load(trans_errors_path)\n",
    "\n",
    "# 计算中值\n",
    "median_value = np.median(trans_errors)\n",
    "\n",
    "# 找到最接近中值的索引\n",
    "median_index = np.argmin(np.abs(trans_errors - median_value))\n",
    "print(median_index)\n",
    "\n",
    "# 打开 results_sparse.txt 文件并提取对应行的名字\n",
    "results_path = f'D:/gs-localization/output/cambridge_full/{scene}/results_sparse.txt'\n",
    "with open(results_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# 提取每一行的第一个字段（假设是名字）\n",
    "names = [line.split()[0] for line in lines]\n",
    "\n",
    "# 根据找到的索引获取对应的名字\n",
    "median_name = names[median_index]\n",
    "\n",
    "# 输出结果\n",
    "print(f\"最接近中值错误的图片名字: {median_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ef400d21-a592-4a3c-a023-f8760d0ee3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\27118\\AppData\\Local\\Temp\\ipykernel_30920\\907923877.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  initial_R = torch.tensor(viewpoint.R_gt)\n",
      "C:\\Users\\27118\\AppData\\Local\\Temp\\ipykernel_30920\\907923877.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  initial_T = torch.tensor(viewpoint.T_gt).squeeze()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "???\n",
      "???\n"
     ]
    }
   ],
   "source": [
    "i = 21\n",
    "image = 21\n",
    "viewpoint = Camera.init_from_intrinsic(dataset, i)\n",
    "viewpoint.compute_grad_mask(config)\n",
    "viewpoint.R = viewpoint.R_gt\n",
    "viewpoint.T = viewpoint.T_gt\n",
    "initial_R = torch.tensor(viewpoint.R_gt)\n",
    "initial_T = torch.tensor(viewpoint.T_gt).squeeze()\n",
    "print(\"???\")\n",
    "gradient_decent(viewpoint, config, initial_R, initial_T)\n",
    "print(\"???\")\n",
    "\n",
    "file.close()\n",
    "\n",
    "render_pkg = render(\n",
    "            viewpoint, Model, pipeline_params, background\n",
    "        )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6ab0b216-b905-4ceb-86a9-de8260c1d5bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "# 假设 viewpoint.original_image 和 render_pkg[\"render\"] 是 Tensor\n",
    "ground_truth_tensor = viewpoint.original_image\n",
    "localized_tensor = render_pkg[\"render\"]\n",
    "\n",
    "# 找到每个像素中 R、G、B 三个通道的最大值\n",
    "max_vals, _ = localized_tensor.max(dim=0)  # 得到每个像素的最大值 (H, W)\n",
    "\n",
    "# 找到哪些像素的最大值超过 1\n",
    "exceeds_one_mask = max_vals > 1  # 布尔掩码，标记哪些像素的最大值超过 1\n",
    "\n",
    "# 对超过 1 的地方，将 R、G、B 值同时按最大值进行归一化\n",
    "localized_tensor[:, exceeds_one_mask] = localized_tensor[:, exceeds_one_mask] / (max_vals[exceeds_one_mask] )\n",
    "\n",
    "# 将 Tensor 转换为 PIL 图像\n",
    "tensor_to_pil = transforms.ToPILImage()\n",
    "\n",
    "ground_truth_image = tensor_to_pil(ground_truth_tensor)\n",
    "localized_image = tensor_to_pil(localized_tensor)\n",
    "\n",
    "# 确保两张图片大小相同（可以选择调整大小）\n",
    "width, height = ground_truth_image.size\n",
    "localized_image = localized_image.resize((width, height))\n",
    "\n",
    "# 创建一个新的空白图像，用来合成 ground truth 和 localized image\n",
    "combined_image = Image.new('RGB', (width, height))\n",
    "\n",
    "# 将图像转换为 NumPy 数组，方便逐像素操作\n",
    "ground_truth_array = np.array(ground_truth_image)\n",
    "localized_image_array = np.array(localized_image)\n",
    "\n",
    "# 根据条件 x < ay 来合成图像\n",
    "for y in range(height):\n",
    "    for x in range(width):\n",
    "        if x < (y * (width / height)):  # 根据比例 x < ay 来判断\n",
    "            combined_image.putpixel((x, y), tuple(ground_truth_array[y, x]))  # 放置 ground truth\n",
    "        else:\n",
    "            combined_image.putpixel((x, y), tuple(localized_image_array[y, x]))  # 放置 localized image\n",
    "\n",
    "# 绘制白色虚线对角线\n",
    "draw = ImageDraw.Draw(combined_image)\n",
    "line_length = 20  # 每个虚线段的长度\n",
    "gap_length = 10   # 每段虚线之间的间隔\n",
    "\n",
    "# 计算对角线的总长度\n",
    "diagonal_length = int((width**2 + height**2)**0.5)\n",
    "\n",
    "# 循环绘制虚线\n",
    "for i in range(0, diagonal_length, line_length + gap_length):\n",
    "    start_x = int(i * (width / diagonal_length))  # 起点 x\n",
    "    start_y = int(i * (height / diagonal_length))  # 起点 y\n",
    "    end_x = int((i + line_length) * (width / diagonal_length))  # 终点 x\n",
    "    end_y = int((i + line_length) * (height / diagonal_length))  # 终点 y\n",
    "\n",
    "    # 绘制虚线的段\n",
    "    draw.line((start_x, start_y, end_x, end_y), fill=\"white\", width=3)\n",
    "\n",
    "# 画小框\n",
    "small_box_start = (200, 150)  # 小框左上角起始点 (x, y)\n",
    "small_box_width = 150         # 小框的宽度\n",
    "small_box_height = 100        # 小框的高度\n",
    "small_box_end = (small_box_start[0] + small_box_width, small_box_start[1] + small_box_height)\n",
    "\n",
    "# 绘制蓝色小框\n",
    "draw.rectangle([small_box_start, small_box_end], outline=\"green\", width=2)\n",
    "\n",
    "# 提取小框中的部分\n",
    "small_box_region = combined_image.crop((small_box_start[0], small_box_start[1], small_box_end[0], small_box_end[1]))\n",
    "\n",
    "# 放大小框中的部分\n",
    "scale_factor = 1.6  # 放大倍数\n",
    "large_box_region = small_box_region.resize((int(small_box_width * scale_factor), int(small_box_height * scale_factor)))\n",
    "\n",
    "# 将放大的大框放置在小框旁边，覆盖图片部分区域\n",
    "large_box_start_x = small_box_start[0]   # 小框右边再加10像素\n",
    "large_box_start_y = small_box_start[1] + small_box_width - 20\n",
    "\n",
    "# 确保大框不会超出图片边界\n",
    "if large_box_start_x + large_box_region.width > width:\n",
    "    large_box_start_x = width - large_box_region.width - 10\n",
    "if large_box_start_y + large_box_region.height > height:\n",
    "    large_box_start_y = height - large_box_region.height - 10\n",
    "\n",
    "# 将放大的区域粘贴回原图中\n",
    "combined_image.paste(large_box_region, (large_box_start_x, large_box_start_y))\n",
    "\n",
    "# 画大框\n",
    "large_box_end = (large_box_start_x + large_box_region.width, large_box_start_y + large_box_region.height)\n",
    "draw.rectangle([large_box_start_x, large_box_start_y, large_box_end[0], large_box_end[1]], outline=\"green\", width=3)\n",
    "\n",
    "\n",
    "ground_truth_image.save(f\"C:/Users/27118/Desktop/pics2/{scene}_orig.png\")\n",
    "localized_image.save(f\"C:/Users/27118/Desktop/pics2/{scene}_loc.png\")\n",
    "localized_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5509692-147a-4be1-a44b-e685dc3c85c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582330f4-661f-4c5b-8dc8-ec2849431267",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372148b8-0046-435e-99dc-4fe83eb3a83d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "620b93aa-f042-4ef0-9cf8-877c75930b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# 场景顺序\n",
    "scenes = [\"chess\", \"fire\", \"heads\", \"office\", \"pumpkin\", \"redkitchen\", \"stairs\"]\n",
    "\n",
    "# 图片路径模板\n",
    "image_paths = [f\"C:/Users/27118/Desktop/{scene}_compare.png\" for scene in scenes]\n",
    "\n",
    "# 打开所有图片，并获取宽高信息\n",
    "images = [Image.open(img_path) for img_path in image_paths]\n",
    "width, height = images[0].size  # 假设所有图片大小相同\n",
    "\n",
    "# 创建一个白色的图片，用作第 8 张图\n",
    "white_image = Image.new('RGB', (width, height), color='white')\n",
    "\n",
    "# 添加白色图片到 images 列表中，确保最后总共有 8 张图片\n",
    "images.append(white_image)\n",
    "\n",
    "# 创建一个新的空白图像，宽度为4张图片的宽度，高度为两行的图片高度\n",
    "new_image = Image.new('RGB', (width * 4, height * 2))\n",
    "\n",
    "# 逐一粘贴图片到新图像中\n",
    "for i, img in enumerate(images):\n",
    "    # 计算每张图片的位置\n",
    "    x_offset = (i % 4) * width  # 每行最多放置4张图片\n",
    "    y_offset = (i // 4) * height  # 放置到第几行\n",
    "    new_image.paste(img, (x_offset, y_offset))\n",
    "\n",
    "# 显示合成后的图像\n",
    "new_image.show()\n",
    "\n",
    "# 保存最终合成图像\n",
    "new_image.save(\"C:/Users/27118/Desktop/combined_image.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dc0ab640-4424-4311-8a6e-6570e4ec1c21",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:/gs-localization/output/7scenes/{scene}/rot_errors.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:/gs-localization/output/7scenes/\u001b[39m\u001b[38;5;132;01m{scene}\u001b[39;00m\u001b[38;5;124m/rot_errors.npy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 加载.npy文件\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 查看文件内容\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39mmedian(data))\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\gaussian_splatting\\lib\\site-packages\\numpy\\lib\\npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:/gs-localization/output/7scenes/{scene}/rot_errors.npy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 指定.npy文件的路径\n",
    "f\"D:/gs-localization/output/7scenes/{scene}/results_sparse.txt\"\n",
    "file_path = 'D:/gs-localization/output/7scenes/{scene}/rot_errors.npy'\n",
    "\n",
    "# 加载.npy文件\n",
    "data = np.load(file_path)\n",
    "\n",
    "# 查看文件内容\n",
    "print(np.median(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19378a46-5737-4330-8661-eefee72db101",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
